{
  "fetch_date": "2026-01-17T09:31:16.249Z",
  "week_range": "Jan 10–16",
  "articles": {
    "ieee": [],
    "robotreport": [
      {
        "title": "NEURA Robotics partners with Bosch to advance German-made robotics",
        "link": "https://www.therobotreport.com/neura-robotics-partners-bosch-advance-german-made-robotics/",
        "pubDate": "2026-01-16T22:00:12.000Z",
        "content": "NEURA Robotics' quadruped, 4NE1 humanoid robot, and 4NE1 Mini at CES 2026. [https://www.therobotreport.com/wp-content/uploads/2026/01/CES-2026-NEURA-Robotics-Booth-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/CES-2026-NEURA-Robotics-Booth-featured.jpg NEURA Robotics’ quadruped, 4NE1 humanoid robot, and 4NE1 Mini at CES 2026. | Source: NEURA Robotics NEURA Robotics GmbH this week announced a new strategic technology and development partnership with Robert Bosch GmbH. The companies plan to support the industrial deployment of humanoid robotics and physical AI from Germany. The partners said they are expanding the data foundation for humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] robotics. NEURA and Bosch will jointly collect real-world work, movement, and environmental data in Bosch [https://www.therobotreport.com/tag/bosch/] facilities using advanced sensor technologies. As part of the collaboration, the companies will co-develop AI [https://www.therobotreport.com/category/design-development/ai-cognition/]-based core and functional software, as well as intuitive user interfaces. The acquired real-world production data feeds directly into the Neuraverse, NEURA’s robotics ecosystem. It connects robots, the “world of things,” and physical AI to improve robotic capabilities through real industrial experience. Software updates and AI will enable continuous improvement across the entire fleet, asserted the company [https://www.therobotreport.com/tag/neura-robotics/]. The combination of real-world data, Neuraverse, and Bosch’s expertise in manufacturing will create a feedback loop that supports ongoing innovation across the robotics ecosystem, the German [https://www.therobotreport.com/tag/germany] companies claimed. The partnership is intended to promote practical use of humanoids in existing industrial processes, supporting people in daily operations by increasing productivity, flexibility, and safety. QUADRUPED, 4NE1 MINI DEBUT Earlier this month, NEURA Robotics introduced its Quadruped robot and 4NE1 Mini robots at CES [https://www.therobotreport.com/tag/ces] 2026 in Las Vegas. As with all of the company’s robots, these legged [https://www.therobotreport.com/tag/legged-robots/] models are fully integrated with the Neuraverse [https://neura-robotics.com/neuraverse/]. NEURA said it engineered Quadruped [https://neura-robotics.com/product/quadruped-reservation/] for multimodal cognitive interaction [https://www.therobotreport.com/category/design-development/haptics/] and fully autonomous operation. The company said it can handle challenging terrain and complex environments. The robot measures less than 1 m (3.2 ft.) in height and has a 21.9 kg (48.5 lb.) payload. It features multi-sensor fusion, intelligent mapping, and 360° environment vision. NEURA Quadruped is now available for reservation at a fee of €100 ($116.02 U.S.) and a full price of €50,000 ($58,004.64). The 4NE1 Mini [https://neura-robotics.com/product/4ne1-mini-reservation/] aims to offer cognitive intelligence in a versatile form factor. Its smaller size and easy handling make this robot well-suited for science and education compared to larger humanoid robots, NEURA said. Standing at 52 in., it combines natural language interaction, computer vision, and reinforcement learning with multi-language voice recognition and human-detection safety features. 4NE1 Mini is also available for reservation at €100 and a full price of €19,999 $23,195.18) for the Standard model and €29,999 ($34,793.35) for the Pro version. NEURA CONTINUES GLOBAL EXPANSION In addition to the two new robots, NEURA has also been expanding its global footprint. Last month, the company opened [https://neura-robotics.com/expansion-to-switzerland/] a new location on Zollikerstrasse in Zurich. NEURA plans to use this 1,800-sq.-m (19,375-sq.-ft.) hub for research, development, and partner collaborations in the field of physical AI. Metzingen, Germany-based NEURA said it is a pioneer in “cognitive robotics” that combines sensors and components for physical AI on each of its devices. The company [https://www.therobotreport.com/tag/neura-robotics/] said its machines can learn, adapt, and act autonomously in real-world environments. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- The post NEURA Robotics partners with Bosch to advance German-made robotics [https://www.therobotreport.com/neura-robotics-partners-bosch-advance-german-made-robotics/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "NEURA Robotics and Bosch plan to co-develop AI-based core and functional software, as well as intuitive user interfaces.\nThe post NEURA Robotics partners with Bosch to advance German-made robotics app"
      },
      {
        "title": "IFR names top 5 global robotics trends of 2026",
        "link": "https://www.therobotreport.com/ifr-top-5-global-robotics-trends-of-2026/",
        "pubDate": "2026-01-16T18:13:12.000Z",
        "content": "Top 5 global robotics trends from IFR. [https://www.therobotreport.com/wp-content/uploads/2026/01/Robot_Trends_2026-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/Robot_Trends_2026-featured.jpg The International Federation of Robotics expects robot utility to continue to spread this year. Source: IFR The global market value of industrial robot installations has reached an all-time high of $16.7 billion, according to the International Federation of Robotics, or IFR. The Frankfurt, Germany-based organization [https://www.therobotreport.com/tag/international-federation-of-robotics/] predicted that a number of technological innovations, market forces, and new fields of business will drive future demand. Here are the top five trends the IFR identified for the robotics industry in 2026. 1. AI GIVES ROBOTS GREATER AUTONOMY Robots that use artificial intelligence [https://www.therobotreport.com/category/design-development/ai-cognition/] to work independently are becoming more common. The main benefit of AI in this context is the increased autonomy of robots empowered by AI. Different types of AI drive this trend: Analytical AI helps to process large datasets, detect patterns, and provide actionable insights. For example, it can autonomously anticipate failures before they occur in smart factories. Such AI can also support path planning and resource allocation in logistics. Generative AI [https://therobotreport.com/tag/generative-ai], on the other hand, marks a shift from rule-based automation to intelligent, self-evolving systems. GenAI creates new outputs and enables robots to learn new tasks autonomously and generate training data through simulation. This also allows a new kind of human–robot interaction [https://www.therobotreport.com/category/design-development/haptics/] with natural language and vision-based commands. A key trend to further develop autonomy in robotics is agentic AI. This technology combines analytical AI for structured decision-making and generative AI for adaptability. The hybrid approach aims to make modern robotics capable of working independently in complex, real-world environments. 2. ROBOTS GAIN VERSATILITY AS IT MEETS OT Demand for versatile robots is accelerating, according to the IFR. This directly reflects a market push toward a convergence of information technology (IT) and operational technology (OT). The combination of IT’s data-processing power and OT’s physical control capabilities can enhance robot versatility through real-time data exchange, automation, and advanced analytics. This integration is a foundational element of the digital enterprise and Industry 4.0. The IT/OT convergence breaks down silos, creating a seamless flow of data between the digital and physical worlds, which significantly enhances the capabilities and versatility of robotics. 3. IFR EXPECTS HUMANOIDS TO PROVE RELIABILITY AND EFFICIENCY New Atlas humanoid at CES 2026. [https://www.therobotreport.com/wp-content/uploads/2026/01/69743-on-site-photo-3.jpeg]https://www.therobotreport.com/wp-content/uploads/2026/01/69743-on-site-photo-3.jpeg Boston Dynamics unveiled a new version of Atlas at CES 2026. | Credit: Hyundai The field of humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] robotics is expanding rapidly, noted the IFR. Many see humanoid technology as promising for industrial applications where flexibility is required, typically in environments designed for humans. Pioneered by the automotive [https://www.therobotreport.com/category/markets-industries/automotive/] industry, applications in warehousing [https://www.automatedwarehouseonline.com/] and manufacturing [https://www.therobotreport.com/category/markets-industries/manufacturing/] are coming into focus worldwide. Today, companies and researchers are moving beyond prototypes to deploy humanoids in real life. Reliability and efficiency are key to industrial success. In competing with traditional automation, these robots need to match stringent requirements for cycle times, energy consumption, and maintenance costs. Industry standards [https://www.therobotreport.com/tag/standards] also define safety levels, durability criteria, and consistent performance for humanoid robots on the factory floor. Humanoids intended to fill labor gaps need to achieve human-level dexterity and productivity, key measures to prove real-world efficiency. 4. SAFETY AND SECURITY REMAIN CONCERNS FOR DEVELOPERS, USERS As robots increasingly operate alongside humans in factories and service settings, ensuring they operate safely [https://www.therobotreport.com/category/safety-security/] is not just important; it’s essential for the robotics industry. The AI-driven autonomy fundamentally changes the safety landscape, which makes testing, validation, and human oversight much more complex—but also more necessary. The need for such validation and control becomes particularly clear in the intended use of humanoids, the IFR acknowledged. Robots need to be designed and certified in line with ISO safety standards and clearly defined liability frameworks. In the context of AI in robotics and the convergence of IT and OT, a spectrum of safety and security concerns arises that demand robust governance and clear assignment of liability. The rapid expansion of robotics systems into cloud-connected and AI-driven environments is exposing industrial production to a growing array of cybersecurity [https://www.therobotreport.com/tag/cybersecurity] threats. Experts have cited a rise in hacking attempts targeting robot controllers and cloud platforms, enabling unauthorized access and potential system manipulation. As robots become more integrated into workplaces, concerns are mounting over the sensitive data they collect — including video, audio, and sensor streams, noted the IFR. Deep learning models, which are often described as “black boxes,” can produce results that are difficult or impossible to explain, even to their own developers. The legal and ethical ambiguity surrounding liability has prompted calls for clear frameworks to govern AI deployment. 5. ROBOTS BECOME ALLIES IN TACKLING LABOR GAPS, SAYS IFR Two women working with a collaborative robot. [https://www.therobotreport.com/wp-content/uploads/2026/01/AdobeStock_1779960153.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/AdobeStock_1779960153.jpg Source: Adobe Stock Employers around the world are struggling to find people with the specialized skills required. These unfilled jobs leave existing staff covering extra shifts, with rising stress and fatigue across all sectors. The IFR said the adoption of robotics and automation is a key strategy for addressing this issue. In this transformation process, employers can benefit from taking their human workforce [https://www.therobotreport.com/tag/workforce] on board. The close cooperation with employees in implementing robots plays a crucial role in ensuring acceptance, both in industrial manufacturing settings as well as in manifold service [https://www.therobotreport.com/tag/service-robots/] applications, the IFR said. Robots can deliver benefits such as tackling labor shortages, taking away routine tasks, or opening up new career opportunities. If these benefits are properly communicated, they will be accepted as allies in the workplace. At the same time, robots are a way to make a workplace much more attractive to young people, said the organization [https://www.therobotreport.com/tag/ifr/]. Companies and governments are pushing skilling and upskilling programs to help workers keep up with changing skills demand and compete in an automation-driven economy. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- Editor’s Note: This article was syndicated from the IFR’s blog [https://ifr.org/ifr-press-releases/news/top-5-global-robotics-trends-2026]. The post IFR names top 5 global robotics trends of 2026 [https://www.therobotreport.com/ifr-top-5-global-robotics-trends-of-2026/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "The IFR has made its predictions of the top 5 robotics industry trends for 2026, including an increased focus on cybersecurity.\nThe post IFR names top 5 global robotics trends of 2026 appeared first o"
      },
      {
        "title": "Humanoid and Siemens proof of concept shows the way to industrial deployments",
        "link": "https://www.therobotreport.com/humanoid-siemens-proof-of-concept-may-lead-more-industrial-deployments/",
        "pubDate": "2026-01-16T16:10:17.000Z",
        "content": "A mobile manipulator from Humanoid picking up a tote from a conveyor. The company recently completed a proof of concept with Siemens. [https://www.therobotreport.com/wp-content/uploads/2026/01/HMND_SIEMENS_featured.jpg] The HMND 01 wheeled, two-armed robot picks up a tote. Source: Humanoid Humanoid and Siemens AG yesterday said that they have completed a proof of concept, or POC, demonstrating the use of a mobile manipulator in industrial logistics. The POC involved Humanoid’s HMND 01 wheeled Alpha robot. The company deployed the robot in actual operations at a Siemens facility. Humanoid, which is formally known as SKL Robotics Ltd., structured the POC in two phases. The first phase focused on in-house development and demonstration. The company [https://www.therobotreport.com/tag/humanoid/] said its team built a “physical twin” to support testing, optimization, and rapid iteration. In the POC, the HMND 01 Alpha conducted a tote-to-conveyor de-stacking task within Siemens’ logistics process. The mobile manipulator [https://www.therobotreport.com/category/robots-platforms/humanoids/] autonomously picked totes from a storage stack, transported them to a conveyor, and placed them at the designated pickup point for human operators. The system repeated this sequence until the stack was fully empty, which demonstrated how robots can take on repetitive logistics tasks, Humanoid said. The second phase included a two-week on-site deployment at the Siemens Electronics Factory in Erlangen, Germany, where the company [https://www.therobotreport.com/tag/siemens/] assessed the robots in a real-world production environment. HMND 01 MEETS METRICS FOR SUCCESS AT SIEMENS Humanoid said the POC measured both the performance and reliability of its robots under autonomous operation. They met all target metrics, including a throughput of 60 tote moves per hour, operation with two different tote sizes, continuous autonomous task execution for more than 30 minutes, and uptime exceeding 8 hours. The partners also evaluated the project using the overall pick-and-place success rate and the autonomous pick-and-place success rate, which were both above 90%. Humanoid and Siemens said this POC is a first step toward a long-term strategic collaboration. The companies added that they are open to expanding the scope and adding use cases. The partners also may progress toward a broader rollout, deploying a greater number of robots across Siemens’ facilities, based on the robot’s specific skill set. “As Siemens’ ‘Customer 0,’ the Electronics Factory Erlangen is excited to partner with the Humanoid team,” said Stephan Schlauss, global head of manufacturing motion control at Siemens. “We’re tackling production automation, discovering new opportunities for Siemens, and are eager to advance this promising technology across our factory network to deliver customer value.” HUMANOID PLANS TO DEPLOY ITS ROBOTS IN MORE SETTINGS “At Humanoid, we are a commercially driven company. Our focus is on creating robots that deliver measurable value in real-world settings,” stated Artem Sokolov, founder and CEO of Humanoid [https://thehumanoid.ai/]. “Working closely with industrial and technology partners allows us to validate our systems against real operational requirements and understand which use cases matter outside the lab.” “This joint POC with Siemens showed clear potential for practical deployment of mobile manipulator [https://www.therobotreport.com/category/robots-platforms/humanoids/] robots,” he added. “We see them move steadily toward the real world, and partnerships like this one help accelerate that transition.” Humanoid this week also partnered [https://www.therobotreport.com/schaeffler-humanoid-partner-build-deploy-hundreds-robots/] with motion technology provider Schaeffler Technologies AG. Over the next five years, the companies agreed to bring hundreds of Humanoid’s robots into Schaeffler’s production facilities. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- The post Humanoid and Siemens proof of concept shows the way to industrial deployments [https://www.therobotreport.com/humanoid-siemens-proof-of-concept-may-lead-more-industrial-deployments/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Humanoid successfully demonstrated its HMND 01 Alpha wheeled mobile manipulator at a Siemens production facility in Germany. \nThe post Humanoid and Siemens proof of concept shows the way to industrial"
      },
      {
        "title": "MassRobotics opens applications for fourth Form and Function Robotics Challenge",
        "link": "https://www.therobotreport.com/massrobotics-opens-applications-for-fourth-form-and-function-robotics-challenge/",
        "pubDate": "2026-01-15T20:27:49.000Z",
        "content": "The 2025 Form and Function Challenge Winners. [https://www.therobotreport.com/wp-content/uploads/2026/01/northeastern-form-and-function-2025-winner-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/northeastern-form-and-function-2025-winner-featured.jpg Northeastern University won the 2025 Form and Function Robotics Challenge. | Source: MassRobotics MassRobotics this week announced its fourth annual Form and Function Robotics Challenge. For the challenge, selected university teams will have the opportunity to compete for a $10,000 grand prize. There are $1,000 awards for second and third place, as well as a $1,000 Audience Choice Award. Applications [https://www.massrobotics.org/form-function-challenge/] are now open and will be accepted through Feb. 2, 2026. The challenge invites participants from around the world to showcase innovative robotics projects. The challenge will culminate in live, in-person demonstrations for industry leaders, investors, and the broader robotics community at the 2026 Robotics Summit & Expo [https://www.roboticssummit.com/]. The event will be in Boston on May 27 and 28. MassRobotics said it is the largest independent robotics hub dedicated to accelerating innovation, commercialization, and adoption. Its stated mission is to help create and scale the next generation of successful robotics and AI technology companies. The nonprofit [https://www.therobotreport.com/tag/massrobotics] added that it provides entrepreneurs and startups with the workspace, resources, programming, and connections they need to develop, prototype, test, and commercialize their products and solutions. MASSROBOTICS SEEKS CREATIVE SOLUTIONS TO REAL PROBLEMS The Form and Function Robotics Challenge gives students a platform to demonstrate creative solutions for a range of problems, said MassRobotics. It is looking for projects that balance compelling design with real-world functionality. MassRobotics said will evaluate submissions on both technical execution and presentation quality. The organization added that it will emphasize systems that work within realistic prototyping constraints while delivering strong form and function. Previous winning teams have represented leading institutions. These include Northeastern University [https://www.therobotreport.com/northeastern-soft-robotic-arm-wins-massrobotics-form-function-challenge-at-robotics-summit/], Tufts University, Seoul National University, Harvard University, Wentworth Institute of Technology, the University of British Columbia, MIT, Indiana University Bloomington, Worcester Polytechnic Institute (WPI), and the University of Waterloo. Seoul National University, team shown here at the Robotics Summit & Expo, was the 2024 winner of the MassRobotics Form and Function Robotics Challenge. [https://www.therobotreport.com/wp-content/uploads/2025/01/Form_Function.jpg]https://www.therobotreport.com/wp-content/uploads/2025/01/Form_Function.jpg Seoul National University won the 2024 Form and Function Challenge. | Source: MassRobotics PARTNERS TO SUPPORT FORM AND FUNCTION CHALLENGE TEAMS MassRobotics partners Advanced Micro Devices, Dassault Systèmes [https://www.therobotreport.com/tag/dassault-systemes/], Harmonic Drive [https://www.therobotreport.com/tag/harmonic-drive], Maeden, maxon motor, and Mitsubishi Electric [https://www.therobotreport.com/tag/mitsubishi/] are supporting the challenge. “AMD is committed to powering the next generation of robotics startups by delivering the right platforms that enable breakthrough innovation,” said KV Thanjavur Bhaaskar, robotics lead at AMD [https://www.therobotreport.com/tag/amd/]. “That journey begins at the source, academia, where bold ideas are first imagined and engineered.” “Mitsubishi Electric Automation is proud to support the Form and Function Robotics Challenge and our longstanding partnership with MassRobotics,” added Dr. William Nguyen, development manager at MEAU. “By providing students with access to industrial-grade PLCs, HMIs, Servos, Robotics simulation software, we’re helping bridge academic innovation with real-world automation.” “At maxon, we’re excited to back the Form and Function Robotics Challenge by providing student teams with the high‑precision motors and controllers they need to bring their concepts to life,” said Nicole Mathieu, senior inside sales engineer for mobility solutions and industrial automation at maxon [https://www.therobotreport.com/tag/maxon/]. “This initiative connects classroom innovation with the performance standards of real‑world engineering.” ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- The post MassRobotics opens applications for fourth Form and Function Robotics Challenge [https://www.therobotreport.com/massrobotics-opens-applications-for-fourth-form-and-function-robotics-challenge/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "The latest MassRobotics Form and Function challenge will culminate with in-person demonstrations at the Robotics Summit & Expo.\nThe post MassRobotics opens applications for fourth Form and Function Ro"
      },
      {
        "title": "Mytra closes $120M funding for pallet-storing robots",
        "link": "https://www.therobotreport.com/mytra-closes-150m-series-c-funding-pallet-storing-robots/",
        "pubDate": "2026-01-15T19:56:24.000Z",
        "content": "hero image of the mytrabot in a storage array. [https://www.therobotreport.com/wp-content/uploads/2026/01/mytra-mytrabot-featured.jpg] Mytra said it addresses bottlenecks for warehouse-dependent organizations, from Fortune 100 suppliers to local grocers. | Credit: Mytra Mytra Inc. today said it has closed its $120 million Series C round. The company has been developing an automated storage and retrieval system, or ASRS, to handle pallets with loads weighing up to 1,360 kg (3,000 lb.). Material handling and movement represent nearly 50% of manufacturing labor [https://edge.prnewswire.com/c/link/?t=0&l=en&o=4595517-1&h=942911768&u=https%3A%2F%2Fwww.census.gov%2Fnewsroom%2Fblogs%2Frandom-samplings%2F2017%2F10%2Fbeyond-products.html%23%3A%7E%3Atext%3DManufacturing%2520industry%2520employees%2520work%2520in%2Coccupations%2520(see%2520Figure%25202)&a=Material+handling+and+movement+represent+nearly+50%25+of+manufacturing+labor], according to the U.S. Census Bureau. However, they have not changed much in a century, noted Mytra. As a result, more than 400,000 industrial roles are open [https://nam.org/mfgdata/facts-about-manufacturing-expanded/#:~:text=7.,756%2C000%20between%202021%20and%202023] today, and that figure is headed to 2 million [https://nam.org/2-1-million-manufacturing-jobs-could-go-unfilled-by-2030-13743/?stream=workforce] by 2030, reported the National Association of Manufacturers. Meanwhile, Arker Warehouse said that about 60% of warehouse footprint is “dead space [https://edge.prnewswire.com/c/link/?t=0&l=en&o=4595517-1&h=2440888238&u=https%3A%2F%2Farkerwarehouse.com%2Fwarehouse-space%2F&a=roughly+60%25+of+warehouse+footprint+is+dead+space]” — aisles and clearance that add cost but no value. Approximately 80% of industrial facilities have no automation [https://edge.prnewswire.com/c/link/?t=0&l=en&o=4595517-1&h=1206064214&u=https%3A%2F%2Fwww.researchandmarkets.com%2Freports%2F5459087%2Fwarehouse-automation-market-share-analysis%3Fsrsltid%3DAfmBOoqho_MMYwnwAor9vDXnQ6L-1hmg3wzaAFHxJKuLaqplfH3pR_wF&a=80%25+of+industrial+facilities+have+zero+automation] because of cost, complexity, and limited flexibility once installed, noted Research and Markets. “I saw firsthand that material flow needs a fundamental platform shift, not incremental improvements,” said Chris Walti [https://www.therobotreport.com/new-dimensions-in-warehouse-automation-podcast/], co-founder and CEO of Mytra. “We’re not building better warehouse robots; we’re rebuilding the infrastructure layer that every industrial process depends on. Material flow should work like cloud computing: abstracted, programmable, and continuously optimizing.” MYTRA ABSTRACTS MATERIAL FLOW FOR EFFICIENCY Mytra Robotics has developed mobile robots [https://www.therobotreport.com/category/robots-platforms/amrs/] that can climb its ASRS [https://www.therobotreport.com/tag/asrs/] and store pallets with a passive infrastructure. The company [https://www.therobotreport.com/tag/mytra/] was the 2025 RBR50 Startup of the Year [https://www.therobotreport.com/rbr50-2025/]. In addition to its climbing Mytrabots, the company [https://www.therobotreport.com/tag/mytra/]‘s built-in inventory management software uses AI [https://www.therobotreport.com/category/design-development/ai-cognition/] to coordinate storage and queuing of the pallets [https://www.therobotreport.com/tag/palletizing/] as needed for trailer loading or depalletizing. According to Mytra, its automation [https://www.therobotreport.com/why-the-future-of-robotics-isnt-necessarily-humanoid/] is also useful for cross-docking warehouses because it can statically stage and store full pallets, queuing them for the trailer loading process. The system abstracts material flow into software-defined primitives — move, store, pick, route — that standardize operations and make every cubic foot of space addressable. Mytra said its early deployments have demonstrated 32% reductions in material handling [https://www.automatedwarehouseonline.com/category/move/] labor and 34% improvements in storage density. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- CUSTOMER BASE AND INVESTOR LIST GROW Founded in 2022, Mytra last year signed contracts with some of the world’s largest organizations. It now counts a Fortune 100 food company and a Fortune 500 industrial-supply distribution company as customers. In 2025 alone, Mytra said it signed a large-scale deployment 60x the size of its largest prior installation, shipped two pilot systems, went live in production at its new customer site, and moved into a new facility 7x its previous space. The Brisbane, Calif.-based company also grew its team by 78%. Mytra has added Gabi Gantus as chief financial officer, Ingrid Cotoros as chief development officer, and Nigel Marcussen as vice president of scaling. Zach Kirkhorn, former chief financial officer at Tesla, has joined its board. Avenir Growth led Mytra’s Series C round, which brought its total funding to more than $200 million. New investors Kivu Ventures, Liquid 2, D.E. Shaw, and Offline Ventures participated alongside existing investors Eclipse, Greenoaks, Abstract Ventures, and Promus Ventures. The company’s strategic investors include Lineage and RyderVentures, the corporate venture capital arm of Ryder System Inc. Mytra said it plans to use its latest funding to accelerate and scale deployment to meet customer demand and to acquire strategic talent. The post Mytra closes $120M funding for pallet-storing robots [https://www.therobotreport.com/mytra-closes-150m-series-c-funding-pallet-storing-robots/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Mytra Robotics has Series C funding to scale its automated warehouse storage and retrieval systems, whose shuttles can move loaded pallets.\nThe post Mytra closes $120M funding for pallet-storing robot"
      },
      {
        "title": "Skild AI raises $1.4B to build ‘omni-bodied’ robot brain",
        "link": "https://www.therobotreport.com/skild-ai-raises-1-4b-building-omni-bodied-robot-skild-brain/",
        "pubDate": "2026-01-15T15:25:47.000Z",
        "content": "Humanoid robots and robot arms clean curtains, leap over obstacles, load a dishwasher, and cook an egg. The Skild Brain enables robots to handle a wide range of tasks, says Skild AI. [https://www.therobotreport.com/wp-content/uploads/2026/01/Skild_AI-funding.jpg] If there is a machine that moves, the Skild Brain will eventually be able to operate it, says Skild AI. Source: Skild AI To get to a general-purpose robot, developers need a unified robotics foundation model, according to Skild AI. The company yesterday said it has raised close to $1.4 billion, bringing its valuation to more than $14 billion. It is building the Skild Brain, which it claimed is “the industry’s first unified robotics foundation model.” Unlike traditional AI [https://www.therobotreport.com/category/design-development/ai-cognition/] models that are tailored to specific robot designs, the foundation model is intended to be “omni-bodied” and to control any robot without prior knowledge of its exact body form, including quadrupeds [https://www.therobotreport.com/tag/legged-robots], humanoids [https://www.therobotreport.com/category/robots-platforms/humanoids/], tabletop arms [https://www.therobotreport.com/category/robots-platforms/collaborative-robot/], and mobile manipulators [https://www.therobotreport.com/tag/mobile-manipulation/]. The Skild Brain will enable robots to handle everything from simple household [https://www.therobotreport.com/tag/household/] chores like cleaning [https://cts.businesswire.com/ct/CT?id=smartlink&url=https%3A%2F%2Fx.com%2FSkildAI%2Fstatus%2F2011179163076493703&esheet=54396603&newsitemid=20260114335623&lan=en-US&anchor=cleaning&index=2&md5=80c452396f50d2b9a80810784fdab021], loading a dishwasher [https://cts.businesswire.com/ct/CT?id=smartlink&url=https%3A%2F%2Fx.com%2FSkildAI%2Fstatus%2F2010823480439185888&esheet=54396603&newsitemid=20260114335623&lan=en-US&anchor=loading+a+dishwasher&index=3&md5=8011a3cf78134b3bca96e252b8788699], and cooking an egg [https://cts.businesswire.com/ct/CT?id=smartlink&url=https%3A%2F%2Fx.com%2FSkildAI%2Fstatus%2F2010823287954128918&esheet=54396603&newsitemid=20260114335623&lan=en-US&anchor=making+an+egg&index=4&md5=c5270325f8cf17915d5a0efb6b876e82] to physically demanding challenges such as navigating slippery terrain [https://cts.businesswire.com/ct/CT?id=smartlink&url=https%3A%2F%2Fyoutu.be%2FGTImKXRBB6A&esheet=54396603&newsitemid=20260114335623&lan=en-US&anchor=navigating+slippery+terrain&index=5&md5=9da4ca4a6e6a397b439307b33badab28], said the company [http://therobotreport.com/tag/skild-ai]. Founded in 2023 by two pioneers in the field of self-supervised and adaptive robotics, Skild AI said its scalable foundation model for robotics “serves as a shared brain across diverse robot embodiments.” It has offices in Pittsburgh, the San Francisco Bay Area, and Bengaluru, India. SKILD BRAIN TRAINS ON ALTERNATE DATA SOURCES “One of the biggest challenges in building a robotics foundation model is that, unlike language or video models, there is no Internet of robotics,” noted Skild AI. The company said it addresses this challenge by pre-training the Skild Brain on alternate data sources, including learning by watching human videos on the Internet and practicing in physics-based simulations [https://cts.businesswire.com/ct/CT?id=smartlink&url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fcase-studies%2Fskild-ai%2F&esheet=54396603&newsitemid=20260114335623&lan=en-US&anchor=physics-based+simulations&index=7&md5=3929e3219f459d96544f9cf518901f58]. In contrast with robots that are designed for specific applications or that are deployed only in isolated or constrained environments, Skild AI said its model can work across different morphologies to vastly expand the available training set. The Skild Brain can also adapt to unpredictable scenarios, such as loss of limbs, jammed wheels, and increased payload, or even an entirely new body, without retraining or fine-tuning, the company explained. “The Skild Brain can control robots it has never trained on, adapting in real time to extreme changes in form or environments. The model is forced to adapt rather than memorize – much like intelligence in nature,” stated Deepak Pathak, co-founder and CEO of Skild AI. “We believe that a unified, omni-bodied brain is the fastest way to establish a continuous data flywheel where the model gets better with every single deployment, no matter what the hardware or task.” Other companies working on general-purpose AI for robotics include Qualcomm [https://www.therobotreport.com/qualcomm-introduces-general-purpose-architecture-for-robotics/], Sanctuary AI [https://www.therobotreport.com/tag/sanctuary-ai/], and X Square Robot [https://www.therobotreport.com/x-square-robot-secures-140m-in-funding-for-ai-foundation-models/], among others. IN-CONTEXT LEARNING TO SCALE ROBOT INTELLIGENCE Skild AI has shared its progress in a steady cadence [https://cts.businesswire.com/ct/CT?id=smartlink&url=https%3A%2F%2Fx.com%2FSkildAI%2Fhighlights&esheet=54396603&newsitemid=20260114335623&lan=en-US&anchor=released+a+steady+cadence+of+results&index=9&md5=0e4f125d42ffe833eea464307d888e97] over the past year, demonstrating the Skild Brain’s ability to adapt robots’ movements to what they see in the world around them. It credited in-context learning for this generalizability. When the model is introduced in a new body or unseen environment where its actions may fail, its adjusts the robot’s behavior based on its live in-context experience, the company asserted. Skild AI said it has published this research. “We believe this omni-bodied learning is essential for building AGI [artificial general intelligence] that works reliably in the physical world, paving the way for robots that can safely help humans in everyday environments,” said Abhinav Gupta, co-founder and president of Skild AI. “This enables robots to operate dynamically in complex environments, without requiring preprogrammed instructions for each scenario.” ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- SKILD AI SCALES FOR FUTURE DEPLOYMENTS Skild AI said it has grown from zero to about $30 million in revenue in just a few months in 2025. The company is deploying its technology in a variety of environments and scenarios, including security and facility inspection [https://www.therobotreport.com/tag/inspection], last-mile and point-to-point delivery [https://www.therobotreport.com/tag/delivery-robots], warehouses [https://www.automatedwarehouseonline.com/], manufacturing [https://www.therobotreport.com/category/markets-industries/manufacturing/], data centers, and construction [https://www.therobotreport.com/tag/construction] tasks. SoftBank Group [https://www.therobotreport.com/tag/softbank] led Skild AI’s latest funding, with participation from NVIDIA [https://www.therobotreport.com/tag/nvidia/] venture capital arm NVentures, entities administered by Macquarie Capital, Jeff Bezos through Bezos Expeditions, Disruptive, and 1789 Capital. Lightspeed, Felicis, Coatue, and Sequoia Capital expanded their investments. Skild AI previously raised [https://www.therobotreport.com/skild-ai-grabs-300m-to-build-foundation-ai-model-for-robotics/] $300 million in 2024. “Skild AI is building foundational technology for physical AI across robots, tasks, and environments,” said Dennis Chang, managing partner at SoftBank Investment Advisers. “We’re proud to partner with Deepak, Abhinav, and the Skild AI team to bring that shared vision into real-world applications worldwide.” In addition, Samsung [https://www.therobotreport.com/tag/samsung/], LG [https://www.therobotreport.com/tag/lg-electronics], Schneider [https://www.therobotreport.com/tag/schneider-electric/], CommonSpirit, and Salesforce Ventures joined as strategic investors. Other investors included TF Capital, Andra Capital, Palo Alto Growth Capital, KIC, Alpha Square, Mirae Asset, and Destiny. “Solving intelligence for the physical world unlocks enormous commercial value and long-term strategic national importance,” said Rita Waite, a partner at IQT. “Skild AI is uniquely positioned to do both, and we’re excited to be working with this team as they build.” Skild AI said it plans to ultimately deploy robotics in consumer [https://www.therobotreport.com/category/consumer-robotics] homes, with enterprise tasks as the first application. It plans to use the new capital to continue scaling model training and growing deployments of its technology. “Skild AI is poised to be a critical force behind re-inventing American manufacturing with automation,” the company [https://www.skild.ai/] said. The post Skild AI raises $1.4B to build ‘omni-bodied’ robot brain [https://www.therobotreport.com/skild-ai-raises-1-4b-building-omni-bodied-robot-skild-brain/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Skild AI has received investment from SoftBank, NVIDIA, Bezos Expeditions, and more as it builds a brain to operate any robot.\nThe post Skild AI raises $1.4B to build ‘omni-bodied’ robot brain appeare"
      },
      {
        "title": "Caterpillar partners with NVIDIA to lay the foundation for autonomous systems",
        "link": "https://www.therobotreport.com/caterpillar-partners-with-nvidia-to-lay-the-foundation-for-autonomous-systems/",
        "pubDate": "2026-01-14T22:05:08.000Z",
        "content": "An autonomous dozer from Caterpillar. [https://www.therobotreport.com/wp-content/uploads/2026/01/caterpillar-autonomy-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/caterpillar-autonomy-featured.jpg An autonomous dozer from Caterpillar. | Source: Caterpillar Caterpillar Inc. last week announced an expanded collaboration with NVIDIA Corp. The companies said they plan to develop AI-enhanced products and manufacturing systems together. “As AI moves beyond data to reshape the physical world, it is unlocking new opportunities for innovation — from job sites and factory floors to offices,” stated Caterpillar CEO Joe Creed. “Caterpillar is committed to solving our customers’ toughest challenges by leading with advanced technology in our machines and every aspect of business. Our collaboration with NVIDIA is accelerating that progress like never before.” The NVIDIA Jetson Thor [https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-thor/] platform enables real-time AI [https://www.therobotreport.com/category/design-development/ai-cognition/] inference on Cat construction [https://www.therobotreport.com/tag/construction/], mining [https://www.therobotreport.com/category/mining/], and power [https://www.therobotreport.com/category/energy-solar-renewables] equipment. Caterpillar said this lays the foundation for next-generation autonomy and intelligent in-cab experiences. It added that the upgrades will ensure that assets are ready for AI-assisted and potentially autonomous operations. The Irving, Texas-based company [https://www.therobotreport.com/tag/caterpillar/] cited the following examples: * In-cab AI features: An intelligent operator assistant can provide customers with personalized insights, as well as real-time coaching, productivity tips, and safety alerts. * Autonomy at scale: Construction and mining machines equipped with AI-driven recommendations, capable of processing billions of data points in milliseconds to navigate [https://www.therobotreport.com/tag/navigation] complex, variable jobsite conditions. * A new level of machine intelligence: Cat fleets powered by AI, machine learning, computer vision [https://www.therobotreport.com/category/technologies/cameras-imaging-vision/], and edge computing that process sensor data in real time and serve as a digital nervous system for customers’ jobsites. AI ASSISTANT TO HELP WITH MAINTENANCE AND TROUBLESHOOTING At CES [https://www.therobotreport.com/tag/CES/] 2026, Caterpillar debuted the Cat AI Assistant, a proactive partner embedded in Cat digital and onboard products to help customers take confident action. It built the assistant using NVIDIA [https://www.therobotreport.com/tag/nvidia/] Riva open speech models for accuracy and lifelike voices. Cat AI Assistant will answer questions and provide personalized recommendations on equipment, parts, maintenance, and more. In cab, it will use voice activation to enable settings, guide troubleshooting, and connect users to the right resources across Cat apps and websites. Caterpillar said the assistant uses its own trusted data stored on the Helios unified data platform, so customers get reliable, context-rich information to make daily work easier. NVIDIA AND CATERPILLAR HAVE ‘FULL-SPECTRUM’ PARTNERSHIP “For a century, Caterpillar has built the industrial machines that shaped the world,” said Jensen Huang, founder and CEO of NVIDIA. “In the age of AI, NVIDIA and Caterpillar are partnering across the full spectrum – from autonomous construction fleets to the AI data centers powering the next industrial revolution.” In addition, Caterpillar asserted that it is using its NVIDIA AI Factory to create safer, leaner, and more resilient manufacturing [https://www.therobotreport.com/category/markets-industries/manufacturing/] systems to meet evolving industry needs. The company said its digital data platform takes advantage of this accelerated AI infrastructure and NVIDIA AI libraries to automate and accelerate important production processes, including forecasting and scheduling. Caterpillar is also building physically accurate digital twins of its factories on NVIDIA Omniverse libraries and OpenUSD. With these digital twins, Caterpillar teams can design, simulate [https://www.therobotreport.com/category/software-simulation/], and optimize layouts and production processes before building in the real world. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- The post Caterpillar partners with NVIDIA to lay the foundation for autonomous systems [https://www.therobotreport.com/caterpillar-partners-with-nvidia-to-lay-the-foundation-for-autonomous-systems/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Caterpillar plans to make upgrades with NVIDIA to ensure that its assets are ready for AI-assisted and potentially autonomous operations.\nThe post Caterpillar partners with NVIDIA to lay the foundatio"
      },
      {
        "title": "Patents vs. trade secrets in the age of AI robotics",
        "link": "https://www.therobotreport.com/patents-vs-trade-secrets-in-the-age-of-ai-robotics/",
        "pubDate": "2026-01-14T18:44:37.000Z",
        "content": "[https://www.therobotreport.com/wp-content/uploads/2026/01/protecting-ai-driven-ip.jpg] This image was created using Google Gemini. It’s no surprise that artificial intelligence (AI) has become a go-to tool in the race to innovate, especially in the robotics industry. Whether it helps in the creation of new algorithms or novel innovations for automation, AI can be a creative partner, or in some instances, even contribute to the creation of inventive concepts. Given the prevalence of AI [https://www.therobotreport.com/category/design-development/ai-cognition/], robotics executives are grappling with the fundamental question: can you protect innovation not entirely created by humans? And if so, how? The answer to that question includes two distinct paths: patents or trade secrets. Choosing the right path in the age of AI has never mattered more. DETOUR AHEAD: PATENT ROUTE POSSIBLY CLOSED FOR AI-GENERATED INNOVATIONS For the robotics industry, patents can provide broad protection for innovative concepts and can cover almost any novel aspect of a technology, including hardware, software [https://www.therobotreport.com/category/software-simulation/], compositions, materials, processes, and business methods [https://www.therobotreport.com/category/markets-industries/]. Patents can also provide protection for improvements to existing technology — the innovation need not be radical or revolutionary in order to be patentable, merely new and not obvious. By protecting these innovative concepts, an emerging robotics company can generate significant competitive advantages for itself. A strong and robust patent portfolio can prevent competitors from making, using, selling or offering for sale your patented technology. In addition, patents can create tangible value for investors and partners, and can be out-licensed to generate an additional revenue source. A strong patent portfolio can often help attract investment dollars as venture capitalists and angel investors often look to see whether the company has implemented a strategy for protecting its intellectual property. A solid patent strategy can also minimize the risks associated with reverse-engineering by a competitor, a common occurrence in robotics, where hardware and firmware can often be deconstructed. However, in the U.S., the patent term is limited to 20 years from the effective filing date of the patent. Once the term expires, anyone, including your competitors, can legally copy or reproduce your innovation. Further, pursuing patent protection can be time consuming and expensive as the preparation and subsequent examination by the Patent Office, including potentially multiple rounds of prosecution, can present significant obstacles for the robotics company. Despite these issues, patents should be viewed as a long-term investment in the company’s assets, and the cost of filing is simply the cost of doing business. Importantly, however, patents also require that the inventor be human. Under the U.S. Patent Act, an “inventor” is defined as an individual, and the courts have interpreted this to mean a natural person. Recently, in Thaler v. Vidal [https://www.cafc.uscourts.gov/opinions-orders/21-2347.OPINION.8-5-2022_1988142.pdf] (2022), the Federal Circuit confirmed that an AI system cannot be named as an inventor under U.S. patent law. In other words, patent protection may not be an option if the AI is the sole inventor of your robotic design or process, absent significant human involvement. The U.S. Patent and Trademark Office recently further clarified this point noting that inventorship is determined by whether a natural person conceived the invention, not by the use of AI tools. AI systems are considered instruments, similar to laboratory equipment or software. If AI was involved, what are your options? TRADE SECRETS MAY BE AN ALTERNATIVE ROUTE FOR AI-GENERATED TECHNOLOGIES Trade secrets are an option for protecting AI-conceived technologies, especially those that may not qualify for patent protection. Indeed, certain robotics innovations – specifically those that can be kept secret, AI-conceived or otherwise – may be more effectively leveraged if held as trade secrets instead of being disclosed to the world through the patenting process. For robotics companies, this could include motion control algorithms [https://www.therobotreport.com/category/robot-components/motioncontrol/], machine learning models, data sets, or the processes your AI uses to generate optimized designs. Trade secret protection can provide, in many instances, a viable option to protect the IP assets of the company when used in conjunction with or as an alternative to patent protection. Unlike patents, there is no requirement under trade secret law that the source of the innovation be human. Thus, AI-conceived innovations are just as eligible for trade secrets as human derived innovations. The two requirements for trade secret protection are that the innovation must be kept secret and that the innovation provides value by being kept secret. Contrast this with patent protection, the innovation must be disclosed for the world to see. It should be noted that even if implemented in a product or made available for purchase on the open market, the technology can be held as a trade secret, so long it is not disclosed, cannot be discovered, or reverse-engineered. However, protection is lost as soon as a trade secret is divulged, whether intentionally or unintentionally. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- Because all that is needed to maintain a technology as a trade secret is ensure its secrecy, trade secrets avoid the effort and expense associated with pursuing patent protection. The requirement that the technology remain secret, however, can require continuous diligence and can present a challenge in trade secret protection. Therefore, it is critical that employees are educated on keeping information confidential, and that a corporate program is implemented to maintain secrecy of the company’s technology. Another advantage to pursuing trade secret protection is that it is oftentimes easier to obtain an injunction against misappropriation by a competitor. This is because courts might be more willing to issue a preliminary injunction in a trade secret case. Whereas for patent cases, in the absence of a prior favorable court finding of infringement and validity of the same patent, courts may not be as willing to issue preliminary injunctions, thus allowing wrongdoers to continue infringing. Because of the potential to prevent wrongdoers from benefiting from a misappropriated technology, trade secret protection may be considered as a viable alternative or complement to protecting owners of robotic technology against infringement. Despite clear benefits, the trade secret path does have inherent risks. Once the information becomes public or is",
        "excerpt": "Greenberg Traurig shares insights about how to choose the right IP strategy when algorithms, and not humans, drive innovation\nThe post Patents vs. trade secrets in the age of AI robotics appeared firs"
      },
      {
        "title": "CES 2026 robotics recap; industry experts make predictions",
        "link": "https://www.therobotreport.com/ces-2026-robotics-recap-industry-experts-make-predictions/",
        "pubDate": "2026-01-13T21:08:29.000Z",
        "content": "The Robot Report Podcast [https://soundcloud.com/robot-report-podcast] · CES 2026 robotics recap; 2026 predictions [https://soundcloud.com/robot-report-podcast/ces-2026-robotics-recap-2026-predictions] In Episode 226 of The Robot Report Podcast, hosts Steve Crowe and Mike Oitzman recap the major robotics news of the week. They discuss the latest robotics news and highlights from CES 2026. Steve attended the event, formerly the Consumer Electronics Show, and shares his assessment of products demonstrated last week in Las Vegas. Also on the podcast this week, we hear from a couple of industry leaders with their predictions for the year ahead. Chris Matthieu, vice president of the developer ecosystem at RealSense [https://www.therobotreport.com/tag/RealSense/], discusses the importance of vision, perception, and AI for robotics in 2026. Ahti Heinen, co-founder and CEO of Starship Technologies [https://www.therobotreport.com/tag/Starship-Technologies/], also shares his perspective about robotics this year. headshots of Chris Matthieu and Ahti Heinla. [https://www.therobotreport.com/wp-content/uploads/2026/01/chris-ahti-headshots.jpg] Chris Matthieu (L) and Ahti Heinla (R) offer their perspectives on the year ahead. Chris Matthieu [https://www.linkedin.com/in/chrismatthieu/] is a serial entrepreneur and active developer who has built and sold five companies in the areas of communications, IoT, and decentralized supercomputing. At RealSense, which spun out [https://www.therobotreport.com/intel-spins-out-realsense-as-standalone-company/] of Intel last year, he focuses on AI, robotics, and stereo depth technologies. Matthieu is a frequent speaker at robotics and emerging technology conferences. Ahti Heinla [https://www.linkedin.com/in/ahtiheinla/] is the co-founder and CEO of Starship Technologies, a leader in autonomous, AI-powered robots that conduct deliveries [https://www.therobotreport.com/tag/delivery-robots] in real-world environments. One of the original engineers behind Skype’s billion-dollar success, he later made a quiet pivot into robotics, spending the past decade advancing practical, consumer-facing AI. Under Heinla’s leadership, Starship has completed more than 9 million autonomous deliveries with a fleet of over 2,700 SAE [https://www.sae.org/news/blog/sae-levels-driving-automation-clarity-refinements] Level 4 robots navigating streets, sidewalks, weather, and people without human intervention. ---------------------------------------- SHOW TIMELINE * 5:21 – Chris Matthieu, VP, developer ecosystem, at RealSense * 9:04 – CES [https://www.therobotreport.com/tag/ces2026/] 2026 recap with editor Steve Crowe * 52:35 – Ahti Heinen, co-founder and CEO of Starship Technologies * 54:21 – News of the week ---------------------------------------- NEWS OF THE WEEK MOBILEYE TO ACQUIRE MENTEE ROBOTICS IN BID TO DOMINATE PHYSICAL AI [https://www.therobotreport.com/mobileye-to-acquire-mentee-robotics-for-900m-bid-dominate-physical-ai/] Mobileye, a leader in computer vision and machine learning technology for the automotive industry, announced plans to acquire Mentee Robotics, which is developing a vertically integrated humanoid robot, for $900 million. The proposed deal highlights a deep connection between the two organizations: Mobileye co-founder and CEO Amnon Shashua is also a co-founder of Mentee. Shashua has maintained a measured outlook regarding the go-to-market strategy for the MenteeBot system, citing the current developmental state of humanoid technology. According to the company’s roadmap, production is slated to begin in 2027 alongside partner Aumovio with the long-term goal of reaching household deployments by 2030. OSHKOSH ACQUIRES CORE TECHNOLOGY DEVELOPED BY CANVAS [https://www.therobotreport.com/oshkosh-acquires-core-technology-developed-by-canvas/] Meanwhile, Oshkosh [https://www.therobotreport.com/tag/Oshkosh/] has introduced and updated HARR-E — Hailable Autonomous Refuse Robot, Electric — a system designed for on-demand refuse collection that residents can summon via smartphone or virtual assistant. Canvas and Oshkosh cited their six-year partnership focusing on innovation across the robotics and automation sectors. Canvas previously developed the 1200CX, a worker-controlled robot capable of precisely spraying layers of joint compound onto walls in a single step. AMAZON ACQUIRES RIGHTBOT, ADDS TO DELIVERY AND PACKAGING INNOVATION TEAM [https://www.therobotreport.com/amazon-acquires-rightbot-adds-robotics-delivery-packaging-innovation-team/] Amazon has acquired Rightbot Technologies, which developed a robot for truck unloading. In 2023, Amazon’s Industrial Innovation Fund led an investment of $4 million into Rightbot as it emerged from stealth. There are a number of competitors in this space, including Boston Dynamics, Slip Robotics, Pickle Robot, Anyware Robotics, and Dexterity. It remains to be seen whether Amazon continues development of a competing system and how it incorporates Rightbot’s employees into its Robotics Delivery and Packaging Innovation team. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- The post CES 2026 robotics recap; industry experts make predictions [https://www.therobotreport.com/ces-2026-robotics-recap-industry-experts-make-predictions/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Catch up on CES 2026 robotics highlights; explore more 2026 predictions; and analyze major acquisitions by Mobileye, Oshkosh, and Amazon.\nThe post CES 2026 robotics recap; industry experts make predic"
      },
      {
        "title": "A3 releases full three-part national safety standard for industrial robots",
        "link": "https://www.therobotreport.com/now-available-full-403-page-ansi-a3-r15-06-2025-robot-safety-standard/",
        "pubDate": "2026-01-13T19:58:50.000Z",
        "content": "AI generated image of a production line with industrial robot workcells surrounded by guarding fences. [https://www.therobotreport.com/wp-content/uploads/2026/01/robots-production-AI-featured.jpg] The standard outlines all of the latest industrial robot workcells and application safety guarding requirements. | Image generated by Gemini AI The Association for Advancing Automation, or A3, today announced the full publication of the American National Standard for Industrial Robots and Robot Systems — Safety Requirements. Released on Oct. 29, 2025, the comprehensive 403-page document completes a three-part framework designed to govern the safe manufacture, integration, and use of industrial robotics. DOCUMENT INTEGRATES NATIONAL, INTERNATIONAL STANDARDS The new documentation [https://www.automate.org/store/products/ansi-a3-r15-06-2025-american-national-standard-for-industrial-robots-and-robot-systems-safety-requirements-pdf-download], categorized under reference numbers ANSI/A3 R15.06-2025 and ANSI/A3 R15.06-3-2025, combines three distinct parts under one cover to ensure they are used together. * Parts 1 and 2: Approved on Aug. 21, 2025, these sections serve as a revision of the previous ANSI/RIA R15.06-2012 standard [https://www.therobotreport.com/updated-ansi-a3-standards-address-industrial-robot-safety/]. They represent the U.S. national adoption of international standards ISO 10218-1:2025 (Ed. 3) and ISO 10218-2:2025 (Ed. 2) [https://www.therobotreport.com/iso-10218-industrial-robot-safety-standard-receives-major-overhaul/] * Part 3: Approved on Oct. 7, 2025, was developed with collaboration from standards [https://www.therobotreport.com/tag/standards] experts in the U.S. and Canada to address user requirements not covered by ISO standards. A3 UPDATES SAFETY REQUIREMENTS The standard provides updated guidelines across three specific areas: 1. Safety [https://www.therobotreport.com/tag/safety] requirements for industrial robots [https://www.therobotreport.com/category/robots-platforms/industrial-robots/] 2. Safety requirements for industrial robot applications and robot cells 3. Safety requirements for the use of industrial robot cells (new) The primary focus of the updated standard is the emphasis on risk assessment and the establishment of stringent personnel safety protocols, said Ann Arbor, Mich.-based A3 [https://www.therobotreport.com/tag/association-for-advancing-automation]. It provides a technical roadmap for manufacturers, system integrators, and end users to maintain safety in industrial environments. The complete three-part standard is currently available for purchase as a PDF download [https://www.automate.org/store/products/ansi-a3-r15-06-2025-american-national-standard-for-industrial-robots-and-robot-systems-safety-requirements-pdf-download] through the A3 online store. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- The post A3 releases full three-part national safety standard for industrial robots [https://www.therobotreport.com/now-available-full-403-page-ansi-a3-r15-06-2025-robot-safety-standard/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "A3 has published a comprehensive three-part national safety standard governing the manufacture, integration, and use of industrial robotics.\nThe post A3 releases full three-part national safety standa"
      },
      {
        "title": "X Square Robot secures $140M in funding for AI foundation models",
        "link": "https://www.therobotreport.com/x-square-robot-secures-140m-in-funding-for-ai-foundation-models/",
        "pubDate": "2026-01-13T19:36:55.000Z",
        "content": "The Quanta X2 semi-humanoid robot from X Square Robot. [https://www.therobotreport.com/wp-content/uploads/2026/01/X-square-quanta-x2.jpg] The Quanta X2 semi-humanoid robot Source: X Square Robot While artificial intelligence has advanced with large language models, or LLMs, robots need to understand the physical world to become general-purpose. X Square Robot yesterday said it has completed its Series A++ funding round, raising RMB 1 billion, about $140 million U.S. “At X Square, we believe the key to enabling robots to truly master real-world tasks lies in the ‘robot brain’ — a foundation model for the physical world that parallels virtual LLMs to shatter generalization bottlenecks,” stated Wang Qian, founder and CEO of X Square Robot. “This investment underscores shared confidence in our role as a catalyst for technological progress and will accelerate our expansion into high-value applications.” Founded in 2023, X Square Robot said it is developing “general-purpose, end-to-end embodied AI foundation models.” The Shenzhen, China-based company [https://www.x2robot.com/en], formally known as Variable Robotics Technology Co., added that its WALL-A system integrates vision-language-action (VLA) models with world models. WALL-A SUPPORTS CONTINUOUS REAL-WORLD EVOLUTION “By using world models to predict actions and causal inference to understand feedback, the model significantly enhances the zero-shot generalization capabilities of robots performing mobile manipulation [https://www.therobotreport.com/tag/mobile-manipulation/] tasks in unstructured environments,” said X Square. The company added that large-scale, real-robot reinforcement learning (RL) allows its foundation model to learn through physical interaction. It said this data-driven approach enables a robot to autonomously refine its skills and perform complex tasks in real-world environments. Alongside the development of WALL‑A, X Square Robot introduced [https://www.therobotreport.com/x-square-robot-debuts-foundation-model-embodied-ai-100m-series-a/] WALL‑OSS in September 2025. It said it designed the open‑source version of its model family to democratize embodied intelligence and accelerate community‑driven innovation. “The next phase of competition in embodied intelligence is essentially a battle of foundation models built on data closed loops and their capacity for model evolution,” noted Wang. To win this race, X Square Robot said it is developing a closed-loop iteration of hardware, data, and models. The company claimed that it is the first one in China [https://www.therobotreport.com/category/china] to scale up real-world data resources. X Square has developed advanced data-capture tools, including teleoperation [https://www.therobotreport.com/tag/teleoperation/], exoskeletons [https://www.therobotreport.com/tag/exoskeleton], and the Universal Manipulation Interface (UMI). The company has also established a data pipeline to generate high-quality data at scale. By using the foundation model to provide feedback on hardware design and data processing, X Square said it is improving acquisition efficiency and model performance, creating a self-reinforcing “flywheel.” QUANTA X1 DELIVERS FOOD USING FOUNDATION MODELS X Square Robot said the adaptability of its systems proves that embodied AI [https://www.therobotreport.com/category/design-development/ai-cognition/] has moved into practical, real-world deployment. The company recently demonstrated in autonomous food delivery in which its Quanta X1 wheeled bimanual robot, powered by WALL-A, successfully navigated indoor and outdoor tasks to complete a delivery [https://www.therobotreport.com/tag/delivery-robots] in an open environment. During the mission, Quanta X1 handled challenges such as strong winds, deformed packaging, and visual occlusions. Much like a human, the robot used the model’s causal inference to “fill in the blanks” when objects were partially hidden. When encountering operational stalls or friction, the robot autonomously self-corrected and completed the task loop without any human intervention. Quanta X1 also demonstrated this capacity in complex logistics. Facing piles of parcels [https://www.automatedwarehouseonline.com/tag/parcel], the robot used zero-shot generalization to identify irregular items. X Square noted that the model can unlock the potential of high DoF (degree-of-freedom) dexterous hands [https://www.therobotreport.com/category/technologies/grippers-end-effectors/], enabling robots to master human-like skills ranging from tool use to precise card dealing. This conquers the “last centimeter” of precision manipulation [https://www.x2robot.com/en/product/68ad9a319d9a1f1ecd40c31d], it said. X SQUARE GOES FROM FULL-STACK R&D TO MULTI-SCENARIO DEPLOYMENT X Square said its hardware architecture was driven by model and data requirements. It has released two wheeled robots: the Quanta X1 [https://www.x2robot.com/en/product/68ad78759d9a1f1ecd404a0e] and the Quanta X2 [https://www.x2robot.com/en/product/quantum2] semi-humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] robot. In addition, the company said it has developed and used core components including robotic arms, joint modules, and controllers [https://www.therobotreport.com/category/technologies/controllers/] to be ready for mass production and commercial scale. “X Square Robot continues to iterate across our three core pillars: models, data pipelines, and hardware,” said Wang. “By leveraging our technical depth and full-stack R&D, we consistently push the boundaries of robot performance.” The company asserted that its systems are driving “a new era where embodied intelligence powers every layer of productivity.” It cited increasing deployments of its technologies across key industries such as advanced manufacturing [https://www.therobotreport.com/category/markets-industries/manufacturing/], autonomous logistics [https://www.therobotreport.com/category/markets-industries/logistics-warehousing-asrs/], and senior healthcare [https://www.therobotreport.com/category/markets-industries/biotechnology-medical-healthcare/] services. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- STRATEGIC PARTNERS INVEST IN X SQUARE ROBOT ByteDance and HongShan, participated in X Square Robot’s latest investment, along with several other strategic Chinese partners. Previous rounds such as X Square’s $100 million Series A [https://www.therobotreport.com/x-square-robot-debuts-foundation-model-embodied-ai-100m-series-a/] in September included backers such as Alibaba Group and Meituan. The company said it has demonstrated “compelling model capabilities and product potential that continue to attract investor confidence.” “We’re honored to have the strong endorsement of our world-class strategic investors,” said Wang. The post X Square Robot secures $140M in funding for AI foundation models [https://www.therobotreport.com/x-square-robot-secures-140m-in-funding-for-ai-foundation-models/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "X Square Robot has raised $140 million to build the WALL-A model for general-purpose robots just four months after raising $100 million.\nThe post X Square Robot secures $140M in funding for AI foundat"
      },
      {
        "title": "4 physical AI predictions for 2026 — and beyond, from UR",
        "link": "https://www.therobotreport.com/four-physical-ai-predictions-2026-beyond-universal-robots/",
        "pubDate": "2026-01-13T14:49:51.000Z",
        "content": "Physical AI such as this force- and power-limited arm will get smarter thanks to math and collaboration, says a UR VP. [https://www.therobotreport.com/wp-content/uploads/2026/01/IMTS_AI_demo-7.jpg] Physical AI such as this force- and power-limited arm will get smarter thanks to math and collaboration, says a UR VP. Source: Universal Robots The robotics industry is evolving faster than ever, and the signals of what’s next are already visible. As someone focused on shaping the future of automation, I see four trends that will redefine how physical AI creates value. From smarter math and cooperative behaviors to industry-specific AI and a new data economy, here’s what I predict will matter most in the years ahead. 1. PREDICTIVE MATH IS A SILENT REVOLUTION FOR PHYSICAL AI The next big leap in robotics won’t come from hardware; it will come from math. Today, robots are reactive: They respond to inputs and adapt in real time. Tomorrow, they will anticipate. Robots like this cobot arm today learn tasks such as assembly via demonstration and reinforcement learning. [https://www.therobotreport.com/wp-content/uploads/2026/01/AICA_Reinforcement-Learning-Assembly_NVIDIA-GTC-2025-286x300.jpg] Robots learn tasks such as assembly via demonstration and reinforcement learning. Source: Universal Robots Emerging mathematical techniques, such as dual numbers and jets, are quietly reshaping how we think about modeling change. These tools allow systems to capture not just what happens when a robot moves, but also how those movements ripple through its entire environment. That means faster optimization, richer scenario planning, and adaptive control that feels almost intuitive. Imagine robots that could forecast the impact of a path adjustment before executing it or simulate multiple “what-if” scenarios in milliseconds. This isn’t science fiction. It’s a natural evolution of how we compute derivatives and predict system behavior. While these methods are still largely in research, their potential to transform robotics is undeniable. In my view, predictive intelligence will define the next generation of automation. The question isn’t whether this shift will happen but how soon and who will lead the way. 2. ROBOTS TO GO FROM SOLO TO SYNERGY Imitation learning will become a defining capability in the next wave of automation. Today, most robots operate as independent units, managed by centralized fleet systems or pre-programmed routines. Tomorrow, they will learn from each other and from humans — some guided, some autonomous – forming adaptive teams that share behaviors and strategies in real time. This evolution builds on research where robots not only follow a leader’s trajectory but also observe, imitate, and refine actions collaboratively, enabling dynamic coordination without rigid scripts. Industrial [https://www.therobotreport.com/category/robots-platforms/industrial-robots/] robotics vendors have laid the groundwork with fleet management and synchronized motion [https://www.therobotreport.com/category/robot-components/motioncontrol/] for multi-arm systems, but true peer-to-peer learning and self-organization are still emerging. However, I am certain that in 2026, we will see real deployments leveraging imitation-learned physical AI models. And the benefits are clear: * Faster configuration – and reconfiguration of workflows without complex programming * Improved resilience when conditions change unexpectedly * Natural human-robot [https://www.therobotreport.com/category/design-development/haptics/] collaboration, where robots intuitively follow human intent or a master robot’s pace As safety [https://www.therobotreport.com/tag/safety] standards [https://www.therobotreport.com/tag/standards], inter-robot communication, and orchestration tools mature, expect imitation-driven collaboration to move from niche pilots into widespread adoption across factories and warehouses [https://www.automatedwarehouseonline.com/]. This will transform robots from isolated units into cooperative, continuously learning teams. UR cobots work together, coordinated by advanced software. Physical AI will provide new capabilities, says UR. [https://www.therobotreport.com/wp-content/uploads/2026/01/Versandmanufaktur_UR.jpg] Software enables multiple robots to work together, but self-organization is still emerging. Source: Universal Robots 3. MANUFACTURERS TURN TO PURPOSE-BUILT AI Rather than generic AI [https://www.therobotreport.com/category/design-development/ai-cognition/] platforms, manufacturers [https://www.therobotreport.com/category/markets-industries/manufacturing/] will increasingly adopt task-specific AI built for a single process like welding, sanding, inspection, or assembly. Expect AI welding, AI finishing, AI assembly, and AI inspection [https://www.therobotreport.com/tag/inspection] to become standard features in new robotic cells, bringing automation to tasks once considered too variable or complex. These vertical applications will come out of the box pre-trained, pre-integrated, and ready to deliver measurable gains from Day 1. Welding [https://www.therobotreport.com/tag/welding/] is a flagship example with AI-driven capabilities like vision [https://www.therobotreport.com/category/technologies/cameras-imaging-vision/]-guided seam tracking and machine learning-assisted parameter optimization already transforming the trade of welding. The next frontier includes is complex, dexterous tasks such as assembly [https://www.therobotreport.com/tag/assembly], fastening, and intricate handling that have been traditionally resistant to automation. In industrial settings, AI will enable robots to manage variability in parts and processes, while in service industries, similar approaches will tackle tasks like packaging, sorting, and even delicate material handling. Logistics [https://www.therobotreport.com/category/markets-industries/logistics-warehousing-asrs/] is also an industry where we’ve seen great advancements, with AI-powered robotic systems now demonstrating the ability to perform complex pick, stow, and touch operations efficiently and at scale. In 2026, I anticipate we will also see investments spreading from logistics into retail [https://www.therobotreport.com/category/markets-industries/retail-wholesale]. This is especially exciting, as it marks another step in bringing robotic automation closer to our daily lives, and retail is an industry I will monitor closely. Siemens' SIMATIC Robot Pick AI, a pre-trained, deep learning-based vision software, uses Universal Robots to perform tasks previously limited to manual intervention. Here is the physical AI system in action for intralogistics technology company Mecalux. [https://www.therobotreport.com/wp-content/uploads/2026/01/Siemens_UR_-Zivid-_fulfillment.jpg] Siemens’ SIMATIC Robot Pick AI, a pre-trained, deep learning-based vision software, uses UR to automate tasks for intralogistics technology company Mecalux. Source: Universal Robots 4. DATA FROM PHYSICAL AI IS THE NEW FUEL The next big shift won’t just be in how robots move or think, it will be in how their data creates value. Today, most of the rich information robots generate — sensor readings, vision frames, force profiles — stays on the edge, inside the customer’s site. That’s great for privacy and speed, but it means AI developers often lack the real-world data they need to build smarter applications. A UR8 Long robot arm in a Hirebotics welding cell. [https://www.therobotreport.com/wp-content/uploads/2026/01/UR8-Long_Hirebotics_welding-244x300.jpg] A UR8 Long robot arm in a Hirebotics welding cell. Source: Universal Robots In the future, I see robot manufacturers creating secure, opt-in data exchanges. With customer consent and strong privacy safeguards, anonymized performance data could be aggregated and offered to AI developers as training sets or model services. Imagine welding robots sharing de-identified seam quality metrics,",
        "excerpt": "Trends such as industry-specific AI and a new data economy will affect physical AI in 2026, says a Universal Robots executive.\nThe post 4 physical AI predictions for 2026 — and beyond, from UR appeare"
      },
      {
        "title": "Schaeffler to deploy hundreds of Humanoid robots in its factories",
        "link": "https://www.therobotreport.com/schaeffler-humanoid-partner-build-deploy-hundreds-robots/",
        "pubDate": "2026-01-13T10:00:40.000Z",
        "content": "Schaeffler plans to deploy robots from Humanoid across its global factories, as seen here. [https://www.therobotreport.com/wp-content/uploads/2026/01/Schaeffler_Humanoids-in-Global-Factories.jpg] Schaeffler plans to deploy robots from Humanoid across its global factories. Source: Humanoid While humanoid robotics developers have gotten a lot of public and investor interest, many still need partners for the technologies that go into their systems. SKL Robotics Ltd., which does business as Humanoid, today announced a strategic partnership with motion technology company Schaeffler Technologies AG. “Real-world integration is the ultimate test for humanoid robots,” said Artem Sokolov, founder of Humanoid. “Moving from pilots to large-scale deployment requires close alignment on systems, data, safety, and operations with our partners.” “Our collaboration with Schaeffler allows us to validate and scale humanoid deployment with clear requirements, measurable outcomes, and long-term plans,” he added. “Together, we are laying the groundwork for humanoid robots to become a practical and reliable part of industrial environments within just a few years.” Sokolov founded Humanoid in 2024, and the company [https://www.therobotreport.com/tag/humanoid/] has more than 200 engineers, researchers, and innovators from top global tech companies. With offices in London, Boston, and Vancouver, Humanoid said it is building commercially viable, scalable, and safe robots for real-world applications. Humanoid recently noted that it went from a concept to a functional prototype of its HMND 01 mobile manipulator [https://www.therobotreport.com/tag/mobile-manipulation] in just seven months [https://www.therobotreport.com/humanoid-takes-seven-month-path-to-hmnd-01-alpha/]. The company’s HMND 01 Alpha Bipedal model achieved [https://www.therobotreport.com/humanoid-sayss-first-bipedal-robot-can-start-walking-just-48-hours-after-assembly/] stable locomotion only 48 hours after final assembly. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- SCHAEFFLER TO BE PREFERRED PARTNER FOR ACTUATORS “As a motion technology company, we want to play a key role in the growing humanoid robotics market,” stated Klaus Rosenfeld, CEO of Schaeffler AG. “To do so, we are relying on our decades-long manufacturing excellence and industrialization expertise. With Humanoid, we are gaining an attractive partner in Europe that will enable us to drive forward joint innovations in the field of humanoid robotics.” Under the new agreement, the companies will also strengthen cooperation on the supply and integration of actuator [https://www.therobotreport.com/category/actuators-motors-servos/] components for Humanoid’s robotic platforms. They will explore the joint development of next-generation actuators. Schaeffler said it will become Humanoid’s preferred supplier for joint actuators used in its wheeled platform. Schaeffler’s planetary gear actuator is a highly efficient drive system that enables precise and energy-efficient motion sequences. [https://www.therobotreport.com/wp-content/uploads/2026/01/Schaeffler_gear-289x300.jpg] Schaeffler’s planetary gear actuator is designed to enable precise and energy-efficient motion sequences. Source: Schaeffler In addition, the companies plan to collaborate on data collection and skill development for humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] robots. This includes gathering robot-specific and use case-driven data through teleoperation [https://www.therobotreport.com/tag/teleoperation/], synthetic data generation, and peripheral data. Schaeffler said it will use the information to train AI [https://www.therobotreport.com/category/design-development/ai-cognition/] models and continuously improve robot performance, with skills tailored to its own operational needs. The Herzogenaurach, Germany-based company [https://www.therobotreport.com/tag/schaeffler/] has also worked with [https://www.therobotreport.com/schaeffler-plans-global-use-agility-robotics-digit-humanoid/] Agility Robotics. At CES [https://www.therobotreport.com/tag/ces/] last week, Schaeffler showed its motion-control [https://www.therobotreport.com/category/robot-components/motioncontrol/] systems, including a new planetary gear actuator [https://www.schaeffler.com/en/media/press-releases/press-releases-detail.jsp?id=88156672] for humanoid robots. The company said its new system combines a two-stage planetary gearbox, electric motor, encoder, and controller in a compact unit for precise movement and continuous operation. HUMANOID TO DEPLOY HUNDREDS OF ROBOTS INTO PRODUCTION Over the next five years, the companies agreed to bring hundreds of Humanoid’s robots into Schaeffler’s production facilities, fostering further industrial automation. Beyond deployment, the partnership covers actuator supply, data collection, skill development, and other critical areas. Initial deployments are scheduled to begin with beta-stage robots in 2026 and 2027. This phase will focus on validating technical integration; operational performance; and compliance with Schaeffler’s system, safety, IT, and security requirements. The companies said they will measure success across performance, efficiency, availability, reliability, serviceability, and ease of integration. Following validation, Humanoid plans to offer its products under robot-as-a-service (RaaS [https://www.automatedwarehouseonline.com/category/raas/]) models for the gamma phase, with both RaaS and CapEx (capital expenditure) options available for later stages. The post Schaeffler to deploy hundreds of Humanoid robots in its factories [https://www.therobotreport.com/schaeffler-humanoid-partner-build-deploy-hundreds-robots/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Schaeffler will provide actuators for Humanoid's systems, which will be available through a robotics-as-a-service model.\nThe post Schaeffler to deploy hundreds of Humanoid robots in its factories appe"
      },
      {
        "title": "TESOLLO uses own actuator in DG-5F-S humanoid robotic hand",
        "link": "https://www.therobotreport.com/tesollo-uses-own-actuator-dg-5f-s-humanoid-robotic-hand/",
        "pubDate": "2026-01-12T22:28:25.000Z",
        "content": "The TESOLLO DG-5F-S robotic hand. [https://www.therobotreport.com/wp-content/uploads/2026/01/DG-5F-S_Compact_Humanoid_Hand_Image1.jpg] The DG-5F-S robotic hand. Source: TESOLLO TESOLLO Inc. this month launched its new humanoid robotic hand, the DG-5F-S. The company said the latest model builds on its proprietary actuator technology, resulting in a more compact and lightweight design than its predecessor while maintaining the core structure. “Since 2023, TESOLLO has been developing proprietary actuators optimized for humanoid robotic hands, moving away from the use of generic actuators,” stated Youngjin Kim, CEO of TESOLLO. “Our focus has been on achieving high torque density, durability, and maintainability.” “The DG-5F-S represents a significant advancement of that technology, achieving compactness and lightweight design while enhancing compatibility with various humanoid platforms,” he added. Founded in 2019, TESOLLO said its name is a combination of “technology” and “sole,” representing its commitment to using technology to open new horizons in the robotic automation market. The Incheon, South Korea-based company [https://www.therobotreport.com/tag/tesollo/] was an exhibitor [https://exhibitors.ces.tech/8_0/exhibitor/exhibitor-details.cfm?exhid=001Pp000003RdUzIAK] at CES [https://www.therobotreport.com/tag/ces/] last week. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- NEW MODEL BUILDS ON DG-5F ROBOT HAND Like TESOLLO’s flagship model DG-5F [https://en.tesollo.com/dg-5f-m/], the DG-5F-S features a five-finger, 20 degrees-of-freedom (DoF) configuration. Each finger has four independently driven joints copying the structure of a human hand. This design enables the precise manipulation and dexterity required for humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] robots, the company asserted. “Moving away from the conventional approach of applying widely used general-purpose actuators, we have been developing dedicated actuators optimized for humanoid robot hands in-house since 2023,” Youngjin Kim said. The DG-5F debuted at the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) in 2024. Since then, major Korean and global technology companies have adopted the gripper [https://www.therobotreport.com/category/technologies/grippers-end-effectors/], expanding its use cases across industries. Now exported to 16 countries, the DG-5F has proven both its technical excellence and market viability, said TESOLLO. The company said it built on that success in developing the DG-5F-S, which is optimized for integration into humanoid platforms through miniaturization and weight reduction. Weighing under 1 kg (2.2 lb.) and sized comparably to a human hand, the DG-5F-S is compatible with a wide range of humanoid platforms, TESOLLO said. TESOLLO has designed its robot hands to be compatible with humanoids. [https://www.therobotreport.com/wp-content/uploads/2026/01/DG-5F-S_Compact_Humanoid_Hand_Image2.jpg] TESOLLO has designed its robot hands to be compatible with a range of humanoids. Source: TESOLLO TESOLLO OFFERS AN AFFORDABLE, ACCURATE OPTION TESOLLO said the price of the DG-5F-S is about 60% of that of the DG-5F, providing a more accessible option for startups, research institutions, and small to midsize companies. The DG-5F-S employs a direct-drive mechanism, minimizing backlash and enabling high positional accuracy and intuitive control, the company [https://en.tesollo.com/] explained. TESOLLO said its gripping and manipulation algorithms allow for stable handling of objects with diverse shapes and materials. In addition, the new robotic hand includes support for industry-standard communication protocols to enhance user convenience in real-world environments, according to the company. TESOLLO offers a variety of customization options, including tactile [https://www.therobotreport.com/category/design-development/haptics/] sensor [https://www.therobotreport.com/category/technologies/sensors-sensing/] integration, waterproof covers, and manipulation algorithm tuning. TESOLLO said they make the DG-5F-S suitable for both research and industrial applications. With the DG-5F-S, TESOLLO aims to accelerate the adoption of humanoid robot hands and expand real-world applications. It described the new end effector as “a key platform driving the transition of humanoid robots from research and prototyping to commercialization, further solidifying TESOLLO’s position in the global robotics market.” After the prototype debut at CES, TESOLLO plans to officially launch the DG-5F-S in the first half of 2026. The post TESOLLO uses own actuator in DG-5F-S humanoid robotic hand [https://www.therobotreport.com/tesollo-uses-own-actuator-dg-5f-s-humanoid-robotic-hand/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "TESOLLO said the technology it developed in house enables a smaller and lighter 20-DoF robot hand.\nThe post TESOLLO uses own actuator in DG-5F-S humanoid robotic hand appeared first on The Robot Repor"
      },
      {
        "title": "AGIBOT makes its U.S. debut with more than 5,100 robots shipped",
        "link": "https://www.therobotreport.com/agibot-makes-u-s-debut-with-more-than-5100-robots-shipped/",
        "pubDate": "2026-01-12T18:24:51.000Z",
        "content": "The AGIBOT A2 on stage at CES 2026 doing a Tai Chi performance. [https://www.therobotreport.com/wp-content/uploads/2026/01/agibot-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/agibot-featured.jpg The AGIBOT A2 on stage at CES 2026, doing a Tai Chi performance. | Source: AGIBOT AGIBOT Innovation Technology Co. last week made its U.S. debut at CES 2026. The Shanghai, China-based company also announced it was ranked No. 1 globally in both humanoid robot shipment volume and market share in 2025, according to the latest report released by Omdia. Omdia wrote [https://drive.google.com/file/d/1Y67UM23sUW5Wly1A49DdIxHcXB1JFfqD/view] that AGIBOT shipped more than 5,100 robots during the year. The company [https://www.therobotreport.com/tag/agibot/] has captured 39% of global market share, it said. “Bringing our full robotics portfolio to CES marks a defining moment for AGIBOT,” said Dr. Yao Maoqing, partner, senior vice president, and president of the Embodied Business Unit at AGIBOT. “It demonstrates how we are able to build an ecosystem of humanoid robots, not for a single task or setting, but for a future where embodied intelligence can serve people across industries, environments, and everyday life.” Founded in 2023, the company [https://www.agibot.com/] offers a range of humanoid robots and mobile robots [https://www.therobotreport.com/category/robots-platforms/amrs/] for cleaning [https://www.therobotreport.com/tag/cleaning/] and service [https://www.therobotreport.com/tag/service-robots] settings. In December, AGIBOT released [https://www.prnewswire.com/news-releases/agibot-makes-debut-at-fortune-event-with-full-size-humanoid-robot-agibot-a2-as-special-guest-302630428.html] its updated humanoid, AGIBOT A2. Later that month, the company rolled out [https://www.prnewswire.com/apac/news-releases/agibot-announces-the-rollout-of-its-5-000th-mass-produced-humanoid-robot-302635107.html] its 5,000th mass-produced robot at its factory. Also at CES [https://www.therobotreport.com/tag/ces/], AGIBOT introduced [https://www.therobotreport.com/agibot-launches-genie-sim-3-0-robot-simulation-platform/] Genie Sim 3.0, a robot simulation platform powered by NVIDIA Isaac Sim. The company said it delivers a unified, open simulation [https://www.therobotreport.com/category/software-simulation/] workflow. The platform brings together digital asset generation, scene generalization, data collection, automated evaluation, and physics-based simulation in a single toolchain. Last week, AgiBot Research introduced [https://agibot.com/research/sop_en] the SOP (scalable online post-training) framework, which it said is designed to enable online updates of vision-language-action (VLA) models across robot fleets. It said the framework shifts the learning paradigm from offline to distributed online training and can lead to significant performance improvements on the path toward general-purpose robots. OMDIA LOOKS AT AGIBOT AND GLOBAL HUMANOID MARKET Robotic shipments market share by vendor, according to Omedia's data. [https://www.therobotreport.com/wp-content/uploads/2026/01/omediachart.png]https://www.therobotreport.com/wp-content/uploads/2026/01/omediachart.png Robotic shipments market share by vendor, according to Omdia’s data. | Source: AGIBOT Omdia [https://omdia.tech.informa.com/] reported that the global humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] market entered a phase of rapid growth in 2025. Total annual shipments reached approximately 13,000 units, the organization claimed. AGIBOT shipped more than 5,100 humanoid robots during the year, according to Omdia. Its product portfolio spans full-sized humanoids, compact half-sized humanoids, and wheeled robots. However, that number also includes the number of mobile manipulators [https://www.therobotreport.com/tag/mobile-manipulation] the company has shipped. In December, AGIBOT said it had produced [https://www.prnewswire.com/apac/news-releases/agibot-announces-the-rollout-of-its-5-000th-mass-produced-humanoid-robot-302635107.html] 1,742 units of its A-Series robots and 1,846 units of its X-Series, both of which are bipedal. In addition, the company has manufactured 1,412 units of its G-Series, which are mobile manipulators. This means AGIBOT has shipped 3,588 humanoids. Omdia’s report did not go into detail about whether it included dual-armed mobile manipulators as “humanoids” in its report. HUMANOID MARKET HAS ROOM TO GROW So far, humanoids have been commercially deployed across a wide range of scenarios, said Omida. These include reception and hospitality, entertainment and commercial performances, industrial intelligent manufacturing, logistics sorting, security inspection and patrol, data collection and training, scientific research, and education. Omdia predicted exponential growth for the humanoid robot market over the next decade, projecting global annual shipments to reach 2.6 million units by 2035. Other research firms, such as Interact Analysis [https://www.therobotreport.com/tag/interact-analysis], have much more conservative estimates for humanoid robots in the years to come. That company found [https://www.therobotreport.com/despite-the-hype-interact-analysis-expects-humanoid-adoption-to-remain-slow/] that, despite the hype and significant investment activity around humanoids, market growth will be relatively slow, reaching over 40,000 units by 2032 with a total market revenue of about $2 billion. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- The post AGIBOT makes its U.S. debut with more than 5,100 robots shipped [https://www.therobotreport.com/agibot-makes-u-s-debut-with-more-than-5100-robots-shipped/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "A recent report from Omdia sheds light on the wider humanoid robot market and where AGIBOT fits into it. \nThe post AGIBOT makes its U.S. debut with more than 5,100 robots shipped appeared first on The"
      }
    ],
    "arxiv": [
      {
        "id": "2601.09838",
        "title": "Interprofessional and Agile Development of Mobirobot: A Socially Assistive Robot for Pediatric Therapy Across Clinical and Therapeutic Settings",
        "authors": [
          "Leonie Dyck",
          "Aiko Galetzka",
          "Maximilian Noller",
          "Anna-Lena Rinke",
          "Jutta Bormann",
          "Jekaterina Miller",
          "Michelle Hochbaum",
          "Julia Siemann",
          "Jördis Alboth",
          "Andre Berwinkel",
          "Johanna Luz",
          "Britta Kley-Zobel",
          "Marcine Cyrys",
          "Nora Flöttmann",
          "Ariane Vogeler",
          "Mariia Melnikova",
          "Ira-Katharina Petras",
          "Michael Siniatchkin",
          "Winfried Barthlen",
          "Anna-Lisa Vollmer"
        ],
        "abstract": "Title:\n          Interprofessional and Agile Development of Mobirobot: A Socially Assistive Robot for Pediatric Therapy Across Clinical and Therapeutic Settings\n        Comments:\n          submitted to Frontiers in Digital Health\n        \n          Introduction: Socially assistive robots hold promise for enhancing therapeutic engagement in paediatric clinical settings. However, their successful implementation requires not only technical robustness but also context-sensitive, co-designed solutions. This paper presents Mobirobot, a socially assistive robot developed to support mobilisation in children recovering from trauma, fractures, or depressive disorders through personalised exercise programmes.\nMethods: An agile, human-centred development approach guided the iterative design of Mobirobot. Multidisciplinary clinical teams and end users were involved throughout the co-development process, which focused on early integration into real-world paediatric surgical and psychiatric settings. The robot, based on the NAO platform, features a simple setup, adaptable exercise routines with interactive guidance, motivational dialogue, and a graphical user interface (GUI) for monitoring and no-code system feedback.\nResults: Deployment in hospital environments enabled the identification of key design requirements and usability constraints. Stakeholder feedback led to refinements in interaction design, movement capabilities, and technical configuration. A feasibility study is currently underway to assess acceptance, usability, and perceived therapeutic benefit, with data collection including questionnaires, behavioural observations, and staff-patient interviews.\nDiscussion: Mobirobot demonstrates how multiprofessional, stakeholder-led development can yield a socially assistive system suited for dynamic inpatient settings. Early-stage findings underscore the importance of contextual integration, robustness, and minimal-intrusion design. While challenges such as sensor limitations and patient recruitment remain, the platform offers a promising foundation for further research and clinical application.",
        "link": "https://arxiv.org/abs/2601.09838",
        "submittedDate": "2026-01-17T09:31:14.847Z"
      },
      {
        "id": "2601.09856",
        "title": "How Human Motion Prediction Quality Shapes Social Robot Navigation Performance in Constrained Spaces",
        "authors": [
          "Andrew Stratton",
          "Phani Teja Singamaneni",
          "Pranav Goyal",
          "Rachid Alami",
          "Christoforos Mavrogiannis"
        ],
        "abstract": "Title:\n          How Human Motion Prediction Quality Shapes Social Robot Navigation Performance in Constrained Spaces\n        \n          Motivated by the vision of integrating mobile robots closer to humans in warehouses, hospitals, manufacturing plants, and the home, we focus on robot navigation in dynamic and spatially constrained environments. Ensuring human safety, comfort, and efficiency in such settings requires that robots are endowed with a model of how humans move around them. Human motion prediction around robots is especially challenging due to the stochasticity of human behavior, differences in user preferences, and data scarcity. In this work, we perform a methodical investigation of the effects of human motion prediction quality on robot navigation performance, as well as human productivity and impressions. We design a scenario involving robot navigation among two human subjects in a constrained workspace and instantiate it in a user study ($N=80$) involving two different robot platforms, conducted across two sites from different world regions. Key findings include evidence that: 1) the widely adopted average displacement error is not a reliable predictor of robot navigation performance and human impressions; 2) the common assumption of human cooperation breaks down in constrained environments, with users often not reciprocating robot cooperation, and causing performance degradations; 3) more efficient robot navigation often comes at the expense of human efficiency and comfort.",
        "link": "https://arxiv.org/abs/2601.09856",
        "submittedDate": "2026-01-17T09:31:14.848Z"
      },
      {
        "id": "2601.09920",
        "title": "SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Grasping",
        "authors": [
          "Ruopeng Huang",
          "Boyu Yang",
          "Wenlong Gui",
          "Jeremy Morgan",
          "Erdem Biyik",
          "Jiachen Li"
        ],
        "abstract": "Title:\n          SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Grasping\n        \n          Accurate and safe grasping under dynamic and visually occluded conditions remains a core challenge in real-world robotic manipulation. We present SyncTwin, a digital twin framework that unifies fast 3D scene reconstruction and real-to-sim synchronization for robust and safety-aware grasping in such environments. In the offline stage, we employ VGGT to rapidly reconstruct object-level 3D assets from RGB images, forming a reusable geometry library for simulation. During execution, SyncTwin continuously synchronizes the digital twin by tracking real-world object states via point cloud segmentation updates and aligning them through colored-ICP registration. The updated twin enables motion planners to compute collision-free and dynamically feasible trajectories in simulation, which are safely executed on the real robot through a closed real-to-sim-to-real loop. Experiments in dynamic and occluded scenes show that SyncTwin improves grasp accuracy and motion safety, demonstrating the effectiveness of digital-twin synchronization for real-world robotic execution.",
        "link": "https://arxiv.org/abs/2601.09920",
        "submittedDate": "2026-01-17T09:31:14.848Z"
      },
      {
        "id": "2601.10116",
        "title": "CoCoPlan: Adaptive Coordination and Communication for Multi-robot Systems in Dynamic and Unknown Environments",
        "authors": [
          "Xintong Zhang",
          "Junfeng Chen",
          "Yuxiao Zhu",
          "Bing Luo",
          "Meng Guo"
        ],
        "abstract": "Title:\n          CoCoPlan: Adaptive Coordination and Communication for Multi-robot Systems in Dynamic and Unknown Environments\n        Comments:\n          8 pages, 8 figures, published to RA-L\n        \n          Multi-robot systems can greatly enhance efficiency through coordination and collaboration, yet in practice, full-time communication is rarely available and interactions are constrained to close-range exchanges. Existing methods either maintain all-time connectivity, rely on fixed schedules, or adopt pairwise protocols, but none adapt effectively to dynamic spatio-temporal task distributions under limited communication, resulting in suboptimal coordination. To address this gap, we propose CoCoPlan, a unified framework that co-optimizes collaborative task planning and team-wise intermittent communication. Our approach integrates a branch-and-bound architecture that jointly encodes task assignments and communication events, an adaptive objective function that balances task efficiency against communication latency, and a communication event optimization module that strategically determines when, where and how the global connectivity should be re-established. Extensive experiments demonstrate that it outperforms state-of-the-art methods by achieving a 22.4% higher task completion rate, reducing communication overhead by 58.6%, and improving the scalability by supporting up to 100 robots in dynamic environments. Hardware experiments include the complex 2D office environment and large-scale 3D disaster-response scenario.",
        "link": "https://arxiv.org/abs/2601.10116",
        "submittedDate": "2026-01-17T09:31:14.848Z"
      },
      {
        "id": "2601.10233",
        "title": "Proactive Local-Minima-Free Robot Navigation: Blending Motion Prediction with Safe Control",
        "authors": [
          "Yifan Xue",
          "Ze Zhang",
          "Knut Åkesson",
          "Nadia Figueroa"
        ],
        "abstract": "Title:\n          Proactive Local-Minima-Free Robot Navigation: Blending Motion Prediction with Safe Control\n        Comments:\n          Co-first authors: Yifan Xue and Ze Zhang\n        \n          This work addresses the challenge of safe and efficient mobile robot navigation in complex dynamic environments with concave moving obstacles. Reactive safe controllers like Control Barrier Functions (CBFs) design obstacle avoidance strategies based only on the current states of the obstacles, risking future collisions. To alleviate this problem, we use Gaussian processes to learn barrier functions online from multimodal motion predictions of obstacles generated by neural networks trained with energy-based learning. The learned barrier functions are then fed into quadratic programs using modulated CBFs (MCBFs), a local-minimum-free version of CBFs, to achieve safe and efficient navigation. The proposed framework makes two key contributions. First, it develops a prediction-to-barrier function online learning pipeline. Second, it introduces an autonomous parameter tuning algorithm that adapts MCBFs to deforming, prediction-based barrier functions. The framework is evaluated in both simulations and real-world experiments, consistently outperforming baselines and demonstrating superior safety and efficiency in crowded dynamic environments.",
        "link": "https://arxiv.org/abs/2601.10233",
        "submittedDate": "2026-01-17T09:31:14.848Z"
      },
      {
        "id": "2601.10340",
        "title": "CHORAL: Traversal-Aware Planning for Safe and Efficient Heterogeneous Multi-Robot Routing",
        "authors": [
          "David Morilla-Cabello",
          "Eduardo Montijano"
        ],
        "abstract": "Title:\n          CHORAL: Traversal-Aware Planning for Safe and Efficient Heterogeneous Multi-Robot Routing\n        \n          Monitoring large, unknown, and complex environments with autonomous robots poses significant navigation challenges, where deploying teams of heterogeneous robots with complementary capabilities can substantially improve both mission performance and feasibility. However, effectively modeling how different robotic platforms interact with the environment requires rich, semantic scene understanding. Despite this, existing approaches often assume homogeneous robot teams or focus on discrete task compatibility rather than continuous routing. Consequently, scene understanding is not fully integrated into routing decisions, limiting their ability to adapt to the environment and to leverage each robot's strengths. In this paper, we propose an integrated semantic-aware framework for coordinating heterogeneous robots. Starting from a reconnaissance flight, we build a metric-semantic map using open-vocabulary vision models and use it to identify regions requiring closer inspection and capability-aware paths for each platform to reach them. These are then incorporated into a heterogeneous vehicle routing formulation that jointly assigns inspection tasks and computes robot trajectories. Experiments in simulation and in a real inspection mission with three robotic platforms demonstrate the effectiveness of our approach in planning safer and more efficient routes by explicitly accounting for each platform's navigation capabilities. We release our framework, CHORAL, as open source to support reproducibility and deployment of diverse robot teams.",
        "link": "https://arxiv.org/abs/2601.10340",
        "submittedDate": "2026-01-17T09:31:14.849Z"
      },
      {
        "id": "2601.10365",
        "title": "FastStair: Learning to Run Up Stairs with Humanoid Robots",
        "authors": [
          "Yan Liu",
          "Tao Yu",
          "Haolin Song",
          "Hongbo Zhu",
          "Nianzong Hu",
          "Yuzhi Hao",
          "Xiuyong Yao",
          "Xizhe Zang",
          "Hua Chen",
          "Jie Zhao"
        ],
        "abstract": "Title:\n          FastStair: Learning to Run Up Stairs with Humanoid Robots\n        \n          Running up stairs is effortless for humans but remains extremely challenging for humanoid robots due to the simultaneous requirements of high agility and strict stability. Model-free reinforcement learning (RL) can generate dynamic locomotion, yet implicit stability rewards and heavy reliance on task-specific reward shaping tend to result in unsafe behaviors, especially on stairs; conversely, model-based foothold planners encode contact feasibility and stability structure, but enforcing their hard constraints often induces conservative motion that limits speed. We present FastStair, a planner-guided, multi-stage learning framework that reconciles these complementary strengths to achieve fast and stable stair ascent. FastStair integrates a parallel model-based foothold planner into the RL training loop to bias exploration toward dynamically feasible contacts and to pretrain a safety-focused base policy. To mitigate planner-induced conservatism and the discrepancy between low- and high-speed action distributions, the base policy was fine-tuned into speed-specialized experts and then integrated via Low-Rank Adaptation (LoRA) to enable smooth operation across the full commanded-speed range. We deploy the resulting controller on the Oli humanoid robot, achieving stable stair ascent at commanded speeds up to 1.65 m/s and traversing a 33-step spiral staircase (17 cm rise per step) in 12 s, demonstrating robust high-speed performance on long staircases. Notably, the proposed approach served as the champion solution in the Canton Tower Robot Run Up Competition.",
        "link": "https://arxiv.org/abs/2601.10365",
        "submittedDate": "2026-01-17T09:31:14.849Z"
      },
      {
        "id": "2601.09755",
        "title": "Heterogeneous computing platform for real-time robotics",
        "authors": [
          "Jakub Fil",
          "Yulia Sandamirskaya",
          "Hector Gonzalez",
          "Loïc Azzalin",
          "Stefan Glüge",
          "Lukas Friedenstab",
          "Friedrich Wolf",
          "Tim Rosmeisl",
          "Matthias Lohrmann",
          "Mahmoud Akl",
          "Khaleel Khan",
          "Leonie Wolf",
          "Kristin Richter",
          "Holm Puder",
          "Mazhar Ali Bari",
          "Xuan Choo",
          "Noha Alharthi",
          "Michael Hopkins",
          "Mansoor Hanif Christian Mayr",
          "Jens Struckmeier",
          "Steve Furber"
        ],
        "abstract": "Title:\n          Heterogeneous computing platform for real-time robotics\n        \n          After Industry 4.0 has embraced tight integration between machinery (OT), software (IT), and the Internet, creating a web of sensors, data, and algorithms in service of efficient and reliable production, a new concept of Society 5.0 is emerging, in which infrastructure of a city will be instrumented to increase reliability, efficiency, and safety. Robotics will play a pivotal role in enabling this vision that is pioneered by the NEOM initiative - a smart city, co-inhabited by humans and robots. In this paper we explore the computing platform that will be required to enable this vision. We show how we can combine neuromorphic computing hardware, exemplified by the Loihi2 processor used in conjunction with event-based cameras, for sensing and real-time perception and interaction with a local AI compute cluster (GPUs) for high-level language processing, cognition, and task planning. We demonstrate the use of this hybrid computing architecture in an interactive task, in which a humanoid robot plays a musical instrument with a human. Central to our design is the efficient and seamless integration of disparate components, ensuring that the synergy between software and hardware maximizes overall performance and responsiveness. Our proposed system architecture underscores the potential of heterogeneous computing architectures in advancing robotic autonomy and interactive intelligence, pointing toward a future where such integrated systems become the norm in complex, real-time applications.",
        "link": "https://arxiv.org/abs/2601.09755",
        "submittedDate": "2026-01-17T09:31:14.849Z"
      },
      {
        "id": "2503.01238",
        "title": "A Taxonomy for Evaluating Generalist Robot Manipulation Policies",
        "authors": [
          "Jensen Gao",
          "Suneel Belkhale",
          "Sudeep Dasari",
          "Ashwin Balakrishna",
          "Dhruv Shah",
          "Dorsa Sadigh"
        ],
        "abstract": "Title:\n          A Taxonomy for Evaluating Generalist Robot Manipulation Policies\n        Comments:\n          IEEE Robotics and Automation Letters (RA-L)\n        \n          Machine learning for robot manipulation promises to unlock generalization to novel tasks and environments. But how should we measure the progress of these policies towards generalization? Evaluating and quantifying generalization is the Wild West of modern robotics, with each work proposing and measuring different types of generalization in their own, often difficult to reproduce settings. In this work, our goal is (1) to outline the forms of generalization we believe are important for robot manipulation in a comprehensive and fine-grained manner, and (2) to provide reproducible guidelines for measuring these notions of generalization. We first propose STAR-Gen, a taxonomy of generalization for robot manipulation structured around visual, semantic, and behavioral generalization. Next, we instantiate STAR-Gen with two case studies on real-world benchmarking: one based on open-source models and the Bridge V2 dataset, and another based on the bimanual ALOHA 2 platform that covers more dexterous and longer horizon tasks. Our case studies reveal many interesting insights: for example, we observe that open-source vision-language-action models often struggle with semantic generalization, despite pre-training on internet-scale language datasets. We provide videos and other supplementary material at our website this http URL.",
        "link": "https://arxiv.org/abs/2503.01238",
        "submittedDate": "2026-01-17T09:31:14.850Z"
      },
      {
        "id": "2503.19225",
        "title": "CoinFT: A Coin-Sized, Capacitive 6-Axis Force Torque Sensor for Robotic Applications",
        "authors": [
          "Hojung Choi",
          "Jun En Low",
          "Tae Myung Huh",
          "Seongheon Hong",
          "Gabriela A. Uribe",
          "Kenneth A. W. Hoffmann",
          "Julia Di",
          "Tony G. Chen",
          "Andrew A. Stanley",
          "Mark R. Cutkosky"
        ],
        "abstract": "Title:\n          CoinFT: A Coin-Sized, Capacitive 6-Axis Force Torque Sensor for Robotic Applications\n        \n          We introduce CoinFT, a capacitive 6-axis force/torque (F/T) sensor that is compact, light, low-cost, and robust with an average root-mean-squared error of 0.16N for force and 1.08mNm for moment when the input ranges from 0~14N and 0~5N in normal and shear directions, respectively. CoinFT is a stack of two rigid PCBs with comb-shaped electrodes connected by an array of silicone rubber pillars. The microcontroller interrogates the electrodes in different subsets in order to enhance sensitivity for measuring 6-axis F/T. The combination of features of CoinFT enables various contact-rich robot interactions across different embodiment domains including drones, robot end-effectors, and wearable haptic devices. We demonstrate the utility of CoinFT through two representative applications: a multi-axial contact-probing experiment in which a CoinFT mounted beneath a hemispherical fingertip measures 6-axes of force and torque representative of manipulation scenarios, and an attitude-based force-control task on a drone. The design, fabrication, and firmware of CoinFT are open-sourced at this https URL.",
        "link": "https://arxiv.org/abs/2503.19225",
        "submittedDate": "2026-01-17T09:31:14.850Z"
      },
      {
        "id": "2506.00070",
        "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics",
        "authors": [
          "Dongyoung Kim",
          "Sumin Park",
          "Huiwon Jang",
          "Jinwoo Shin",
          "Jaehyung Kim",
          "Younggyo Seo"
        ],
        "abstract": "Title:\n          Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics\n        Comments:\n          29 pages, 13 figures\n        \n          Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. To rigorously evaluate Robot-R1, we also introduce a new benchmark that demands the diverse embodied reasoning capabilities for the task. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and movement reasoning.",
        "link": "https://arxiv.org/abs/2506.00070",
        "submittedDate": "2026-01-17T09:31:14.850Z"
      },
      {
        "id": "2508.12681",
        "title": "Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory",
        "authors": [
          "Johann Licher",
          "Max Bartholdt",
          "Henrik Krauss",
          "Tim-Lukas Habich",
          "Thomas Seel",
          "Moritz Schappler"
        ],
        "abstract": "Title:\n          Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory\n        Comments:\n          Submitted to IEEE Transactions on Robotics, 20 pages, 14 figures\n        \n          Dynamic control of soft continuum robots (SCRs) holds great potential for expanding their applications, but remains a challenging problem due to the high computational demands of accurate dynamic models. While data-driven approaches like Koopman-operator-based methods have been proposed, they typically lack adaptability and cannot reconstruct the full robot shape, limiting their applicability. This work introduces a real-time-capable nonlinear model-predictive control (MPC) framework for SCRs based on a domain-decoupled physics-informed neural network (DD-PINN) with adaptable bending stiffness. The DD-PINN serves as a surrogate for the dynamic Cosserat rod model with a speed-up factor of 44000. It is also used within an unscented Kalman filter for estimating the model states and bending compliance from end-effector position measurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the GPU. In simulation, it demonstrates accurate tracking of dynamic trajectories and setpoint control with end-effector position errors below 3 mm (2.3% of the actuator's length). In real-world experiments, the controller achieves similar accuracy and accelerations up to 3.55 m/s2.",
        "link": "https://arxiv.org/abs/2508.12681",
        "submittedDate": "2026-01-17T09:31:14.850Z"
      },
      {
        "id": "2601.05529",
        "title": "Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making",
        "authors": [
          "Jua Han",
          "Jaeyoon Seo",
          "Jungbin Min",
          "Jean Oh",
          "Jihie Kim"
        ],
        "abstract": "Title:\n          Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making\n        \n          One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how \"rare\" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.",
        "link": "https://arxiv.org/abs/2601.05529",
        "submittedDate": "2026-01-17T09:31:14.850Z"
      }
    ]
  },
  "total_count": 28
}