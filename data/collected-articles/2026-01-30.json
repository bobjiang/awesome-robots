{
  "fetch_date": "2026-01-30T04:46:09.096Z",
  "week_range": "Jan 23–29",
  "articles": {
    "ieee": [],
    "robotreport": [
      {
        "title": "RobCo raises Series C funding to scale industrial automation",
        "link": "https://www.therobotreport.com/robco-raises-100m-scale-industrial-automation/",
        "pubDate": "2026-01-29T18:42:52.000Z",
        "content": "Three robotic arms. RobCo applies physical AI and modular hardware to industrial automation. [https://www.therobotreport.com/wp-content/uploads/2026/01/RobCo_Lightspeed-copy.jpg] RobCo applies physical AI and modular hardware to industrial automation. Source: Lightspeed Venture Partners The new year is bringing fresh funding across the robotics landscape. RobCo GmbH today said it has raised Series C funding of $100 million to advance its physical AI roadmap, expand enterprise deployments, and deepen its presence in the U.S. market. “With $100 million of additional funding, we will become the dominant AI robotics company for manufacturing in the U.S. and Europe,” stated Roman Hölzl, founder and CEO of RobCo. “This will allow us to execute on our purpose of automating the ordinary, so humans can do the extraordinary.” Founded in 2020, RobCo said it develops robotic systems that bring learning and autonomy into industrial [https://www.therobotreport.com/category/robots-platforms/industrial-robots/] operations. The Munich-based company [https://www.therobotreport.com/tag/robco/] said its Autonomous Manufacturing Platform combines modular hardware with a physical AI [https://www.therobotreport.com/category/design-development/ai-cognition/] software stack that enables fast deployment, continuous improvement, and application-specific performance. ROBCO BUILDS FULL-STACK PLATFORM FOR AUTONOMY RobCo asserted that it “has been vertically integrated from Day 1, developing hardware and software as a single full-stack platform.” The platform combines perception, motion [https://www.therobotreport.com/category/robot-components/motioncontrol/] planning, and self-learning methods to enable increasingly autonomous robot operations inside real production environments. The company said its robots can acquire task-specific skills through demonstration and self-learning rather than manual programming. This allows for rapid deployment and iteration, as well as easier adaptation to complex or variable processes. RobCo claimed that it acts as a “single pane of glass” for customers. In addition, RobCo said it designed its system “to reduce friction between today’s processes and end-to-end automation, allowing teams to focus less on setting up and maintaining systems and more on their core business processes and value drivers.” Robotic arm using AI. RobCo has expanded into U.S. manufacturing. [https://www.therobotreport.com/wp-content/uploads/2026/01/RobCo_banner.jpg] RobCo has expanded into U.S. manufacturing. Source: RobCo DEPLOYMENTS SCALE FOR U.S. ENTERPRISES RobCo delivers its technology through a recurring robotics-as-a-service (RaaS [https://www.automatedwarehouseonline.com/category/raas/]) model, which it said helps companies automate manual tasks while minimizing operational complexity and risk. The company supports a wide range of workflows, including machine tending [https://www.therobotreport.com/tag/machine-tending], palletizing [https://www.therobotreport.com/tag/palletizing/], dispensing, and welding [https://www.therobotreport.com/tag/welding]. Last year, RobCo expanded into the U.S., and it now operates in San Francisco and Austin. The U.S. has become both a strategic priority and a major growth market as manufacturers [https://www.therobotreport.com/category/markets-industries/manufacturing/] accelerate automation efforts in response to labor constraints, reshoring [https://www.therobotreport.com/tag/reshoring] initiatives, and rising operational complexity, the company noted. RobCo added that its robots are deployed across a range of industrial environments, from large global manufacturers such as BMW to companies including DynaEnergetics, Fabricated Extrusion Company, T-Systems, and Rosenberger. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- LIGHTSPEED DOUBLES DOWN ON INDUSTRIAL ROBOTICS Lightspeed Venture Partners and Lingotto Innovation led RobCo’s Series C round. Other participants included Sequoia Capital, Greenfield Partners, Kindred Capital, Leitmotif, and The Friedkin Group. “After leading RobCo’s Series B [https://www.therobotreport.com/robco-raises-42-5m-for-automation-for-small-midsize-manufacturers/], we’re excited to double down and co-lead this $100 million round,” said Alexander Schmitt, a partner at Lightspeed. “Our bar is exceptionally high, and RobCo has continued to raise the standard for what modern robotics can look like in real-world production.” “RobCo has what it takes to build a global champion: systems that already deliver in industrial environments today and a platform grounded in physical AI that can scale across use cases and geographies,” he added. RobCo said its venture capital investors have experience building category-defining technology companies, while its industrial investors use deep connections in industry to back growth companies over the long term. “Manufacturing is entering a new phase where autonomy will be a decisive advantage,” said Morgan Samet, managing partner and co-head of Lingotto Innovation, an investment management company now owned by Exor. “RobCo stands out because it brings physical AI into real production environments, combining proven deployment today with a clear, step-by-step path toward higher autonomy — allowing learning systems to support people where it matters most on the factory floor.” The post RobCo raises Series C funding to scale industrial automation [https://www.therobotreport.com/robco-raises-100m-scale-industrial-automation/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "RobCo plans to use the new capital to continue developing its physical AI systems and expand enterprise deployments in the U.S. and Europe.\nThe post RobCo raises Series C funding to scale industrial a"
      },
      {
        "title": "NHTSA to investigate Waymo after an AV hit a child near a Santa Monica school",
        "link": "https://www.therobotreport.com/nhtsa-investigates-waymo-autonomous-vehicle-hit-child-near-santa-monica-school/",
        "pubDate": "2026-01-29T17:38:04.000Z",
        "content": "Waymo's sensor stack on top of a Waymo autonomous vehicle. The NHTSA is investigating some safety incidents. [https://www.therobotreport.com/wp-content/uploads/2026/01/waymo-featured.gif]https://www.therobotreport.com/wp-content/uploads/2026/01/waymo-featured.gif Waymo’s sensor stack on top of a Waymo autonomous vehicle. | Source: Waymo The National Highway Traffic Safety Administration, or NHTSA, today said it is opening an investigation into Waymo LLC, a self-driving vehicle developer and subsidiary of Alphabet Inc. The agency is investigating an incident in which a Waymo vehicle struck a child near an elementary school in Santa Monica, Calif., last week. According to [https://static.nhtsa.gov/odi/inv/2026/INOA-PE26001-10005.pdf] the NHTSA, on Jan. 23, a child ran across the street from behind a double-parked SUV toward an elementary school. When the child emerged from behind the SUV, a Waymo [https://www.therobotreport.com/tag/waymo/] autonomous vehicle (AV [https://www.therobotreport.com/category/robots-platforms/self-driving-vehicles/]) struck the child. The accident occurred during normal school drop-off hours, said the NHTSA. It added that other children, a crossing guard, and several double-parked vehicles were in the vicinity. Waymo claimed [https://waymo.com/blog/2026/01/a-commitment-to-transparency-and-road-safety] that its technology detected the child as soon as she emerged from behind the parked vehicles. The company [https://www.therobotreport.com/tag/waymo/] said its vehicle braked hard, reducing speed from approximately 17 mph to under 6 mph (27.3 to 9.6 kph) before contact was made. After the AV struck the child, Waymo said the child stood up immediately and walked to the sidewalk. The company called 911, and the vehicle moved to the side of the road to wait for law enforcement to arrive. Waymo also noted that it voluntarily contacted the NHTSA on the same day and that it plans to fully cooperate with the investigation. It said the child sustained minor injuries. In addition, Waymo asserted that its peer-reviewed study showed that a fully attentive human driver would have made contact with the pedestrian at approximately 14 mph (22.5 kph). This is difficult to verify, as the company has not released a video of the incident. INSIDE THE NHTSA INVESTIGATION, WAYMO’S SCHOOL BUS TROUBLES [https://www.therobotreport.com/wp-content/uploads/2025/12/waymo-school-buses.jpg]https://www.therobotreport.com/wp-content/uploads/2025/12/waymo-school-buses.jpg Screenshot from a video capturing one of the incidents of a Waymo robotaxi illegally passing a stopped school bus. | Credit: Austin ISD The NHTSA’s Office of Defects Investigation (ODI) has opened a preliminary evaluation into whether the automated driving system (ADS) exercised appropriate caution given its proximity to the elementary school during drop-off hours. ODI said it will likely examine the ADS’s intended behavior in school zones and neighboring areas, especially during normal school pick up/drop off times, including but not limited to its adherence to posted speed limits. The office will also investigate Waymo’s post-impact response. But this isn’t the first problem Waymo has had when trying to navigate school drop-offs. In December, Waymo announced [https://www.therobotreport.com/waaymo-recalls-robotaxi-software-after-school-bus-safety-failures/] that it would file a voluntary software recall in response to its robotaxis [https://www.therobotreport.com/tag/robotaxi/] illegally passing stopped school buses in a number of incidents across different states. In September, WXIA-TV in Atlanta aired a video [https://www.11alive.com/article/news/local/waymo-car-seen-passing-school-bus-illegally-in-atlanta/85-26b34104-7a13-4abc-8dd4-127d7930e4c4] showing a Waymo illegally passing a stopped school bus. A month later, the NHTSA opened an investigation into Waymo around “traffic safety violations relating to stopping when encountering a school bus, particularly when the bus is boarding or offboarding students.” Residents and school officials saw similar problems in Austin, Texas. The Austin Independent School District detailed 19 different instances of a Waymo robotaxi “illegally and dangerously” passing Austin school buses since the 2025-2026 school year in a letter [https://static.nhtsa.gov/odi/inv/2025/INOT-PE25013-30888P1.pdf] it sent to the NHTSA. The school district requested that Waymo cease operations during the times school buses would be loading and unloading. LOOKING BACK AT WAYMO’S SAFETY RECORD This also isn’t the first time a Waymo vehicle has struck a person. In February 2024 [https://www.sfexaminer.com/news/technology/waymo-autonomous-vehicle-hits-bicyclist-in-san-francisco/article_d8c04fcc-c60c-11ee-9880-976e5722a965.html], a Waymo vehicle ran into a bicyclist in Potrero Hill in San Francisco. The cyclist turned left in front of the vehicle as it entered the intersection. The Waymo vehicle was unable to detect the bicyclist earlier because it was following closely behind a large truck that passed through the intersection immediately before the vehicle started to go through. According to NHTSA, Waymo’s robotaxis surpassed 100 million miles of driving in July 2025, and the company continues to accumulate 2 million miles a week. The company currently operates autonomous ride-hailing services in Atlanta, Austin, Los Angeles, Phoenix, and San Francisco. In the U.S., Waymo said its robotaxis have already driven [https://www.therobotreport.com/waymo-reaches-100m-fully-autonomous-miles-across-all-deployments/] more than 10 million paid rides. Waymo last issued [https://www.therobotreport.com/waymo-updates-1200-robotaxis-in-software-recall/] a voluntary recall in May 2025, when it recalled 1,212 robotaxis to address risks of collisions with chains, gates, and other roadway barriers. The company said it resolved the underlying software issue through a November 2024 update that reduced the likelihood of these types of events. Waymo will be under increasing safety [https://www.therobotreport.com/tag/safety/] scrutiny as it continues to expand its AV service. It intends to expand or launch services in Nashville [https://waymo.com/blog/2025/09/waymo-is-coming-to-nashville-in-partnership-with-lyft], Las Vegas, San Diego, Detroit [https://www.therobotreport.com/waymo-brings-robotaxis-las-vegas-san-diego-detroit/], Washington D.C. [https://www.therobotreport.com/waymo-expand-robotaxi-services-washington-2026/], Miami, Dallas, Seattle, Houston, Orlando, San Antonio, Baltimore, Philadelphia, Pittsburgh, St. Louis [https://www.therobotreport.com/waymo-laying-groundwork-to-bring-robotaxis-to-4-more-cities/], and Denver in the coming years. In addition, the company is planning to bring its technology overseas. Waymo said it wants to deploy in London [https://www.therobotreport.com/waymo-plans-to-bring-driverless-robotaxis-london-2026/] in 2026, and it has already deployed test vehicles in Tokyo [https://www.therobotreport.com/waymo-to-begin-testing-on-tokyo-public-roads/] to learn local traffic patterns. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- The post NHTSA to investigate Waymo after an AV hit a child near a Santa Monica school [https://www.therobotreport.com/nhtsa-investigates-waymo-autonomous-vehicle-hit-child-near-santa-monica-school/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "After its autonomous vehicle struck the child, Waymo said she sustained minor injuries, stood up immediately, and walked to the sidewalk. \nThe post NHTSA to investigate Waymo after an AV hit a child n"
      },
      {
        "title": "ABB Robotics seeks to standardize measurement of robot energy consumption",
        "link": "https://www.therobotreport.com/abb-robotics-standardizes-measurement-robot-energy-consumption/",
        "pubDate": "2026-01-29T15:16:28.000Z",
        "content": "Industrial robots for prefabrication. ABB Robotics says that automating more of construction can improve energy efficiency. [https://www.therobotreport.com/wp-content/uploads/2026/01/Supporting-sustainable-construction.jpg] ABB says that automating more of construction can reduce carbon emissions. Source: ABB Robotics The potential for industrial robots to make production more efficient and environmentally sustainable is commonly cited as a potential benefit of automation. However, this has been difficult to quantify and compare. ABB Robotics this week announced an initiative to develop a global, standardized method for measuring the energy consumption and efficiency of industrial robots. “With no global standard currently in place, it’s a challenge for customers to compare the energy consumption of different robots and choose the most energy efficient solution.” said Gianluca Brotto, head of sustainability at ABB Robotics. “Unlike other products such as fridges, TVs, washing machines and motors, which have clearly defined standards for how to measure and compare energy efficiency, there is no standard for measuring the energy consumption of a robot,” he noted. “This initiative will empower customers to make informed decisions and help the industry reduce its carbon footprint.” MEASUREMENT THE FIRST STEP TO GREATER ENERGY EFFICIENCY More than 4 million industrial robots are operating worldwide, and automation is expanding into new sectors, according to [https://www.therobotreport.com/ifr-4-million-robots-operating-globally-world-robotics-report/] the International Federation of Robotics (IFR [https://www.therobotreport.com/tag/international-federation-of-robotics]). Reducing the environmental impact [https://www.therobotreport.com/tag/sustainability/] of robotic systems is paramount, as companies aim to reduce their emissions from energy consumption in line with the Paris Agreement, noted ABB Robotics. Internal ABB studies found that more than 70% of its customer robots’ carbon footprint comes from electricity usage [https://www.therobotreport.com/category/technologies/batteries-power-supplies/] during their operational phase. The company has worked with the Swedish Institute for Standards (SIS [https://www.sis.se/en/about_sis/sisorganisation/]) to develop a proposal with experts from different robot manufacturers and research institutes in 11 countries. This led to the creation of an International Organization for Standardization (ISO [https://www.therobotreport.com/tag/iso/]) technical specification [https://www.iso.org/standard/89447.html] for global adoption due to be completed by August 2026. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- ABB ROBOTICS HAILS GLOBAL SPECIFICATION “This effort marks a critical step toward improving transparency and supporting the global transition to more sustainable manufacturing,” stated ABB Robotics. “With a new, global ISO technical specification in place, industrial robotics customers will be able to clearly choose the most energy-efficient robot for the job.” ABB Robotics’ portfolio includes industrial [https://www.therobotreport.com/category/robots-platforms/industrial-robots/] robots, force- and power-limited cobot arms [https://www.therobotreport.com/category/robots-platforms/collaborative-robot/], and autonomous mobile robots (AMRs [https://www.therobotreport.com/category/robots-platforms/amrs/]), designed and orchestrated by its software and AI. It launched [https://new.abb.com/news/detail/119894/prsrl-abb-robotics-launches-energy-efficiency-service-to-help-customers-save-costs-and-enhance-sustainability] an Energy Efficiency Service in 2024. The company [https://www.therobotreport.com/tag/abb-robotics/] said its “autonomous versatile robotics” (AVR) and ecosystem help businesses of all sizes and sectors — from automotive [https://www.therobotreport.com/category/markets-industries/automotive] to electronics [https://www.therobotreport.com/tag/electronics/] and logistics [https://www.automatedwarehouseonline.com/tag/abb/] – to become more resilient, flexible, and efficient. In October 2025, ABB Group announced plans to sell [https://www.therobotreport.com/abb-group-sells-abb-robotics-softbank-5-3b/] its robotics unit to SoftBank Group for $5.3 billion. ABB Robotics has about 7,000 employees, and its U.S. headquarters and factory are in Auburn Hills, Mich. The post ABB Robotics seeks to standardize measurement of robot energy consumption [https://www.therobotreport.com/abb-robotics-standardizes-measurement-robot-energy-consumption/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "ABB Robotics said new energy consumption measurement will allow end users to make more informed decisions and support sustainability efforts.\nThe post ABB Robotics seeks to standardize measurement of "
      },
      {
        "title": "9 weeks to 9 days: How autonomous drilling is transforming data center construction",
        "link": "https://www.therobotreport.com/dewalt-drilling-robot/",
        "pubDate": "2026-01-29T13:30:36.000Z",
        "content": "empty warehouse with three hole drilling robots. [https://www.therobotreport.com/wp-content/uploads/2026/01/DEWALT-Robotic-Drilling-featured.jpg] Robots pull hole information from the building CAD plans and accurately drill within about 1/8 in. (3.1 mm). | Credit: DEWALT DEWALT and August Robotics have partnered to deliver an autonomous concrete floor drilling system. It features an industrial DEWALT drilling rig mounted to the August Robotics autonomous mobile robot, or AMR. The offering targets the growing market opportunity for data center construction [https://www.therobotreport.com/tag/construction/], which requires intensive site preparation for server rack installation. Developers must bolt data center racks to the floor, which requires drilling thousands of holes into the concrete slab. August Robotics launched its first Lionel robot in 2019. The AMR [https://www.therobotreport.com/category/robots-platforms/amrs/] is designed for exhibition floor marking [https://augustrobotics.com/exhibition] and industrial layout [https://augustrobotics.com/construction/industrial-layout]. Lionel makes every mark to at least +/- 1/8 in. (3.175 mm) accuracy, measuring each mark independently and avoiding the accumulation of errors. Through the development of Lionel, August Robotics said it mastered the process of indoor positioning accuracy and optimized the user experience. Lionel works from the CAD file for the building or exposition center, which is easy to set up and operate onsite. AUGUST ROBOTICS SEES NEW OPPORTUNITY WITH HOLE DRILLING Close up of the DEWALT drilling rig on the Lionel robot platform. [https://www.therobotreport.com/wp-content/uploads/2026/01/DEWALT-august-closeup.jpg] The DEWALT drilling rig is mounted to the August Robotics AMR base. | Credit: DEWALT Drilling holes is a natural extension of Lionel’s core capabilities [https://augustrobotics.com/construction/drilling], said August Robotics. The new system combines Lionel’s autonomous indoor positioning capabilities with an industrial DEWALT concrete drilling machine. The current platform is dual-branded as both DEWALT and August Robotics, carrying both logos on the product. The partners said the advantages of a robot-mounted drill include speed, accuracy, traceability, and a measurable return on investment. The system compresses the data center floor prep schedule from eight to nine weeks of layout plus drilling down to about seven to nine days with four robots. Other potential benefits include improved safety, reduced fatigue, and the freeing up of skilled workers/apprentices for higher-value tasks. DEWALT, a unit [https://www.dewalt.com/] of Stanley Black & Decker Inc., claimed that the system delivers strong unit economics with the cost per hole reduced from more than $60 to around $20. The joint offering has already generated 21,000 hours of labor capacity in the first six months, through the automation of the drilling process, said the companies. Alex Schickling, director of robotics at DEWALT, told The Robot Report [https://www.therobotreport.com/] that “with the early access program, 12 robots are currently in the field, having drilled 108,000 holes with 99.97% placement accuracy. Meanwhile, the core August Robotics Lionel solution has printed more than 300 million sq. ft. [27.8 million sq. m] of floor space.” “The robot doesn’t mind if there are other things on the floor or not,” he continued. “It’s got the technology to make sure it’s not going to run into anything — it comes from multiple angles if something is blocked, to try to get it from different angles. Whether it’s a new build, it’s a warehouse, it’s a data center, any building that’s got a concrete floor, an open area to drill, it’s going to work very well in.” DEWALT STILL DEVISING GO-TO-MARKET STRATEGY DEWALT said its go-to-market strategy for the drilling robots is still in an early/experimental phase and not fully finalized. According to Schickling, the company expects the robotic drilling system to be available commercially midyear 2026. It demonstrated the product at the World of Concrete Trade Show in Las Vegas last week. Both DEWALT and August Robotics sales teams will work with their respective partners to develop a fleet of robots in the field. The post 9 weeks to 9 days: How autonomous drilling is transforming data center construction [https://www.therobotreport.com/dewalt-drilling-robot/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "DEWALT and August Robotics launched an autonomous drilling robot to accelerate concrete floor preparation for data centers.\nThe post 9 weeks to 9 days: How autonomous drilling is transforming data cen"
      },
      {
        "title": "OnRobot to share automation roadmap advice in Dallas",
        "link": "https://www.therobotreport.com/onrobot-share-automation-roadmap-advice-in-dallas/",
        "pubDate": "2026-01-29T13:25:35.000Z",
        "content": "OnRobot and FANUC will demonstrate applications such as machine tending, shown here, in Dallas. [https://www.therobotreport.com/wp-content/uploads/2026/01/FANUC_OnRobot-tooling-1.png] OnRobot and FANUC will demonstrate applications such as machine tending in Dallas. Source: OnRobot Manufacturers in North Texas face the same struggles with hiring and retaining skilled workers as companies worldwide, but robotics can help. OnRobot A/S plans to host an event in Dallas on Feb. 19 that it said will bring practical automation solutions directly to the region’s manufacturing community. Kristian Hulgard, OnRobot [https://www.therobotreport.com/wp-content/uploads/2026/01/Kristian_1-109x150.jpg] Kristian Hulgard, OnRobot “As one of the largest and fastest-growing manufacturing hubs in the country, Dallas–Fort Worth is feeling the strain of today’s labor market more than most,” said Kristian Hulgard, general manager for the Americas at OnRobot. “Many manufacturers here are running strong order books but simply can’t find enough skilled operators, machinists, or technicians. This roadmap event is about showing practical, proven ways automation can help fill those gaps.” Founded in 2018, OnRobot was formed with the merger of three end-of-arm tooling (EOAT [https://www.therobotreport.com/category/technologies/grippers-end-effectors/]) companies: Perception Robotics, OptoForce, and OnRobot. The Odense, Denmark-based company [https://www.therobotreport.com/tag/onrobot/] also acquired Purple Robotics [https://www.therobotreport.com/onrobot-acquires-purple-robotics-raises-new-funding/] in 2018 and Blue Workforce [https://www.therobotreport.com/onrobot-acquires-blue-workforce-assets-robotics-developers/] in 2019. TEXAS MANUFACTURERS HAVE BIG NEEDS Manufacturers across North Texas are facing persistent hiring challenges. A lack of available applicants and shortages of candidates with the right technical skills remain among the top barriers to hiring for manufacturers [https://www.therobotreport.com/category/markets-industries/manufacturing/], according to the Federal Reserve Bank of Dallas’ Texas Business Outlook Survey [https://www.dallasfed.org/research/surveys/tbos/2025/2507q?]. Many companies also reported increased competition for labor and rising wage pressures. These challenges are forcing manufacturers to rethink how they maintain output, meet customer demand, and scale operations, said OnRobot. The provider of hardware and software for collaborative applications said its grippers can help manufacturers optimize processes such as palletizing, pick and place, sanding, and screwdriving. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- ONROBOT OFFERS FIRSTHAND LOOK AT APPLICATIONS At the “Build your Automation Roadmap” [https://en.onrobot.info/industrial_openhouse] event, attendees will see live demonstrations of FANUC robot arms [https://www.fanucamerica.com/products/robots] equipped with OnRobot EOAT [https://onrobot.com/us/products] for common applications such as machine tending [https://www.therobotreport.com/tag/machine-tending/], material handling [https://www.automatedwarehouseonline.com/category/move/], assembly [https://www.therobotreport.com/tag/assembly], packaging [https://www.therobotreport.com/tag/packaging/], and quality inspection [https://www.therobotreport.com/tag/inspection/]. Automation experts will be on hand to share real-world experience in robotics, tooling, and integration. Their goal is to help manufacturers move from curiosity to confident implementation, the company [https://onrobot.com/us] said. Brent Lindell, FANUC America [https://www.therobotreport.com/wp-content/uploads/2026/01/Brent-Lindell-150x150.jpg] Brent Lindell, FANUC America Kristian Hulgard, general manager, Americas, at OnRobot, will deliver an opening keynote on macrotrends affecting U.S. automation growth, why adoption is accelerating across manufacturing, and what it means for Dallas operations. Brent Lindell, district manager at FANUC America [https://www.fanucamerica.com/], will provide live demonstrations of FANUC [https://www.therobotreport.com/tag/fanuc] robots, identifying the practical approach to the automation journey. Ruben Lanz, Adaptive Vision and Robotics [https://www.therobotreport.com/wp-content/uploads/2026/01/Ruben-Lanz-150x150.jpg] Ruben Lanz, Adaptive Vision and Robotics Ruben Lanz, vice president at Adaptive Vision and Robotics [https://adaptive-companies.com/], will explain how the right components, sourcing strategy, and distribution support can keep automation projects on track. Derrell Guillory, AWC [https://www.therobotreport.com/wp-content/uploads/2026/01/Derrell-Guillory-150x150.jpg] Derrell Guillory, AWC Derrell Guillory, technical account manager at AWC [https://www.awc-inc.com/], will discuss why system integration [https://www.therobotreport.com/tag/integration/] matters, how integrators add value, and how to reduce risk while accelerating deployment. Registration including lunch is free, but space is limited. To learn more, visit: https://en.onrobot.info/industrial_openhouse [https://en.onrobot.info/industrial_openhouse] The post OnRobot to share automation roadmap advice in Dallas [https://www.therobotreport.com/onrobot-share-automation-roadmap-advice-in-dallas/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "OnRobot and FANUC will demonstrate common applications for automation to help manufacturers in North Texas.\nThe post OnRobot to share automation roadmap advice in Dallas appeared first on The Robot Re"
      },
      {
        "title": "Gartner predicts fewer than 20 companies will deploy humanoids at scale by 2028",
        "link": "https://www.therobotreport.com/gartner-predicts-fewer-than-20-companies-will-deploy-humanoids-at-scale-by-2028/",
        "pubDate": "2026-01-28T21:31:40.000Z",
        "content": "From left to right: Unitree's humanoid, Boston Dynamics' Atlas, Figure AI's 02, Apptronik's Apollo, and Tesla's Optimus robot. [https://www.therobotreport.com/wp-content/uploads/2026/01/humanoids-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/humanoids-featured.jpg From left to right: Unitree’s G1, Boston Dynamics’ Atlas, Figure AI’s Figure 02, Apptronik’s Apollo, and Tesla’s Optimus robot. | Source: Unitree, Boston Dynamics, Figure, Apptronik, and Tesla In recent years, humanoid robots have dominated headlines, and the companies developing them have raised hundreds of millions of dollars to get them working in the real world. Despite these efforts, actual humanoid deployments are few and far between, noted Gartner Inc. The market research firm [https://www.gartner.com/en] doesn’t expect this to change anytime soon. It said that through 2028, fewer than 100 companies will progress with humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] proofs of concept beyond experimentation, even though close to 200 exist today. Gartner also predicted that fewer than 20 companies will be going live in production for supply chain [https://www.therobotreport.com/category/markets-industries/logistics-warehousing-asrs/] and manufacturing [https://www.therobotreport.com/category/markets-industries/manufacturing/] use cases. Most deployments of humanoid robots during this time will remain limited to tightly controlled environments, rather than in dynamic and high-throughput operations, it said. “The promise of humanoid robots is compelling, but the reality is that the technology remains immature and far from meeting expectations for versatility and cost-effectiveness,” said Abdil Tunca, senior principal analyst in Gartner’s Supply Chain practice. “CSCOs [chief supply chain officers] must carefully evaluate readiness and avoid overcommitting resources to solutions that cannot yet deliver on their potential.” Robots designed to mimic the human body in shape, function, and locomotion are attracting attention from CSCOs seeking solutions to workforce challenges and rising labor costs. These humanoids feature AI [https://www.therobotreport.com/category/design-development/ai-cognition/]-enabled systems, advanced sensors, and machine learning algorithms intended to dynamically adapt to multiple tasks. BARRIERS PERSIST FOR HUMANOID PROGRESS Humanoid robots incorporate heads with sensors and cameras, arms and grippers for manipulation, and legs for locomotion. While this form factor offers certain advantages, Gartner noted that alternative designs—such as “polyfunctional” robots equipped with wheels or sensors in unconventional placements—may provide superior performance and adaptability for supply chain operations. Despite their potential, Stamford, Conn.-based Gartner said humanoid robots face significant barriers to supply chain, logistics, and manufacturing adoption: * Technological limitations: Current models lack the dexterity, intelligence, and adaptability required for complex, unstructured environments such as mixed SKU picking [https://www.automatedwarehouseonline.com/category/manipulating/], trailer unloading [https://www.automatedwarehouseonline.com/tag/loading/], or exception handling in high velocity warehouses [https://www.automatedwarehouseonline.com/]. * Integration complexity: Compatibility with existing systems and workflows remains a challenge, as is safety [https://www.therobotreport.com/tag/safety/]. * High costs: Substantial upfront investment and ongoing maintenance expenses must be weighed against uncertain returns. With the current technology, humanoids cost multiple times more than task-specific robots while delivering lower throughput and uptime. * Energy constraints: Limited battery [https://www.therobotreport.com/category/technologies/batteries-power-supplies/] life restricts operational time for high-mobility tasks. POLYFUNCTIONAL ROBOTS WILL WIN THE WAREHOUSE, SAYS GARTNER Polyfunctional robots from Diligent Robotics, Kinisi Robotics, Humanoid, and RoboForce. Gartner analyzed the market prospects for such systems. [https://www.therobotreport.com/wp-content/uploads/2026/01/mobile-manipulators.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/mobile-manipulators.jpg Polyfunctional robots, from left: Moxi, Kinisi 01, HMND 01, and Titan. | Source: Diligent Robotics, Kinisi Robotics, Humanoid, and RoboForce Unlike humanoid robots, polyfunctional robots are optimized for flexibility without being constrained by human-like design, Gartner observed. For example, a mobile manipulator [https://www.therobotreport.com/tag/mobile-manipulation] with wheels and a telescopic arm can move boxes, pick cases, scan inventory, and perform inspections, usually with higher uptime and using less energy than a humanoid that is attempting the same tasks. Such robots can integrate features that enhance efficiency and durability, making them better suited for dynamic supply chain environments, said the business and intelligence firm. “Companies with a high risk appetite and focus on innovation are the best candidates for pursuing humanoid robots at present, given the unproven capabilities of these solutions and related lack of clarity for return on investment,” said Caleb Thomson, senior director analyst in Gartner’s Supply Chain practice. “For the majority of companies that will need to prioritize robots that maximize throughput-per-dollar invested, we expect polyfunctional robots to be the superior solution.” To navigate robotics investment decisions effectively, Gartner advised CSCOs to: * Pursue pilot programs to validate feasibility before committing to full-scale deployment. * Collaborate with emerging providers to influence product development and align solutions with operational needs. * Implement continuous monitoring to track performance and guide iterative improvements. * Foster a culture of innovation that supports experimentation and calculated risk-taking. * Prioritize outcome-driven automation that targets specific bottlenecks, rather than generalized “headcount reduction” strategies, which is also less risky from an investment standpoint. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- The post Gartner predicts fewer than 20 companies will deploy humanoids at scale by 2028 [https://www.therobotreport.com/gartner-predicts-fewer-than-20-companies-will-deploy-humanoids-at-scale-by-2028/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Gartner says that relatively few humanoid robot developers will progress proofs of concept beyond experimentation.\nThe post Gartner predicts fewer than 20 companies will deploy humanoids at scale by 2"
      },
      {
        "title": "Introducing Sprout, a new humanoid development platform",
        "link": "https://www.therobotreport.com/introducing-sprout-a-new-humanoid-development-platform/",
        "pubDate": "2026-01-28T20:06:17.000Z",
        "content": "In the high-stakes artificial intelligence race, New York-based Fauna Robotics is betting that the path to general intelligence runs through the physical world. Led by founders Rob Cochran [https://www.linkedin.com/in/rpcochran/] and Josh Merel [https://www.linkedin.com/in/josh-merel-9222b72a2/], the startup is developing a specialized hardware and AI platform to move robots out of factories and into the unstructured environments of everyday life. By engineering safe, lightweight, and inexpensive machines, Fauna said it aims to create a data-driven “flywheel” for embodied AI [https://www.therobotreport.com/category/design-development/ai-cognition/]. The company [https://faunarobotics.com/] said it is pursuing a vision where general-purpose robots don’t just perform tasks, but also live, work, and play alongside their human counterparts. STARTUP FOCUSES ON BUILDING APPLICATIONS OVER TEACHING FUNDAMENTALS Cochran described the concept for Sprout to The Robot Report [https://www.therobotreport.com/]: “It gives people the tools to start building interesting applications, rather than focusing on the kind of fundamentals that make it quite a small community of roboticists that are able to actually engage with robotics today, and so that feels like a really great opportunity.” He added that Fauna was founded on the idea of solving problems immediately. “One of the things we identified when we started this company is … now is the time when we can maybe start to solve some general-purpose robotics problems and in a general-purpose form factor, like a humanoid [https://www.therobotreport.com/tag/humanoid/],” Cochran said. Sprout is engineered for expressive engagement within human-centric environments. These interactions are enhanced by an articulated neck that provides a controllable gaze, allowing Sprout to “look” at people and objects. Fluid movements of the arms and torso further humanize the robot, enabling it to perform social gestures like high-fives, handshakes, and playful poses that bridge the gap between machine and companion, according to Fauna Robotics. SPROUT DESIGNED FOR POSITIVE INTERACTIONS WITH PEOPLE Fauna Robotics said it balanced a friendly aesthetic with rigorous safety [https://www.therobotreport.com/tag/safety/] features in Sprout. The robot’s compact, lightweight frame has soft exterior panels and back-drivable motors to minimize impact forces and ensure safe interactions. Sprout's torso view [https://www.therobotreport.com/wp-content/uploads/2026/01/Sprout-Interactive.jpg] Standing at 107 cm and weighing just 22.7 kg, Sprout is designed to be inherently safe. | Credit: Fauna Robotics The company said it engineered Sprout’s physical form factor for approachability. With a maximum size of 107 cm (42 in.) and weighing 22.7 kg (50 lb.), the robot is intended to be unintimidating, particularly for children. In addition to its small size, Sprout works with a dedicated safety subsystem that monitors real-time conditions to enforce constraints across all mechanical and software levels, explained Fauna. Beyond its protective design, Sprout emphasizes emotional connection through a character-like appearance, which includes an articulated neck, actuated eyebrows, and an LED array that facilitates natural, non-verbal communication and social trust. The robot’s modular architecture has 29 degrees of freedom and is designed for both durability and ease of use. It has a 3.5-hour swappable battery and a resilient build capable of recovering from falls. Fauna asserted that this blend of approachable materials and personalized aesthetics supports its mission to build a versatile platform for education, therapy, and interactive research. FAUNA ROBOTICS BUILDS A PLATFORM FOR AI DEVELOPMENT Sprout also serves as a high-performance, scalable platform powered by the NVIDIA [http://www.therobotreport.com/tag/nvidia/] Jetson AGX Orin for advanced perception and decision-making. For developers, the platform offers a streamlined experience through stable APIs and containerized services that support everything from low-level motor control to high-level autonomy. By using consumer electronics manufacturing processes and a low part count, Fauna said its design ensures that Sprout can scale efficiently from research labs to mass-market production. Cochran compared Sprout to the early development of PCs. “The analogy I like to draw is to the early days of personal computing, [where] it wasn’t really until you had certain things like the Apple II and the invention of BASIC, the programming language, that you had this abstraction that allowed for the concept of an application developer,” he said. “And then you started to get interesting applications, and then the market got bigger. I think that same trajectory could make a lot of sense in the context of [physical AI-based] robotics as well.” ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- SPROUT IS MORE A CHARACTER RATHER THAN A MACHINE Core to Sprout’s design is a character-like presence over utilitarian machinery, drawing on popular culture to foster immediate emotional connections, said Fauna Robotics. Sprout appears more like an animated character come to life than an industrial machine. Central to this design approach is a “non-screen-based” facial design that emphasizes physical embodiment rather than digital displays. With its motorized eyebrows and LED array, the robot can convey a wide range of emotions and intentions, such as “thinking” animations or safety warnings, making it more relatable and engaging for human interaction, Fauna said. headshot of sprout. [https://www.therobotreport.com/wp-content/uploads/2026/01/Sprout-head.jpg] Sprout features a “non-screen-based” facial design that emphasizes physical embodiment rather than digital displays. | Credit: Fauna Robotics These physical cues are governed by a slot-based behavior hierarchy, ensuring that visual and mechanical elements work in harmony to produce coherent, lifelike behaviors. “We’ve provided a software and developer experience packaged around this idea that you can do very low-level programming on this,” Cochran noted. “But you also get mapping and navigation and voice interaction, and HRI [https://www.therobotreport.com/category/design-development/haptics/] features right out of the box.” To facilitate natural social [https://www.therobotreport.com/tag/social-robots] engagement, Sprout incorporates a sophisticated audio-visual and movement suite. A four-microphone array enables speech recognition and sound-source localization, while integrated speakers allow the robot to respond with speech, sound effects, and tones that complement its physical gestures. four views of sprout, showing the sensors and features of the robot platform. [https://www.therobotreport.com/wp-content/uploads/2026/01/sprout-features.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/sprout-features.jpg Hardware overview. Key features of the Sprout robot platform from different perspectives: (A) and (B) are true-color renders (C) and (D) are semi-transparent renders. | Credit: Fauna Robotics Full specifications for Sprout are available on the Fauna Robotics product page [https://faunarobotics.com/product]. The robot’s soft, deformable exterior panels encourage physical contact, while the inclusion of standardized interlocking toy brick components allows for cosmetic customization (see image",
        "excerpt": "Fauna Robotics unveils Sprout: a safe, character-driven robot designed to live, work, and play alongside humans in everyday life.\nThe post Introducing Sprout, a new humanoid development platform appea"
      },
      {
        "title": "Waabi raises $1B to advance autonomous trucks and robotaxis",
        "link": "https://www.therobotreport.com/waabi-raises-1b-to-advance-autonomous-trucks-and-robotaxis/",
        "pubDate": "2026-01-28T16:30:03.000Z",
        "content": "A Uber Freight truck with Waabi technology. Waabi will apply its Physical AI Platform developed for trucks to robotaxis in an expanded partnership with Uber. [https://www.therobotreport.com/wp-content/uploads/2026/01/Waabi_SeriesC.jpg] Waabi will apply its Physical AI Platform developed for trucks to robotaxis in an expanded partnership with Uber. Source: Waabi More AI-enabled vehicles are coming to the roads, as Waabi today said it has closed an oversubscribed $750 million Series C round to accelerate its commercialization of autonomous trucks. The company has also secured additional investment from Uber Technologies Inc. and said the strategic partnership will support its expansion into robotaxis. “Waabi’s Physical AI Platform has enabled us to hit an industry-leading pace in the development and commercialization of autonomous trucks over the past few years,” stated Raquel Urtasun, founder and CEO of Waabi. “Our current self-driving capabilities across highways and generalized surface streets have unlocked a new direct-to-customer model that for the first time solves the pain points of the industry and provides an unprecedented opportunity to quickly and seamlessly enter the robotaxi market, delivering a truly scalable solution for both verticals,” she added. Founded in 2021, Waabi said its Physical AI Platform generalizes to different form factors, geographies, and environments. The platform “combines a verifiable end-to-end AI model capable of reasoning alongside the world’s most advanced neural simulator [https://www.therobotreport.com/category/software-simulation/],” claimed the Toronto-based company [https://www.therobotreport.com/tag/waabi]. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- WAABI AND UBER TO BRING AI BRAIN TO BOTH TRUCKS AND TAXIS Waabi said its approach enables the same AI [https://www.therobotreport.com/category/design-development/ai-cognition/] model to act as a “shared brain” for both autonomous trucks [http://therobotreport.com/tag/autonomous-trucking] and robotaxis [https://www.therobotreport.com/tag/robotaxi]. The company [https://waabi.ai/] asserted that its expansion into self-driving passenger vehicles will ultimately improve the Waabi Driver’s overall capabilities. Under the partnership, Uber will invest additional milestone-based capital to support the development of robotaxis using the Waabi Driver. The companies said they plan to deploy 25,000 or more autonomous vehicles (AVs [https://www.therobotreport.com/category/robots-platforms/self-driving-vehicles/]). “We are thrilled to partner with the best-in-class ridesharing platform to bring about a safer, more efficient, and sustainable future,” said Urtasun. Waabi previously partnered [https://www.therobotreport.com/waabi-partners-with-uber-freight-to-deploy-autonomous-trucks-at-scale/] with Uber Freight in 2023, and Uber participated in its $200 million Series B round [https://www.therobotreport.com/waabi-raises-200m-uber-nvidia-on-the-road-self-driving-trucks/] in 2024. “Waabi’s expanded focus on robotaxis marks an important milestone for their team and the AV industry more broadly,” said Dara Khosrowshahi, CEO of Uber. “We’re very excited to deepen our partnership with Waabi as they significantly scale their Physical AI Platform and enter a new phase of an already remarkable journey.” While it sold [https://www.therobotreport.com/uber-sells-self-driving-unit-aurora-ending-tumultuous-era/] its Advanced Technologies Group (ATG) to Aurora Innovation [https://www.therobotreport.com/tag/aurora/] in 2020, Uber [https://www.therobotreport.com/tag/uber/] has also invested in other AV developers, including Nuro [https://www.therobotreport.com/nuro-closes-203m-propel-ai-first-self-driving-tech-commercial-partnerships/], Lucid [https://www.therobotreport.com/lucid-nuro-uber-team-up-on-global-robotaxi-fleet/], and Wayve [https://www.therobotreport.com/wayve-announces-strategic-partnership-and-investment-from-uber/], and it has partnered with robotaxi providers Motional [https://www.therobotreport.com/motional-partners-with-uber-for-10-year-commercial-agreement/], Waymo [https://www.therobotreport.com/phoenix-residents-can-soon-hail-waymo-robotaxis-with-uber/], and WeRide [https://www.therobotreport.com/uber-to-invest-100m-into-weride-to-bring-robotaxis-to-15-cities/]. AUTOMOTIVE LEADERS, CANADIAN INVESTORS JOIN WAABI ROUND Khosla Ventures and G2 Venture partners co-led Waabi’s Series C round, which the company said is the largest fundraise in Canadian history. “We invest in the companies that are leading the AI era,” said Vinod Khosla, founder of Khosla Ventures. “Waabi has developed a truly groundbreaking physical AI platform that represents a fundamental leap forward in how next-generation driverless technology is being developed.” Other strategic investors included NVentures (NVIDIA [https://www.therobotreport.com/tag/nvidia]’s venture capital arm), Volvo Group Venture Capital, and Porsche Automobil Holding SE. “Waabi is fundamentally changing the trajectory of autonomous transportation,” said Brook Porter, co-founder and partner at G2 Venture Partners. “Their simulation-first, end-to-end AI is a powerful enabler, accelerating commercial adoption while dramatically reducing capital needs to scale. Waabi is unlocking the potential for autonomy to drive vehicle efficiency and utilization, catalyzing the shift to a more sustainable transportation system.” In addition, funds and accounts managed by BlackRock, Radical Ventures, HarbourVest Partners, a wholly owned subsidiary of the Abu Dhabi Investment Authority (ADIA), Linse Capital, Incharge Capital, and others participated in the investment. Waabi noted that Canadian firms such as BDC Capital’s Thrive Venture Fund, Export Development Canada (EDC), TELUS Global Ventures, and BMO Global Asset Management were among its backers. Waabi’s Series C round joins other large robotaxi investments, including Waymo’s $5.6 billion [https://www.therobotreport.com/waymo-raises-5-6b-to-accelerate-self-driving-car-growth/] Series C in 2024, Cruise’s $2.75 billion [https://www.therobotreport.com/cruise-raises-2b-partners-microsoft-autonomous-vehicles/] round in 2021, and Aurora’s $820 million [https://ir.aurora.tech/news-events/press-releases/detail/79/aurora-announces-closing-of-820-million-upsized-public-offering-and-private-placement-of-class-a-common-stock] stock sale in 2024. In autonomous trucking, Einride raised $100 million [https://www.therobotreport.com/einride-raises-100m-to-scale-autonomous-freight-deployments/] in October 2025, while Plus Automation [https://www.therobotreport.com/autonomous-trucking-developer-plus-goes-public-via-spac/] and Kodiak Robotics [https://www.therobotreport.com/kodiak-robotics-autonomous-trucking-developer-goes-public-via-spac/] went public via special-purpose acquisition companies (SPACs) last year. The post Waabi raises $1B to advance autonomous trucks and robotaxis [https://www.therobotreport.com/waabi-raises-1b-to-advance-autonomous-trucks-and-robotaxis/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Waabi plans to apply its Physical AI Platform developed for autonomous trucks to robotaxis with Uber.\nThe post Waabi raises $1B to advance autonomous trucks and robotaxis appeared first on The Robot R"
      },
      {
        "title": "Robotiq brings sense of touch to physical AI with fingertips for 2F grippers",
        "link": "https://www.therobotreport.com/robotiq-brings-sense-touch-physical-ai-fingertips-2f-grippers/",
        "pubDate": "2026-01-27T15:44:03.000Z",
        "content": "Rending of Robotiq's TSF-85 tactile sensor fingertips on its 24-85 Adaptive Gripper. [https://www.therobotreport.com/wp-content/uploads/2026/01/Robotiq_gripper_sensor.jpg] Rending of the TSF-85 tactile sensor fingertips on the 2F-85 Adaptive Gripper. Source: Robotiq Robotic end effectors do not need to be humanoid to be more sensitive and effective, according to Robotiq Inc. The company today launched the TSF-85 tactile sensor fingertips for its 2F-85 Adaptive Gripper. It said that its integrated sensing enables robots to reliably feel, understand, and interact with the world at scale. “Physical AI demands more than clever algorithms—it demands reliable interaction with the real world,” stated Vincent Duchaine, chief technology officer for artificial intelligence at Robotiq. “By combining adaptive gripping with high-frequency tactile sensing, we’re giving robots the sense of touch and control they need to generalize across objects, tasks, and environments without the cost and complexity of anthropomorphic hands.” Founded in 2008, Robotiq said it built the TSF-85 on years of research and field experience, with more than 23,000 grippers [https://www.therobotreport.com/category/technologies/grippers-end-effectors/] deployed worldwide. The Lévis, Quebec-based company [https://www.therobotreport.com/tag/robotiq] said leading AI labs and manufacturers use its products to “bridge the gap between digital intelligence and physical reality.” ROBOTIQ COMMERCIALIZES UNIVERSITY R&D “Tactile sensing has been in academia for a long time,” noted Jennifer Kwiatkowski [https://www.linkedin.com/in/jenniferkwiatkowski/?originalSubdomain=ca], an AI specialist at Robotiq. “Robotics companies are realizing the limits of what vision can do for planning, navigation, and manipulation in terms of their robot perception stacks.” Robotiq’s tactile sensor came out of research that Kwiatkowski worked on at the École de technologie supérieure (ETS [https://www.etsmtl.ca/]) in Montreal, she told The Robot Report [https://www.therobotreport.com/]. “My research was on grasp stability prediction using these sensors and AI,” Kwiatkowski said. “There have been other experiments around object and texture recognition, as well as detecting if there’s a crack in a piece. If you’re squeezing different objects, can you recognize them?” “Robotiq recognized that we had something, having been involved with end-of-arm tooling [EOAT] for 17 years,” she added. “The gripper is the way that the robot interacts with the world, so all of your perception at the end of the arm is what makes your robot useful.” ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- 2F-85 GRIPPER DESIGNED FOR SENSITIVITY, SIMPLICITY While parallel grippers rely on precise positioning and rigid alignment, Robotiq said its 2F-85 EOAT offers both pinch and encompassing grips, with stroke lengths of 85 and 140 mm (3.3 and 5.5 in.). It said this allows the gripper to conform to an object’s shape, reduces grasp planning complexity, and minimizes reliance on unobstructed vision, making it suitable for handling a wide range of objects. Robotiq listed the following capabilities: * A 4×7 static taxel grid to monitor force distribution * Micro-slip detection at 1000 Hz for stable, precise manipulation * An integrated IMU (inertial measurement unit) for proprioceptive sensing and contact awareness “Unlike other sensing mechanisms — optical or magnetic — we’re capacitor-based, like a smartphone touchscreen, which is well-established,” explained Kwiatkowski. “We focus on having good integration of the sensor into the robotic hand, which has to be reliable in the warehouse.” The tactile-enabled 2F grippers integrate into existing robots using native RS-485 communication and a USB conversion board, said Robotiq. The tactile fingertips are designed to preserve grip mechanics with minimal impact on stroke and reach, and they feature robust cabling. ROBOTIQ TOUTS DURABILITY, SCALABILITY OF EOAT Robotiq said that thousands of its grippers are already operating in demanding research and industrial environments worldwide. It claimed that its grippers have a lower bill of materials (BOM) and replacement cost than anthropomorphic or DIY hands, reducing cost and providing “a practical path from lab prototype to large-scale robot fleets.” “We’re in the process of doing tests, but the threshold we’re looking to match is our grippers’ lifecycles, which is on the order of millions of cycles,” Kwiatkowski said. “Another interesting element of our design is scalability. These sensors are not too complex to manufacture, which is important to us if we’re getting orders for hundreds of thousands of grippers.” “Articulated hands are harder to control and have more failure modes,” she said. “Humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] hands are very cool, but they’re more of a 10-year play, while we’ll be ready to ship units in the coming months and iterate toward the best sensor that fits the most use cases.” Robotiq offers new tactile sensor fingertips for robot grippers. [https://www.therobotreport.com/wp-content/uploads/2026/01/Robotiq_tactile_sensors.jpg] The new tactile sensor fingertips are designed to be manufactured at scale. Source: Robotiq TSF-85 SENSORS TO PROVIDE DATA FOR PHYSICAL AI Robotiq said it shares best practices for tactile data handling, including guidance on bias management, normalization, and outlier detection, to help teams generate consistent, high-quality data for model training. It asserted that its sensors enable robots to understand contact geometry, detect incipient slip, and improve generalization across diverse objects, all of which are necessary for physical AI [https://www.therobotreport.com/category/design-development/ai-cognition/] datasets. “It’s important to make sure that you’re aligning your data collection to the appropriate tasks so that you’re not wasting time,” said Kwiatkowski. She cited three mechanisms of the human sense of touch [https://www.therobotreport.com/category/design-development/haptics/] — sustained or fast changes in pressure, vibration, and configuration of the hands. These shape perception of an object’s shape as well as grasping and manipulation capabilities. The lack of reliable data across sensor [https://www.therobotreport.com/category/technologies/sensors-sensing/] modalities has hindered the development and production of physical AI, according to Robotiq. The company said that its standardized hardware and tactile data provide a base for reinforcement learning, vision-language-action (VLA) models, and imitation learning. No matter how good simulations [https://www.therobotreport.com/category/software-simulation/] may be, developers still need real-world data as they build toward generalized systems, Kwiatkowski said. “At the Humanoids Summit, I heard how physical AI is growing into operational AI, but it needs 99.9% reliability,” she recalled. “To build physical AI that truly works, you need hardware that can sense, respond, and learn from every interaction,” said Aleksei Filippov, head of business development at Yango Tech Robotics [https://robotics.yango.com/]. “That’s why we chose Robotiq. With Robotiq precision force control and reliable feedback,",
        "excerpt": "Robotiq says it has combined adaptive gripping with high-frequency tactile sensing, enabling robots to generalize across objects.\nThe post Robotiq brings sense of touch to physical AI with fingertips "
      },
      {
        "title": "Big robot on campus: Starship finds 97% student approval rating",
        "link": "https://www.therobotreport.com/big-robot-campus-starship-finds-97-student-approval-rating/",
        "pubDate": "2026-01-27T14:01:02.000Z",
        "content": "Students on a college campus smiling with a delivery robot from Starship Technologies. [https://www.therobotreport.com/wp-content/uploads/2026/01/Students-smiling-copy.jpg] Students have rapidly accepted delivery robots on campus. Source: Jessica Foster, Starship Technologies While it may not be surprising that Generation Z is open to new technology, a survey of 5,000 students across 65 U.S. college campuses found that 95% “like” or “love” delivery robots from Starship Technologies Inc. The company called this a “generational shift in how humans coexist with autonomous helpers.” “Campuses have long been the birthplace of the world’s most transformative ideas, and today they’re once again leading the way,” said Ahti Heinla, co-founder and CEO of Starship. “What began as a convenient delivery option has grown into a new social standard” “This generation is proving that autonomous technology can coexist in our human communities, redefining the future of urban cities,” he stated. “We’re showing the world that it’s fully automated and frictionless.” Founded in 2014 by Heinla, former chief architect of Skype,and Janus Friis, co-founder of Skype, Starship claimed to be the world’s No. 1 autonomous delivery company [https://www.therobotreport.com/tag/starship-technologies/]. It said it has completed more than [https://www.therobotreport.com/starship-technologies-surpasses-8m-autonomous-deliveries/] 9 million deliveries [https://www.therobotreport.com/tag/delivery-robots/] and operates over 2,700 robots across 270+ locations in seven countries. Starship raised [https://www.therobotreport.com/starship-technologies-obtains-series-c-funding-for-autonomous-deliveries/] $50 million in funding in October 2025, bringing its total funding to more than $280 million. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- FAMILIARITY WITH ROBOTS BUILDS CAMPUS ACCEPTANCE Starship said the results from its 2025 Campus User Survey showed robots receiving one of the highest approval ratings for any AI-powered technology. In addition, 33% of students said they experienced the technology for the first time, which the company said suggested an “early adopter” mindset. A Starship robot at the Northern Arizona University campus. [https://www.therobotreport.com/wp-content/uploads/2026/01/NAU-robot-copy-238x300.jpg] A delivery robot at the Northern Arizona University campus. Source: Jessica Foster, Starship “The robots have become an essential part of everyday life in the U.S., completing almost 7 million orders and traveling nearly 8 million miles across U.S. campuses since 2019,” the company said. “In 2025, they completed 1.5 million miles on U.S. campuses — nearly six trips to the moon — reliably delivering in rain, snow, and freezing temperatures.” Almost three-quarters (72%) of the survey respondents described Starship’s robots as “friendly/cute,” and two thirds (65%) reported that their opinions became “more positive since seeing or using them.” Students often name, help, and treat the robots like mascots, said the company [https://www.starship.xyz/]. It asserted that the survey results show that autonomous technology is as much about culture as it is about technology. Oregon State has one of the largest Starship deployments, with 265,000 orders in 2025 and 1.2 million orders total since launch. STARSHIP TOUTS BENEFITS TO STUDENT HEALTH, STUDIES Beyond practicality, the sidewalk robots have delivered unexpected health benefits on college campuses, noted Starship. They can help students eat more regularly, avoid unsafe late-night walks, manage stress and illness, and access meals despite mobility or social-anxiety challenges. Nearly 40% of students surveyed said delivery robots improve food accessibility, while 1 in 4 reported feeling safer using contactless delivery, particularly during late-night study sessions or poor weather. Students also cited reduced stress around meal access when sick, injured, or overwhelmed during exam periods. Similar to last year’s survey results, over half of those surveyed said they can study more effectively (54%), avoid skipping meals (60%), and save time (51%). “Starship delivery robots have been extremely helpful on campus,” said Amelia Ott, a junior at Purdue University. “Whether I’m studying late, or rushing between classes, they’re an easy, reliable way to get meals. The robots are always on time and can navigate through any terrain or bad weather. They’ve become part of everyday campus life and are fun to see around campus too.” Starship operates on 65 U.S. campuses and plans to expand further in 2026. The San Francisco-based company [https://www.automatedwarehouseonline.com/tag/starship-technologies/] said its delivery robots are becoming a common sight on campuses and cities around the world rather than a novelty. The post Big robot on campus: Starship finds 97% student approval rating [https://www.therobotreport.com/big-robot-campus-starship-finds-97-student-approval-rating/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "As delivery robots become more common on campus, they are finding wide acceptance, found a Starship survey.\nThe post Big robot on campus: Starship finds 97% student approval rating appeared first on T"
      },
      {
        "title": "Multiply Labs partners with AstraZeneca to automate cell therapy manufacturing",
        "link": "https://www.therobotreport.com/multiply-labs-partners-astrazeneca-automate-cell-therapy-manufacturing/",
        "pubDate": "2026-01-27T14:00:53.000Z",
        "content": "Multiply Labs offers a modular approach to Cell Therapy automation that it says reduces manufacturing bottlenecks. [https://www.therobotreport.com/wp-content/uploads/2026/01/multiplylabs-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/multiplylabs-featured.jpg Multiply Labs said its modular approach to cell therapy automation reduces manufacturing bottlenecks. | Source: Multiply Labs Multiply Labs Inc. today announced an agreement with AstraZeneca to evaluate the potential for applying good manufacturing practice, or GMP-ready, robotic systems to commercial-scale cell therapy manufacturing. The collaboration will focus on automation of industry-standard instruments used in cell therapy production using Multiply Labs’ robotic biomanufacturing system. The goal is to enable scalable, high-throughput manufacturing [https://www.therobotreport.com/category/markets-industries/manufacturing/] while maintaining the rigorous quality and regulatory standards required for clinical and commercial use. “Cell therapies are among the most promising, yet complex medicines being developed today,” said Fred Parietti, Ph.D., CEO of Multiply Labs. “Our mission is to make these therapies more widely available by increasing manufacturing efficiency and scale.” “This agreement with AstraZeneca allows us to evaluate our multi-arm robotic clusters in a setting where we can combine some of the world’s best scientific and clinical expertise with our robotic platform to build the next generation of high-throughput, GMP-ready cell therapy manufacturing,” he added. FOUR-ARMED ROBOT MAXIMIZES PHARMACEUTICAL PRODUCTION Founded in 2016, Multiply Labs provides automation to the pharmaceutical [https://www.therobotreport.com/tag/pharmaceutical/] industry. It develops cloud-controlled robotics for the production of advanced therapies at scale. The San Francisco-based company [https://multiplylabs.com/]‘s newest systems [https://multiplylabs.com/products/cell-therapy/] use four robotic arms operating in parallel to run a broad range of cell therapy manufacturing instruments already in use by the industry. It said this architecture minimizes the need for process modifications while maximizing output in existing facilities, targeting higher productivity. Multiply Labs claimed that its customers include some of the largest pharmaceutical manufacturers in the world. The company asserted that its expertise is at the intersection of robotics and biopharma – its team includes mechanical engineers, electrical engineers, computer scientists, software engineers, and pharmaceutical scientists. MULTIPLY LABS BUILDS PARTNER NETWORK This isn’t the first partnership Multiply Labs has made in its efforts to automate cell therapy manufacturing. In September, it partnered [https://multiplylabs.com/2025/09/30/thermo-dynacellect-announcement/] with Thermo Fisher Scientific. The collaboration is focusing on integrating robots with Thermo Fisher’s automated Gibco CTS DynaCellect Magnetic Separation System. The Gibco CTS DynaCellect System is a closed, automated isolation, activation, and bead-removal system for cell therapy development and manufacturing. Together with fit-for-purpose consumables, it offers high cell purity, recovery, and viability. By combining the Gibco CTS DynaCellect System with Multiply Labs’ robotic automation expertise, the partners said they hope to accelerate critical steps in cell therapy production by reducing manual intervention and improving process consistency. In June, Multiply Labs announced [https://multiplylabs.com/2025/06/12/kyverna-announcement/] a pilot collaboration with Kyverna Therapeutics, a clinical-stage biopharmaceutical company focused on developing cell therapies for patients with autoimmune diseases. Kyverna planned to evaluate the automation of its cell therapy manufacturing processes using Multiply Labs’ robotic systems for KYV-102. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- The post Multiply Labs partners with AstraZeneca to automate cell therapy manufacturing [https://www.therobotreport.com/multiply-labs-partners-astrazeneca-automate-cell-therapy-manufacturing/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Multiply Labs' goal is to enable scalable, high-throughput manufacturing while maintaining rigorous quality and regulatory standards.\nThe post Multiply Labs partners with AstraZeneca to automate cell "
      },
      {
        "title": "Vention raises $110M to accelerate physical AI deployments in manufacturing",
        "link": "https://www.therobotreport.com/vention-raises-110m-to-accelerate-physical-ai-deployments-in-manufacturing/",
        "pubDate": "2026-01-27T12:01:41.000Z",
        "content": "Vention is developing physical AI to unify and accelerate deployment of robotic workcells such as this one. [https://www.therobotreport.com/wp-content/uploads/2026/01/Vention_motion-plan.jpg] Vention is developing physical AI to unify and accelerate deployment of robotic workcells. Source: Vention Physical AI, the union of artificial intelligence and robotics, is moving toward wider use. Vention Inc. today said it has raised $110 million ($150 million CAD) in Series D funding to advance its research, add capabilities to its software, expand its portfolio of pre-engineered applications, and grow its presence from North America to Europe. Traditional automation is complex and requires a lot of integration work, according to the Montreal-based company [https://www.therobotreport.com/tag/vention/]. Vention said its Zero-Shot Automation platform combines hardware, software, physical AI, and cloud connectivity to enable faster and more scalable automation of production in support of reshoring [https://www.therobotreport.com/tag/reshoring]. “Manufacturers no longer want automation that requires deep expertise and long commissioning cycles,” stated Etienne Lacroix, CEO of Vention. “They want automation that works as intuitively and reliably as modern software. Physical AI is allowing us to deliver exactly that.” VENTION PLAYBOOK INCLUDES FOUR TARGETS “We’re making automation more accessible by moving integration activities to our online platform,” Lacroix told The Robot Report [https://www.therobotreport.com/]. “We’re going to use the the funds to power four playbooks. The first is serving enterprise clients.” “Vention has become a standard within enterprises — 40% of our revenue comes from companies with $1 billion or more in revenue,” he said. “In the past, automation was decentralized, with separate integrators. Enterprises ended up with orphan machines with spaghetti code.” Many manufacturers [https://www.therobotreport.com/category/markets-industries/manufacturing/] want turnkey offerings, in which everything on the factory floor is governed by standardized platforms while retaining access to data models, digital twins, and code, explained Lacroix. Vention’s second playbook is to continue commercializing physical AI [https://www.therobotreport.com/category/design-development/ai-cognition/] from its roots in research and development. “We’ve always been an innovation powerhouse — that’s why NVIDIA joined as an investor; it saw how fast we’re moving in real industrial use cases,” said Lacroix. NVIDIA [https://www.therobotreport.com/tag/nvidia/] last year partnered [https://www.therobotreport.com/vention-nvidia-partner-bringing-automation-small-manufacturers/] with Vention on motion control [https://www.therobotreport.com/category/robot-components/motioncontrol/] and vision AI, and the partners plan to announce new products soon, he added. Vention has launched automated configuration tools, an AI agent for defining machine specifications, a robotic programming co-pilot, and AI-powered robotic applications. It claimed that these tools can reduce automation project timelines from months to days. The third playbook is to build depth in Vention’s “end-to-end” offering. “We’re the broadest industrial tech stack [https://www.therobotreport.com/vention-announces-full-stack-ai-automation-platform-expansions/],” Lacroix asserted. “We do design, programming, simulation [https://www.therobotreport.com/category/software-simulation/], ordering, deployments from the cloud and digital twin to the floor, plus the operating layer and teleoperation [https://www.therobotreport.com/tag/teleoperation/]. We’re not the deepest in all those verticals, but we’re continuing to build that depth.” The fourth playbook is Vention’s expansion in Europe [https://www.therobotreport.com/tag/europe]. The company has about 310 employees and has 30 open positions. In 2025, it added senior project managers and U.S.-based salespeople and customer success team members. Vention also invested into physical AI researchers last year. MANUFACTURERS DEMAND RELIABILITY, SPEED Vention serves 25 discrete manufacturing industries, said Lacroix. Promising segments this year include defense [https://www.therobotreport.com/category/markets-industries/defense-security/], data center and AI, and end-of-line automation [https://www.therobotreport.com/category/robots-platforms/industrial-robots/]. “We have people in the U.S. certified to serve heavily regulated defense clients,” he noted. “Defense is trying to move fast in Canada and Europe, and a whole class of startups is emerging. There are already a few winners in drones and submarines.” “We don’t manufacture hyperscalers, but it turns out that a lot of their cooling systems are now assembled in Vention-automated assembly lines,” added Lacroix. “We’re putting a lot of emphasis this year on physical AI, because we can figure out a way to take the technology in its current state and create a 99% reliable application at the speed of a human.” “If a manufacturer is operating two shifts, it needs to figure out a way to deploy physical AI for roughly $150,000,” Lacroix said. “Physical AI applied to manufacturing can grasp and position physical objects very precisely. We have good traction in consumer packaged goods and with Fortune 500 companies. They have mostly brownfield facilities but want modular automation that can be deployed quickly.” He said Vention’s tech stack is robot-agnostic, providing robot compatibility without the need to tweak robot drivers. The company [https://vention.io/] started 10 years ago with the Robot Operating System (ROS [https://www.therobotreport.com/tag/robot-operating-system]), which was well-built but slow, recalled Lacroix. “Every half-second counts,” he said. “We built a more efficient robot control layer to be 1:1 with robot planners. They can’t live in controllers, they have to be in the cloud. The systems on the factory floor need to run at the same speed as native planners.” ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- VENTION STAYS FOCUSED ON GENERALIZING ITS PLATFORM New and existing investors in Vention included Investissement Québec, Desjardins Capital, certain funds managed by Fidelity Investments Canada ULC, and NVentures (NVIDIA’s venture capital arm). “We’re quite excited to reach this milestone and have raised $260 million in total capital,” said Lacroix. While humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] robots and generalized AI have garnered a lot of attention lately, Vention is staying focused on robot arms and autonomous mobile robots (AMRs [https://www.therobotreport.com/category/robots-platforms/amrs/]). It has deployed more than 25,000 machines worldwide in over 4,000 factories. “We have discussions all the time about humanoids specific to manufacturing,” Lacroix acknowledged. “We looked at generalized platforms for fit-for-purpose workcells and said, ‘Let’s not generalize the robot, but generalize the platform to lead fit-for-purpose robots.'” “Vention’s AI pipeline uses robust models for real-time collision avoidance and pose estimation that we couldn’t have dreamed of just a year ago,” he said. “We’ll have a stream of announcements in the coming months, and we’ll be at GTC [https://www.therobotreport.com/tag/gtc/], Automate [https://www.therobotreport.com/tag/automate/], and IMTS, as well as Pack Expo and Hannover Messe.” The post Vention raises $110M to accelerate physical AI deployments in manufacturing [https://www.therobotreport.com/vention-raises-110m-to-accelerate-physical-ai-deployments-in-manufacturing/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Vention has raised Series D funding to continue commercializing its robot control platform and expand into Europe.\nThe post Vention raises $110M to accelerate physical AI deployments in manufacturing "
      },
      {
        "title": "AAA20 Group debuts cobot palletizer for food and protein processing",
        "link": "https://www.therobotreport.com/aaa20-group-debuts-cobot-palletizer-food-protein-processing/",
        "pubDate": "2026-01-26T21:24:23.000Z",
        "content": "AAA20 Group's new CP-66-WD wash-down collaborative palletizer. [https://www.therobotreport.com/wp-content/uploads/2026/01/AAA20-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/AAA20-featured.jpg AAA20 Group’s new CP-66-WD wash-down collaborative palletizer. | Source: AAA20 Group AAA20 Group plans to debut its new CP-66-WD wash-down collaborative palletizer at the International Production & Processing Expo, or IPPE, in Atlanta this week. The robot is a sanitary automation system for food manufacturers, particularly beef processors. “Space constraints and stringent sanitation requirements have made automation difficult to deploy in the food industry, particularly in finding solutions to address protein processing labor shortages,” said Marcus Kurle, co-founder of AAA20 Group. “The CP-66-WD was engineered to remove those barriers, giving processors a compact, compliant way to automate one of the most demanding jobs while preserving capital budgets and delivering high hygienic standards,” he noted. Founded in 2020, AAA20 Group specializes in smart robotic palletizing [https://www.therobotreport.com/tag/palletizing/] systems. The Las Vegas-based company [https://aaa20group.com/] uses a robot-as-a-service (RaaS [https://www.automatedwarehouseonline.com/category/raas/]) model, which it says removes traditional barriers to automation and enables rapid deployment. Its systems are in use across the food [https://www.therobotreport.com/tag/food/], beverage, and industrial sectors. COLLABORATIVE PALLETIZER DESIGNED FOR FAST DEPLOYMENT As seen during the COVID-19 pandemic, protein processors have been among the most affected by workforce constraints, which increase production costs and limit throughput. By palletizing with wash-down-ready force- and power-limited robots, producers can improve efficiency, reduce reliance on hard-to-fill labor roles, and help stabilize costs throughout the food supply chain, AAA20 Group said. Traditional automation systems require safety cages, extensive construction, and large capital expenditures. Unlike those, AAA20’s collaborative palletizers are designed for fast deployment and offered through a month-to-month leasing model. This approach allows food [https://www.therobotreport.com/category/markets-industries/food/] and beef processors to automate at a lower cost than manual labor while retaining the flexibility to scale as production needs change. Engineered for food and protein processing environments where cleanliness is critical, the CP-66-WD is a waterproof, IP69K-rated collaborative robot [https://www.therobotreport.com/category/robots-platforms/collaborative-robot/] that can be power-washed like other production equipment. The system enables manufacturers to automate palletizing while meeting sanitation regulations across beef, poultry, and other food operations. The robot is priced at approximately $5,000 per month. AAA20 PRIORITIZES LOW CAPITAL INVESTMENT AAA20 Group said the launch follows recent industry momentum for, including a recent visit to the company’s Las Vegas robotics facility by members of the U.S. Small Business Administration (SBA). The visit, which brought SBA Administrator Kelly Loeffler and representatives from Washington, D.C., underscored growing federal interest in automation that strengthens the U.S. food system and supports domestic protein production. At IPPE, AAA20 Group plans to show the CP-66-WD for facilities that require sanitary automation, minimal footprint integration, and rapid deployment within existing production lines. The company invited food manufacturers and beef processors attending IPPE to visit Booth A2745 to see the CP-66-WD in action and learn how wash-down robotics can help address labor challenges, maintain compliance, and control costs. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- The post AAA20 Group debuts cobot palletizer for food and protein processing [https://www.therobotreport.com/aaa20-group-debuts-cobot-palletizer-food-protein-processing/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "AAA20 Group's CP-66-WD is a waterproof, IP69K-rated collaborative robot that can be power-washed like other production equipment.\nThe post AAA20 Group debuts cobot palletizer for food and protein proc"
      },
      {
        "title": "State of robotics industry report 2026",
        "link": "https://www.therobotreport.com/state-of-robotics-industry-report-2026/",
        "pubDate": "2026-01-26T18:15:11.000Z",
        "content": "[https://www.therobotreport.com/wp-content/uploads/2026/01/ChatGPT-Image-Jan-26-2026-01_06_56-PM-e1769451153613.png] Imaged created using OpenAI’s ChatGPT. The robotics industry enters 2026 at an inflection point. After years of innovation, bold claims, and headline-grabbing demonstrations, the conversation is shifting from what robots could do to what they can reliably do in the real world. The Robot Report‘s State of Robotics Industry Report 2026 examines this transition in detail, offering a clear-eyed assessment of where the market stands today and where it’s headed. This coverage draws on reporting, interviews, and analysis from across the global robotics ecosystem, spanning industrial automation, mobile robots, humanoids, autonomous vehicles, investments, and emerging technologies and applications. It explores how advances in AI, perception, simulation, compute, and edge software are reshaping robot capabilities. It also highlights the challenges facing developers, integrators, and end users, including safety and reliability to cost, regulation, and deployment timelines. Readers will find insights and expert perspectives on the technologies shaping 2026, including foundation models, vision-language-action systems, simulation-first development, and the evolving role of humanoids. The report also examines how geopolitical pressures, supply-chain realities, labor shortages, and shifting investor expectations are influencing product roadmaps and commercialization strategies. The report offers informed predictions on where adoption will accelerate, where hype is likely to cool, and which segments are poised for meaningful progress over the next 12 to 24 months. Whether you’re building robots, deploying them, or investing in the companies behind them, the State of Robotics Industry Report 2026 provides essential context for understanding the state of robotics in 2026. Fill out the form below to download the report for free. The post State of robotics industry report 2026 [https://www.therobotreport.com/state-of-robotics-industry-report-2026/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "This report draws on reporting, interviews, and analysis from across the global robotics ecosystem, spanning industrial automation, mobile robots, humanoids, autonomous vehicles, investments, and emer"
      },
      {
        "title": "Hadrian raises funding for automated manufacturing, bringing valuation to $1.6B",
        "link": "https://www.therobotreport.com/hadrian-brings-in-additional-funding-bringing-its-valuation-to-1-6b/",
        "pubDate": "2026-01-25T13:30:38.000Z",
        "content": "A rendering of a planned Hadrian facility in Mesa, Ariz. [https://www.therobotreport.com/wp-content/uploads/2026/01/Hadrian-factory-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/Hadrian-factory-featured.jpg A rendering of one of Hadrian’s announced facilities in Mesa, Ariz. | Source: Hadrian Hadrian, which uses AI-powered automation and modern software to build manufacturing facilities for aerospace, defense, and emerging industrial programs, recently announced expanded capital. With the latest investment, the company is valued at $1.6 billion. Hadrian said it plans to use the funding to accelerate factory expansion and advance its automated manufacturing roadmap. “For decades, the United States separated design from production and assumed global supply chains would carry the load,” stated Chris Power, the founder and CEO of Hadrian. “That assumption no longer holds,” he said. “This capital accelerates our ability to build the industrial capacity America needs by pairing advanced automation with workforce training designed for the scale of the opportunity in front of us.” OPUS DESIGNED TO PROPEL RESHORING OF MANUFACTURING Demand for domestic manufacturing [https://www.therobotreport.com/category/markets-industries/manufacturing/] capacity across aerospace, defense, and critical infrastructure continues to grow, noted Hadrian. The company [https://www.hadrian.co/] said it is building advanced factories designed to produce mission-critical components, assemblies, and full product lines with speed, reliability, and scale. Hadrian said its platform pairs advanced automation with a rapidly trained workforce to meet the urgent need for a generational re-industrialization effort. This is part of its factories-as-a-service (FaaS) initiative. Opus, Hadrian’s proprietary software stack for production autonomy, powers its factories. It said its manufacturing facilities can go online in under six months. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- HADRIAN BUILDS ON FUNDING TO EXPAND DEFENSE PRODUCTION Hadrian said the latest capital will position it to move faster in scaling high-throughput American factories. The investment was led by accounts advised by T. Rowe Price Associates Inc. It included participation from Altimeter Capital, D1 Capital Partners, StepStone Group, 1789 Capital, Founders Fund, Lux Capital, a16z, Construct Capital, and existing investors. This funding builds on a $260 million round [https://www.prnewswire.com/news-releases/hadrian-raises-260m-to-build-ai-powered-factories-for-america-adds-full-product-manufacturing-opens-arizona-site-302508179.html] from July 2025. Hadrian used that investment to support nearly five football fields’ worth of new manufacturing space, expanded research and development capacity, and dedicated teams focused on shipbuilding and naval [https://www.therobotreport.com/category/maritime/] defense production. Hadrian said it plans to use the latest capital to: * Accelerate factory expansion * Scale workforce training programs * Continue investment in automation, AI [https://www.therobotreport.com/category/design-development/ai-cognition/]-driven tooling, and real-time manufacturing intelligence Last week, Hadrian launched [https://www.prnewswire.com/news-releases/hadrian-launches-additive-manufacturing-division-to-expand-us-defense-production-capacity-302668241.html] Hadrian Additive, a division dedicated to delivering scalable, production-ready additive manufacturing capacity for the U.S. defense [https://www.therobotreport.com/category/markets-industries/defense-security/] industrial base and allied partners. The new division expands Hadrian’s Opus platform to include 3D printing systems built for qualification, repeatability, and sustained throughput. It is intended to enable defense programs to move from validated designs into reliable, at-scale production. Initial additive manufacturing capacity is expected to come online in 2026 as part of Hadrian’s expanding U.S. factory footprint. The post Hadrian raises funding for automated manufacturing, bringing valuation to $1.6B [https://www.therobotreport.com/hadrian-brings-in-additional-funding-bringing-its-valuation-to-1-6b/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Hadrian plans to use the funding to accelerate factory expansion and advance the company's manufacturing roadmap.\nThe post Hadrian raises funding for automated manufacturing, bringing valuation to $1."
      }
    ],
    "arxiv": [
      {
        "id": "2601.21011",
        "title": "Meta-ROS: A Next-Generation Middleware Architecture for Adaptive and Scalable Robotic Systems",
        "authors": [
          "Anshul Ranjan",
          "Anoosh Damodar",
          "Neha Chougule",
          "Dhruva S Nayak",
          "Anantharaman P.N",
          "Shylaja S S"
        ],
        "abstract": "Title:\n          Meta-ROS: A Next-Generation Middleware Architecture for Adaptive and Scalable Robotic Systems\n        Comments:\n          Checkout the Python Library - this https URL To be Submitted in ACM Transactions on Autonomous and Adaptive Systems (TAAS) Journal\n        \n          The field of robotics faces significant challenges related to the complexity and interoperability of existing middleware frameworks, like ROS2, which can be difficult for new developers to adopt. To address these issues, we propose Meta-ROS, a novel middleware solution designed to streamline robotics development by simplifying integration, enhancing performance, and ensuring cross-platform compatibility. Meta-ROS leverages modern communication protocols, such as Zenoh and ZeroMQ, to enable efficient and low-latency communication across diverse hardware platforms, while also supporting various data types like audio, images, and video. We evaluated Meta-ROS's performance through comprehensive testing, comparing it with existing middleware frameworks like ROS1 and ROS2. The results demonstrated that Meta-ROS outperforms ROS2, achieving up to 30% higher throughput, significantly reducing message latency, and optimizing resource usage. Additionally, its robust hardware support and developer-centric design facilitate seamless integration and ease of use, positioning Meta-ROS as an ideal solution for modern, real-time robotics AI applications.",
        "link": "https://arxiv.org/abs/2601.21011",
        "submittedDate": "2026-01-30T04:46:07.297Z"
      },
      {
        "id": "2601.21063",
        "title": "Multi-Robot Decentralized Collaborative SLAM in Planetary Analogue Environments: Dataset, Challenges, and Lessons Learned",
        "authors": [
          "Pierre-Yves Lajoie",
          "Karthik Soma",
          "Haechan Mark Bong",
          "Alice Lemieux-Bourque",
          "Rongge Zhang",
          "Vivek Shankar Varadharajan",
          "Giovanni Beltrame"
        ],
        "abstract": "Title:\n          Multi-Robot Decentralized Collaborative SLAM in Planetary Analogue Environments: Dataset, Challenges, and Lessons Learned\n        \n          Decentralized collaborative simultaneous localization and mapping (C-SLAM) is essential to enable multirobot missions in unknown environments without relying on preexisting localization and communication infrastructure. This technology is anticipated to play a key role in the exploration of the Moon, Mars, and other planets. In this article, we share insights and lessons learned from C-SLAM experiments involving three robots operating on a Mars analogue terrain and communicating over an ad hoc network. We examine the impact of limited and intermittent communication on C-SLAM performance, as well as the unique localization challenges posed by planetary-like environments. Additionally, we introduce a novel dataset collected during our experiments, which includes real-time peer-to-peer inter-robot throughput and latency measurements. This dataset aims to support future research on communication-constrained, decentralized multirobot operations.",
        "link": "https://arxiv.org/abs/2601.21063",
        "submittedDate": "2026-01-30T04:46:07.298Z"
      },
      {
        "id": "2601.21129",
        "title": "WheelArm-Sim: A Manipulation and Navigation Combined Multimodal Synthetic Data Generation Simulator for Unified Control in Assistive Robotics",
        "authors": [
          "Guangping Liu",
          "Tipu Sultan",
          "Vittorio Di Giorgio",
          "Nick Hawkins",
          "Flavio Esposito",
          "Madi Babaiasl"
        ],
        "abstract": "Title:\n          WheelArm-Sim: A Manipulation and Navigation Combined Multimodal Synthetic Data Generation Simulator for Unified Control in Assistive Robotics\n        Comments:\n          Accepted to IEEE International Symposium on Medical Robotics (ISMR) 2026\n        \n          Wheelchairs and robotic arms enhance independent living by assisting individuals with upper-body and mobility limitations in their activities of daily living (ADLs). Although recent advancements in assistive robotics have focused on Wheelchair-Mounted Robotic Arms (WMRAs) and wheelchairs separately, integrated and unified control of the combination using machine learning models remains largely underexplored. To fill this gap, we introduce the concept of WheelArm, an integrated cyber-physical system (CPS) that combines wheelchair and robotic arm controls. Data collection is the first step toward developing WheelArm models. In this paper, we present WheelArm-Sim, a simulation framework developed in Isaac Sim for synthetic data collection. We evaluate its capability by collecting a manipulation and navigation combined multimodal dataset, comprising 13 tasks, 232 trajectories, and 67,783 samples. To demonstrate the potential of the WheelArm dataset, we implement a baseline model for action prediction in the mustard-picking task. The results illustrate that data collected from WheelArm-Sim is feasible for a data-driven machine learning model for integrated control.",
        "link": "https://arxiv.org/abs/2601.21129",
        "submittedDate": "2026-01-30T04:46:07.298Z"
      },
      {
        "id": "2601.21188",
        "title": "Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation",
        "authors": [
          "Hao Cheng",
          "Feitian Zhang"
        ],
        "abstract": "Title:\n          Disturbance-Aware Flight Control of Robotic Gliding Blimp via Moving Mass Actuation\n        \n          Robotic blimps, as lighter-than-air (LTA) aerial systems, offer long endurance and inherently safe operation but remain highly susceptible to wind disturbances. Building on recent advances in moving mass actuation, this paper addresses the lack of disturbance-aware control frameworks for LTA platforms by explicitly modeling and compensating for wind-induced effects. A moving horizon estimator (MHE) infers real-time wind perturbations and provides these estimates to a model predictive controller (MPC), enabling robust trajectory and heading regulation under varying wind conditions. The proposed approach leverages a two-degree-of-freedom (2-DoF) moving-mass mechanism to generate both inertial and aerodynamic moments for attitude and heading control, thereby enhancing flight stability in disturbance-prone environments. Extensive flight experiments under headwind and crosswind conditions show that the integrated MHE-MPC framework significantly outperforms baseline PID control, demonstrating its effectiveness for disturbance-aware LTA flight.",
        "link": "https://arxiv.org/abs/2601.21188",
        "submittedDate": "2026-01-30T04:46:07.298Z"
      },
      {
        "id": "2601.21251",
        "title": "Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies",
        "authors": [
          "Ce Hao",
          "Xuanran Zhai",
          "Yaohua Liu",
          "Harold Soh"
        ],
        "abstract": "Title:\n          Abstracting Robot Manipulation Skills via Mixture-of-Experts Diffusion Policies\n        \n          Diffusion-based policies have recently shown strong results in robot manipulation, but their extension to multi-task scenarios is hindered by the high cost of scaling model size and demonstrations. We introduce Skill Mixture-of-Experts Policy (SMP), a diffusion-based mixture-of-experts policy that learns a compact orthogonal skill basis and uses sticky routing to compose actions from a small, task-relevant subset of experts at each step. A variational training objective supports this design, and adaptive expert activation at inference yields fast sampling without oversized backbones. We validate SMP in simulation and on a real dual-arm platform with multi-task learning and transfer learning tasks, where SMP achieves higher success rates and markedly lower inference cost than large diffusion baselines. These results indicate a practical path toward scalable, transferable multi-task manipulation: learn reusable skills once, activate only what is needed, and adapt quickly when tasks change.",
        "link": "https://arxiv.org/abs/2601.21251",
        "submittedDate": "2026-01-30T04:46:07.298Z"
      },
      {
        "id": "2601.21363",
        "title": "Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control",
        "authors": [
          "Weidong Huang",
          "Zhehan Li",
          "Hangxin Liu",
          "Biao Hou",
          "Yao Su",
          "Jingwen Zhang"
        ],
        "abstract": "Title:\n          Towards Bridging the Gap between Large-Scale Pretraining and Efficient Finetuning for Humanoid Control\n        Comments:\n          ICLR 2026\n        \n          Reinforcement learning (RL) is widely used for humanoid control, with on-policy methods such as Proximal Policy Optimization (PPO) enabling robust training via large-scale parallel simulation and, in some cases, zero-shot deployment to real robots. However, the low sample efficiency of on-policy algorithms limits safe adaptation to new environments. Although off-policy RL and model-based RL have shown improved sample efficiency, the gap between large-scale pretraining and efficient finetuning on humanoids still exists. In this paper, we find that off-policy Soft Actor-Critic (SAC), with large-batch update and a high Update-To-Data (UTD) ratio, reliably supports large-scale pretraining of humanoid locomotion policies, achieving zero-shot deployment on real robots. For adaptation, we demonstrate that these SAC-pretrained policies can be finetuned in new environments and out-of-distribution tasks using model-based methods. Data collection in the new environment executes a deterministic policy while stochastic exploration is instead confined to a physics-informed world model. This separation mitigates the risks of random exploration during adaptation while preserving exploratory coverage for improvement. Overall, the approach couples the wall-clock efficiency of large-scale simulation during pretraining with the sample efficiency of model-based learning during fine-tuning.",
        "link": "https://arxiv.org/abs/2601.21363",
        "submittedDate": "2026-01-30T04:46:07.299Z"
      },
      {
        "id": "2601.21416",
        "title": "Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation",
        "authors": [
          "Alexandre Chapin (LIRIS)",
          "Bruno Machado (LIRIS)",
          "Emmanuel Dellandréa (LIRIS)",
          "Liming Chen (LIRIS)"
        ],
        "abstract": "Title:\n          Spotlighting Task-Relevant Features: Object-Centric Representations for Better Generalization in Robotic Manipulation\n        \n          The generalization capabilities of robotic manipulation policies are heavily influenced by the choice of visual representations. Existing approaches typically rely on representations extracted from pre-trained encoders, using two dominant types of features: global features, which summarize an entire image via a single pooled vector, and dense features, which preserve a patch-wise embedding from the final encoder layer. While widely used, both feature types mix task-relevant and irrelevant information, leading to poor generalization under distribution shifts, such as changes in lighting, textures, or the presence of distractors. In this work, we explore an intermediate structured alternative: Slot-Based Object-Centric Representations (SBOCR), which group dense features into a finite set of object-like entities. This representation permits to naturally reduce the noise provided to the robotic manipulation policy while keeping enough information to efficiently perform the task. We benchmark a range of global and dense representations against intermediate slot-based representations, across a suite of simulated and real-world manipulation tasks ranging from simple to complex. We evaluate their generalization under diverse visual conditions, including changes in lighting, texture, and the presence of distractors. Our findings reveal that SBOCR-based policies outperform dense and global representation-based policies in generalization settings, even without task-specific pretraining. These insights suggest that SBOCR is a promising direction for designing visual systems that generalize effectively in dynamic, real-world robotic environments.",
        "link": "https://arxiv.org/abs/2601.21416",
        "submittedDate": "2026-01-30T04:46:07.299Z"
      },
      {
        "id": "2601.21548",
        "title": "Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning",
        "authors": [
          "Irene Ambrosini",
          "Ingo Blakowski",
          "Dmitrii Zendrikov",
          "Cristiano Capone",
          "Luna Gava",
          "Giacomo Indiveri",
          "Chiara De Luca",
          "Chiara Bartolozzi"
        ],
        "abstract": "Title:\n          Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning\n        \n          Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task's temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.",
        "link": "https://arxiv.org/abs/2601.21548",
        "submittedDate": "2026-01-30T04:46:07.300Z"
      },
      {
        "id": "2601.21829",
        "title": "GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration",
        "authors": [
          "Bsher Karbouj",
          "Baha Eddin Gaaloul",
          "Jorg Kruger"
        ],
        "abstract": "Title:\n          GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration\n        \n          This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccades, eye gaze, gaze transition entropy, fixation dispersion index) with environmental real-time and continuous measurements (illuminance) and task and robot context (bench, task block, induced faults), under controlled manipulations of task difficulty and ambient conditions. For each participant and workload-graded task block, we provide CSV files with ocular metrics aggregated into 250 ms windows, environmental logs, and self-reported mental workload ratings on a 1-10 Likert scale, organized in participant-specific folders alongside documentation. These data can be used to develop and benchmark algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers.",
        "link": "https://arxiv.org/abs/2601.21829",
        "submittedDate": "2026-01-30T04:46:07.300Z"
      },
      {
        "id": "2601.21926",
        "title": "Information Filtering via Variational Regularization for Robot Manipulation",
        "authors": [
          "Jinhao Zhang",
          "Wenlong Xia",
          "Yaojia Wang",
          "Zhexuan Zhou",
          "Huizhe Li",
          "Yichen Lai",
          "Haoming Song",
          "Youmin Gong",
          "Jie Me"
        ],
        "abstract": "Title:\n          Information Filtering via Variational Regularization for Robot Manipulation\n        \n          Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills. However, most existing methods employ an oversized denoising decoder. While increasing model capacity can improve denoising, empirical evidence suggests that it also introduces redundancy and noise in intermediate feature blocks. Crucially, we find that randomly masking backbone features at inference time (without changing training) can improve performance, confirming the presence of task-irrelevant noise in intermediate features. To this end, we propose Variational Regularization (VR), a lightweight module that imposes a timestep-conditioned Gaussian over backbone features and applies a KL-divergence regularizer, forming an adaptive information bottleneck. Extensive experiments on three simulation benchmarks (RoboTwin2.0, Adroit, and MetaWorld) show that, compared to the baseline DP3, our approach improves the success rate by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld, achieving new state-of-the-art results. Real-world experiments further demonstrate that our method performs well in practical deployments. Code will released.",
        "link": "https://arxiv.org/abs/2601.21926",
        "submittedDate": "2026-01-30T04:46:07.300Z"
      },
      {
        "id": "2601.22074",
        "title": "mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning",
        "authors": [
          "Kevin Zakka",
          "Qiayuan Liao",
          "Brent Yi",
          "Louis Le Lay",
          "Koushil Sreenath",
          "Pieter Abbeel"
        ],
        "abstract": "Title:\n          mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning\n        Comments:\n          Code is available at this https URL\n        \n          We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.",
        "link": "https://arxiv.org/abs/2601.22074",
        "submittedDate": "2026-01-30T04:46:07.300Z"
      },
      {
        "id": "2601.21570",
        "title": "EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots",
        "authors": [
          "Zixing Lei",
          "Genjia Liu",
          "Yuanshuo Zhang",
          "Qipeng Liu",
          "Chuan Wen",
          "Shanghang Zhang",
          "Wenzhao Lian",
          "Siheng Chen"
        ],
        "abstract": "Title:\n          EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots\n        Comments:\n          37 pages, 13 figures\n        \n          The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs' success in software automation and science discovery, we introduce \\textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.",
        "link": "https://arxiv.org/abs/2601.21570",
        "submittedDate": "2026-01-30T04:46:07.300Z"
      },
      {
        "id": "2601.21998",
        "title": "Causal World Modeling for Robot Control",
        "authors": [
          "Lin Li",
          "Qihang Zhang",
          "Yiming Luo",
          "Shuai Yang",
          "Ruilin Wang",
          "Fei Han",
          "Mingrui Yu",
          "Zelin Gao",
          "Nan Xue",
          "Xing Zhu",
          "Yujun Shen",
          "Yinghao Xu"
        ],
        "abstract": "Title:\n          Causal World Modeling for Robot Control\n        Comments:\n          Project page: this https URL Code: this https URL\n        \n          This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.",
        "link": "https://arxiv.org/abs/2601.21998",
        "submittedDate": "2026-01-30T04:46:07.301Z"
      },
      {
        "id": "2504.02492",
        "title": "Industrial Internet Robot Collaboration System and Edge Computing Optimization",
        "authors": [
          "Haopeng Zhao",
          "Dajun Tao",
          "Tian Qi",
          "Jingyuan Xu",
          "Zijie Zhou",
          "Lipeng Liu"
        ],
        "abstract": "Title:\n          Industrial Internet Robot Collaboration System and Edge Computing Optimization\n        \n          In industrial Internet environments, mobile robots must generate collision-free global routes under stochastic obstacle layouts and random perturbations in commanded linear and angular velocities. This paper models a differential-drive robot with nonholonomic constraints, then decomposes motion into obstacle avoidance, target turning, and target approaching behaviors to parameterize the control variables. Global path planning is formulated as a constrained optimization problem and converted into a weighted energy function that balances path length and collision penalties. A three-layer neural network represents the planning model, while simulated annealing searches for near-global minima and mitigates local traps. During execution, a fuzzy controller uses heading and lateral-offset errors to output wheel-speed differentials for rapid correction; edge-side computation is discussed to reduce robot-server traffic and latency. Matlab 2024 simulations report deviation within +-5 cm, convergence within 10 ms, and shorter paths than two baseline methods. The approach improves robustness of global navigation in practice.",
        "link": "https://arxiv.org/abs/2504.02492",
        "submittedDate": "2026-01-30T04:46:07.301Z"
      },
      {
        "id": "2507.22188",
        "title": "Transport and Delivery of Objects with a Soft Everting Robot",
        "authors": [
          "Ethan DeVries",
          "Jack Ferlazzo",
          "Mustafa Ugur",
          "Laura H. Blumenschein"
        ],
        "abstract": "Title:\n          Transport and Delivery of Objects with a Soft Everting Robot\n        Comments:\n          8 pages, 11 figures, Published in IEEE Robotics and Automation Letters Link to publication: this https URL Citation: E. M. DeVries, J. Ferlazzo, M. Ugur and L. H. Blumenschein, \"Transport and Delivery of Objects With a Soft Everting Robot,\" in IEEE Robotics and Automation Letters, vol. 11, no. 3, pp. 2935-2942, March 2026, doi: https://doi.org/10.1109/LRA.2026.3655537\n        \n          Soft everting robots present significant advantages over traditional rigid robots, including enhanced dexterity, improved environmental interaction, and safe navigation in unpredictable environments. While soft everting robots have been widely demonstrated for exploration type tasks, their potential to move and deploy payloads in such tasks has been less investigated, with previous work focusing on sensors and tools for the robot. Leveraging the navigation capabilities, and deployed body, of the soft everting robot to deliver payloads in hazardous areas, e.g. carrying a water bottle to a person stuck under debris, would represent a significant capability in many applications. In this work, we present an analysis of how soft everting robots can be used to deploy larger, heavier payloads through the inside of the robot. We analyze both what objects can be deployed and what terrain features they can be carried through. Building on existing models, we present methods to quantify the effects of payloads on robot growth and self-support, and develop a model to predict payload slip. We then experimentally quantify payload transport using soft everting robot with a variety of payload shapes, sizes, and weights and though a series of tasks: steering, vertical transport, movement through holes, and movement across gaps. Overall, the results show that we can transport payloads in a variety of shapes and up to 1.5kg in weight and that we can move through circular apertures with as little as 0.01cm clearance around payloads, carry out discrete turns up to 135 degrees, and move across unsupported gaps of 1.15m in length.",
        "link": "https://arxiv.org/abs/2507.22188",
        "submittedDate": "2026-01-30T04:46:07.301Z"
      },
      {
        "id": "2511.19859",
        "title": "Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation",
        "authors": [
          "Xiangkai Ma",
          "Lekai Xing",
          "Han Zhang",
          "Wenzhong Li",
          "Sanglu Lu"
        ],
        "abstract": "Title:\n          Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation\n        \n          Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual observations and low-level actions, and (ii) unstable training due to competing objectives between visual prediction and action generation. To address these challenges, we propose a Vision-Integrated Trajectory Alignment (VITA) framework that learns a shared discrete latent space for vision and action, enabling joint modeling of perception and motor control. VITA introduces a implicit visual CoT: autoregressively generated tokens is simultaneously decoded into future frames predictions and robot actions, thereby internalizing visual dynamics as an inductive bias for motion planning. Extensive experiments on simulated and real-world environments demonstrate state-of-the-art performance. VITA improves 14.5\\%, 9.6\\% and 12.1\\% over existing baselines on CALVIN, LIBERO and SimplerEnv. Furthermore, VITA attains an average success rate of 80.5\\% across six real-world tasks, demonstrating its potential as a generalist robotic manipulation model.",
        "link": "https://arxiv.org/abs/2511.19859",
        "submittedDate": "2026-01-30T04:46:07.301Z"
      },
      {
        "id": "2601.07362",
        "title": "Large-Scale Autonomous Gas Monitoring for Volcanic Environments: A Legged Robot on Mount Etna",
        "authors": [
          "Julia Richter",
          "Turcan Tuna",
          "Manthan Patel",
          "Takahiro Miki",
          "Devon Higgins",
          "James Fox",
          "Cesar Cadena",
          "Andres Diaz",
          "Marco Hutter"
        ],
        "abstract": "Title:\n          Large-Scale Autonomous Gas Monitoring for Volcanic Environments: A Legged Robot on Mount Etna\n        Comments:\n          This work has been submitted to the IEEE for possible publication. Submitted to IEEE Robotics & Automation Magazine (RAM)\n        \n          Volcanic gas emissions are key precursors of eruptive activity. Yet, obtaining accurate near-surface measurements remains hazardous and logistically challenging, motivating the need for autonomous solutions. Limited mobility in rough volcanic terrain has prevented wheeled systems from performing reliable in situ gas measurements, reducing their usefulness as sensing platforms. We present a legged robotic system for autonomous volcanic gas analysis, utilizing the quadruped ANYmal, equipped with a quadrupole mass spectrometer system. Our modular autonomy stack integrates a mission planning interface, global planner, localization framework, and terrain-aware local navigation. We evaluated the system on Mount Etna across three autonomous missions in varied terrain, achieving successful gas-source detections with autonomy rates of 93-100%. In addition, we conducted a teleoperated mission in which the robot measured natural fumaroles, detecting sulfur dioxide and carbon dioxide. We discuss lessons learned from the gas-analysis and autonomy perspectives, emphasizing the need for adaptive sensing strategies, tighter integration of global and local planning, and improved hardware design.",
        "link": "https://arxiv.org/abs/2601.07362",
        "submittedDate": "2026-01-30T04:46:07.302Z"
      },
      {
        "id": "2601.17550",
        "title": "AsterNav: Autonomous Aerial Robot Navigation In Darkness Using Passive Computation",
        "authors": [
          "Deepak Singh",
          "Shreyas Khobragade",
          "Nitin J. Sanket"
        ],
        "abstract": "Title:\n          AsterNav: Autonomous Aerial Robot Navigation In Darkness Using Passive Computation\n        Comments:\n          8 pages, 10 figures, Published in IEEE Robotics And Automation Letters\n        \n          Autonomous aerial navigation in absolute darkness is crucial for post-disaster search and rescue operations, which often occur from disaster-zone power outages. Yet, due to resource constraints, tiny aerial robots, perfectly suited for these operations, are unable to navigate in the darkness to find survivors safely. In this paper, we present an autonomous aerial robot for navigation in the dark by combining an Infra-Red (IR) monocular camera with a large-aperture coded lens and structured light without external infrastructure like GPS or motion-capture. Our approach obtains depth-dependent defocus cues (each structured light point appears as a pattern that is depth dependent), which acts as a strong prior for our AsterNet deep depth estimation model. The model is trained in simulation by generating data using a simple optical model and transfers directly to the real world without any fine-tuning or retraining. AsterNet runs onboard the robot at 20 Hz on an NVIDIA Jetson Orin$^\\text{TM}$ Nano. Furthermore, our network is robust to changes in the structured light pattern and relative placement of the pattern emitter and IR camera, leading to simplified and cost-effective construction. We successfully evaluate and demonstrate our proposed depth navigation approach AsterNav using depth from AsterNet in many real-world experiments using only onboard sensing and computation, including dark matte obstacles and thin ropes (diameter 6.25mm), achieving an overall success rate of 95.5% with unknown object shapes, locations and materials. To the best of our knowledge, this is the first work on monocular, structured-light-based quadrotor navigation in absolute darkness.",
        "link": "https://arxiv.org/abs/2601.17550",
        "submittedDate": "2026-01-30T04:46:07.302Z"
      },
      {
        "id": "2601.19510",
        "title": "ALRM: Agentic LLM for Robotic Manipulation",
        "authors": [
          "Vitor Gaboardi dos Santos",
          "Ibrahim Khadraoui",
          "Ibrahim Farhat",
          "Hamza Yous",
          "Samy Teffahi",
          "Hakim Hacid"
        ],
        "abstract": "Title:\n          ALRM: Agentic LLM for Robotic Manipulation\n        \n          Large Language Models (LLMs) have recently empowered agentic frameworks to exhibit advanced reasoning and planning capabilities. However, their integration in robotic control pipelines remains limited in two aspects: (1) prior \\ac{llm}-based approaches often lack modular, agentic execution mechanisms, limiting their ability to plan, reflect on outcomes, and revise actions in a closed-loop manner; and (2) existing benchmarks for manipulation tasks focus on low-level control and do not systematically evaluate multistep reasoning and linguistic variation. In this paper, we propose Agentic LLM for Robot Manipulation (ALRM), an LLM-driven agentic framework for robotic manipulation. ALRM integrates policy generation with agentic execution through a ReAct-style reasoning loop, supporting two complementary modes: Code-asPolicy (CaP) for direct executable control code generation, and Tool-as-Policy (TaP) for iterative planning and tool-based action execution. To enable systematic evaluation, we also introduce a novel simulation benchmark comprising 56 tasks across multiple environments, capturing linguistically diverse instructions. Experiments with ten LLMs demonstrate that ALRM provides a scalable, interpretable, and modular approach for bridging natural language reasoning with reliable robotic execution. Results reveal Claude-4.1-Opus as the top closed-source model and Falcon-H1-7B as the top open-source model under CaP.",
        "link": "https://arxiv.org/abs/2601.19510",
        "submittedDate": "2026-01-30T04:46:07.302Z"
      }
    ]
  },
  "total_count": 34
}