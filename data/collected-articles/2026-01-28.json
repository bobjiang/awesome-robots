{
  "fetch_date": "2026-01-28T23:09:48.920Z",
  "week_range": "Jan 22–28",
  "articles": {
    "ieee": [],
    "robotreport": [
      {
        "title": "Gartner predicts fewer than 20 companies will deploy humanoids at scale by 2028",
        "link": "https://www.therobotreport.com/gartner-predicts-fewer-than-20-companies-will-deploy-humanoids-at-scale-by-2028/",
        "pubDate": "2026-01-28T21:31:40.000Z",
        "content": "From left to right: Unitree's humanoid, Boston Dynamics' Atlas, Figure AI's 02, Apptronik's Apollo, and Tesla's Optimus robot. [https://www.therobotreport.com/wp-content/uploads/2026/01/humanoids-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/humanoids-featured.jpg From left to right: Unitree’s G1, Boston Dynamics’ Atlas, Figure AI’s Figure 02, Apptronik’s Apollo, and Tesla’s Optimus robot. | Source: Unitree, Boston Dynamics, Figure, Apptronik, and Tesla In recent years, humanoid robots have dominated headlines, and the companies developing them have raised hundreds of millions of dollars to get them working in the real world. Despite these efforts, actual humanoid deployments are few and far between, noted Gartner Inc. The market research firm [https://www.gartner.com/en] doesn’t expect this to change anytime soon. It said that through 2028, fewer than 100 companies will progress with humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] proofs of concept beyond experimentation, even though close to 200 exist today. Gartner also predicted that fewer than 20 companies will be going live in production for supply chain [https://www.therobotreport.com/category/markets-industries/logistics-warehousing-asrs/] and manufacturing [https://www.therobotreport.com/category/markets-industries/manufacturing/] use cases. Most deployments of humanoid robots during this time will remain limited to tightly controlled environments, rather than in dynamic and high-throughput operations, it said. “The promise of humanoid robots is compelling, but the reality is that the technology remains immature and far from meeting expectations for versatility and cost-effectiveness,” said Abdil Tunca, senior principal analyst in Gartner’s Supply Chain practice. “CSCOs [chief supply chain officers] must carefully evaluate readiness and avoid overcommitting resources to solutions that cannot yet deliver on their potential.” Robots designed to mimic the human body in shape, function, and locomotion are attracting attention from CSCOs seeking solutions to workforce challenges and rising labor costs. These humanoids feature AI [https://www.therobotreport.com/category/design-development/ai-cognition/]-enabled systems, advanced sensors, and machine learning algorithms intended to dynamically adapt to multiple tasks. BARRIERS PERSIST FOR HUMANOID PROGRESS Humanoid robots incorporate heads with sensors and cameras, arms and grippers for manipulation, and legs for locomotion. While this form factor offers certain advantages, Gartner noted that alternative designs—such as “polyfunctional” robots equipped with wheels or sensors in unconventional placements—may provide superior performance and adaptability for supply chain operations. Despite their potential, Stamford, Conn.-based Gartner said humanoid robots face significant barriers to supply chain, logistics, and manufacturing adoption: * Technological limitations: Current models lack the dexterity, intelligence, and adaptability required for complex, unstructured environments such as mixed SKU picking [https://www.automatedwarehouseonline.com/category/manipulating/], trailer unloading [https://www.automatedwarehouseonline.com/tag/loading/], or exception handling in high velocity warehouses [https://www.automatedwarehouseonline.com/]. * Integration complexity: Compatibility with existing systems and workflows remains a challenge, as is safety [https://www.therobotreport.com/tag/safety/]. * High costs: Substantial upfront investment and ongoing maintenance expenses must be weighed against uncertain returns. With the current technology, humanoids cost multiple times more than task-specific robots while delivering lower throughput and uptime. * Energy constraints: Limited battery [https://www.therobotreport.com/category/technologies/batteries-power-supplies/] life restricts operational time for high-mobility tasks. POLYFUNCTIONAL ROBOTS WILL WIN THE WAREHOUSE, SAYS GARTNER Polyfunctional robots from Diligent Robotics, Kinisi Robotics, Humanoid, and RoboForce. Gartner analyzed the market prospects for such systems. [https://www.therobotreport.com/wp-content/uploads/2026/01/mobile-manipulators.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/mobile-manipulators.jpg Polyfunctional robots, from left: Moxi, Kinisi 01, HMND 01, and Titan. | Source: Diligent Robotics, Kinisi Robotics, Humanoid, and RoboForce Unlike humanoid robots, polyfunctional robots are optimized for flexibility without being constrained by human-like design, Gartner observed. For example, a mobile manipulator [https://www.therobotreport.com/tag/mobile-manipulation] with wheels and a telescopic arm can move boxes, pick cases, scan inventory, and perform inspections, usually with higher uptime and using less energy than a humanoid that is attempting the same tasks. Such robots can integrate features that enhance efficiency and durability, making them better suited for dynamic supply chain environments, said the business and intelligence firm. “Companies with a high risk appetite and focus on innovation are the best candidates for pursuing humanoid robots at present, given the unproven capabilities of these solutions and related lack of clarity for return on investment,” said Caleb Thomson, senior director analyst in Gartner’s Supply Chain practice. “For the majority of companies that will need to prioritize robots that maximize throughput-per-dollar invested, we expect polyfunctional robots to be the superior solution.” To navigate robotics investment decisions effectively, Gartner advised CSCOs to: * Pursue pilot programs to validate feasibility before committing to full-scale deployment. * Collaborate with emerging providers to influence product development and align solutions with operational needs. * Implement continuous monitoring to track performance and guide iterative improvements. * Foster a culture of innovation that supports experimentation and calculated risk-taking. * Prioritize outcome-driven automation that targets specific bottlenecks, rather than generalized “headcount reduction” strategies, which is also less risky from an investment standpoint. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- The post Gartner predicts fewer than 20 companies will deploy humanoids at scale by 2028 [https://www.therobotreport.com/gartner-predicts-fewer-than-20-companies-will-deploy-humanoids-at-scale-by-2028/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Gartner says that relatively few humanoid robot developers will progress proofs of concept beyond experimentation.\nThe post Gartner predicts fewer than 20 companies will deploy humanoids at scale by 2"
      },
      {
        "title": "Introducing Sprout, a new humanoid development platform",
        "link": "https://www.therobotreport.com/introducing-sprout-a-new-humanoid-development-platform/",
        "pubDate": "2026-01-28T20:06:17.000Z",
        "content": "In the high-stakes artificial intelligence race, New York-based Fauna Robotics is betting that the path to general intelligence runs through the physical world. Led by founders Rob Cochran [https://www.linkedin.com/in/rpcochran/] and Josh Merel [https://www.linkedin.com/in/josh-merel-9222b72a2/], the startup is developing a specialized hardware and AI platform to move robots out of factories and into the unstructured environments of everyday life. By engineering safe, lightweight, and inexpensive machines, Fauna said it aims to create a data-driven “flywheel” for embodied AI [https://www.therobotreport.com/category/design-development/ai-cognition/]. The company [https://faunarobotics.com/] said it is pursuing a vision where general-purpose robots don’t just perform tasks, but also live, work, and play alongside their human counterparts. STARTUP FOCUSES ON BUILDING APPLICATIONS OVER TEACHING FUNDAMENTALS Cochran described the concept for Sprout to The Robot Report [https://www.therobotreport.com/]: “It gives people the tools to start building interesting applications, rather than focusing on the kind of fundamentals that make it quite a small community of roboticists that are able to actually engage with robotics today, and so that feels like a really great opportunity.” He added that Fauna was founded on the idea of solving problems immediately. “One of the things we identified when we started this company is … now is the time when we can maybe start to solve some general-purpose robotics problems and in a general-purpose form factor, like a humanoid [https://www.therobotreport.com/tag/humanoid/],” Cochran said. Sprout is engineered for expressive engagement within human-centric environments. These interactions are enhanced by an articulated neck that provides a controllable gaze, allowing Sprout to “look” at people and objects. Fluid movements of the arms and torso further humanize the robot, enabling it to perform social gestures like high-fives, handshakes, and playful poses that bridge the gap between machine and companion, according to Fauna Robotics. SPROUT DESIGNED FOR POSITIVE INTERACTIONS WITH PEOPLE Fauna Robotics said it balanced a friendly aesthetic with rigorous safety [https://www.therobotreport.com/tag/safety/] features in Sprout. The robot’s compact, lightweight frame has soft exterior panels and back-drivable motors to minimize impact forces and ensure safe interactions. Sprout's torso view [https://www.therobotreport.com/wp-content/uploads/2026/01/Sprout-Interactive.jpg] Standing at 107 cm and weighing just 22.7 kg, Sprout is designed to be inherently safe. | Credit: Fauna Robotics The company said it engineered Sprout’s physical form factor for approachability. With a maximum size of 107 cm (42 in.) and weighing 22.7 kg (50 lb.), the robot is intended to be unintimidating, particularly for children. In addition to its small size, Sprout works with a dedicated safety subsystem that monitors real-time conditions to enforce constraints across all mechanical and software levels, explained Fauna. Beyond its protective design, Sprout emphasizes emotional connection through a character-like appearance, which includes an articulated neck, actuated eyebrows, and an LED array that facilitates natural, non-verbal communication and social trust. The robot’s modular architecture has 29 degrees of freedom and is designed for both durability and ease of use. It has a 3.5-hour swappable battery and a resilient build capable of recovering from falls. Fauna asserted that this blend of approachable materials and personalized aesthetics supports its mission to build a versatile platform for education, therapy, and interactive research. FAUNA ROBOTICS BUILDS A PLATFORM FOR AI DEVELOPMENT Sprout also serves as a high-performance, scalable platform powered by the NVIDIA [http://www.therobotreport.com/tag/nvidia/] Jetson AGX Orin for advanced perception and decision-making. For developers, the platform offers a streamlined experience through stable APIs and containerized services that support everything from low-level motor control to high-level autonomy. By using consumer electronics manufacturing processes and a low part count, Fauna said its design ensures that Sprout can scale efficiently from research labs to mass-market production. Cochran compared Sprout to the early development of PCs. “The analogy I like to draw is to the early days of personal computing, [where] it wasn’t really until you had certain things like the Apple II and the invention of BASIC, the programming language, that you had this abstraction that allowed for the concept of an application developer,” he said. “And then you started to get interesting applications, and then the market got bigger. I think that same trajectory could make a lot of sense in the context of [physical AI-based] robotics as well.” ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- SPROUT IS MORE A CHARACTER RATHER THAN A MACHINE Core to Sprout’s design is a character-like presence over utilitarian machinery, drawing on popular culture to foster immediate emotional connections, said Fauna Robotics. Sprout appears more like an animated character come to life than an industrial machine. Central to this design approach is a “non-screen-based” facial design that emphasizes physical embodiment rather than digital displays. With its motorized eyebrows and LED array, the robot can convey a wide range of emotions and intentions, such as “thinking” animations or safety warnings, making it more relatable and engaging for human interaction, Fauna said. headshot of sprout. [https://www.therobotreport.com/wp-content/uploads/2026/01/Sprout-head.jpg] Sprout features a “non-screen-based” facial design that emphasizes physical embodiment rather than digital displays. | Credit: Fauna Robotics These physical cues are governed by a slot-based behavior hierarchy, ensuring that visual and mechanical elements work in harmony to produce coherent, lifelike behaviors. “We’ve provided a software and developer experience packaged around this idea that you can do very low-level programming on this,” Cochran noted. “But you also get mapping and navigation and voice interaction, and HRI [https://www.therobotreport.com/category/design-development/haptics/] features right out of the box.” To facilitate natural social [https://www.therobotreport.com/tag/social-robots] engagement, Sprout incorporates a sophisticated audio-visual and movement suite. A four-microphone array enables speech recognition and sound-source localization, while integrated speakers allow the robot to respond with speech, sound effects, and tones that complement its physical gestures. four views of sprout, showing the sensors and features of the robot platform. [https://www.therobotreport.com/wp-content/uploads/2026/01/sprout-features.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/sprout-features.jpg Hardware overview. Key features of the Sprout robot platform from different perspectives: (A) and (B) are true-color renders (C) and (D) are semi-transparent renders. | Credit: Fauna Robotics Full specifications for Sprout are available on the Fauna Robotics product page [https://faunarobotics.com/product]. The robot’s soft, deformable exterior panels encourage physical contact, while the inclusion of standardized interlocking toy brick components allows for cosmetic customization (see image",
        "excerpt": "Fauna Robotics unveils Sprout: a safe, character-driven robot designed to live, work, and play alongside humans in everyday life.\nThe post Introducing Sprout, a new humanoid development platform appea"
      },
      {
        "title": "Waabi raises $1B to advance autonomous trucks and robotaxis",
        "link": "https://www.therobotreport.com/waabi-raises-1b-to-advance-autonomous-trucks-and-robotaxis/",
        "pubDate": "2026-01-28T16:30:03.000Z",
        "content": "A Uber Freight truck with Waabi technology. Waabi will apply its Physical AI Platform developed for trucks to robotaxis in an expanded partnership with Uber. [https://www.therobotreport.com/wp-content/uploads/2026/01/Waabi_SeriesC.jpg] Waabi will apply its Physical AI Platform developed for trucks to robotaxis in an expanded partnership with Uber. Source: Waabi More AI-enabled vehicles are coming to the roads, as Waabi today said it has closed an oversubscribed $750 million Series C round to accelerate its commercialization of autonomous trucks. The company has also secured additional investment from Uber Technologies Inc. and said the strategic partnership will support its expansion into robotaxis. “Waabi’s Physical AI Platform has enabled us to hit an industry-leading pace in the development and commercialization of autonomous trucks over the past few years,” stated Raquel Urtasun, founder and CEO of Waabi. “Our current self-driving capabilities across highways and generalized surface streets have unlocked a new direct-to-customer model that for the first time solves the pain points of the industry and provides an unprecedented opportunity to quickly and seamlessly enter the robotaxi market, delivering a truly scalable solution for both verticals,” she added. Founded in 2021, Waabi said its Physical AI Platform generalizes to different form factors, geographies, and environments. The platform “combines a verifiable end-to-end AI model capable of reasoning alongside the world’s most advanced neural simulator [https://www.therobotreport.com/category/software-simulation/],” claimed the Toronto-based company [https://www.therobotreport.com/tag/waabi]. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- WAABI AND UBER TO BRING AI BRAIN TO BOTH TRUCKS AND TAXIS Waabi said its approach enables the same AI [https://www.therobotreport.com/category/design-development/ai-cognition/] model to act as a “shared brain” for both autonomous trucks [http://therobotreport.com/tag/autonomous-trucking] and robotaxis [https://www.therobotreport.com/tag/robotaxi]. The company [https://waabi.ai/] asserted that its expansion into self-driving passenger vehicles will ultimately improve the Waabi Driver’s overall capabilities. Under the partnership, Uber will invest additional milestone-based capital to support the development of robotaxis using the Waabi Driver. The companies said they plan to deploy 25,000 or more autonomous vehicles (AVs [https://www.therobotreport.com/category/robots-platforms/self-driving-vehicles/]). “We are thrilled to partner with the best-in-class ridesharing platform to bring about a safer, more efficient, and sustainable future,” said Urtasun. Waabi previously partnered [https://www.therobotreport.com/waabi-partners-with-uber-freight-to-deploy-autonomous-trucks-at-scale/] with Uber Freight in 2023, and Uber participated in its $200 million Series B round [https://www.therobotreport.com/waabi-raises-200m-uber-nvidia-on-the-road-self-driving-trucks/] in 2024. “Waabi’s expanded focus on robotaxis marks an important milestone for their team and the AV industry more broadly,” said Dara Khosrowshahi, CEO of Uber. “We’re very excited to deepen our partnership with Waabi as they significantly scale their Physical AI Platform and enter a new phase of an already remarkable journey.” While it sold [https://www.therobotreport.com/uber-sells-self-driving-unit-aurora-ending-tumultuous-era/] its Advanced Technologies Group (ATG) to Aurora Innovation [https://www.therobotreport.com/tag/aurora/] in 2020, Uber [https://www.therobotreport.com/tag/uber/] has also invested in other AV developers, including Nuro [https://www.therobotreport.com/nuro-closes-203m-propel-ai-first-self-driving-tech-commercial-partnerships/], Lucid [https://www.therobotreport.com/lucid-nuro-uber-team-up-on-global-robotaxi-fleet/], and Wayve [https://www.therobotreport.com/wayve-announces-strategic-partnership-and-investment-from-uber/], and it has partnered with robotaxi providers Motional [https://www.therobotreport.com/motional-partners-with-uber-for-10-year-commercial-agreement/], Waymo [https://www.therobotreport.com/phoenix-residents-can-soon-hail-waymo-robotaxis-with-uber/], and WeRide [https://www.therobotreport.com/uber-to-invest-100m-into-weride-to-bring-robotaxis-to-15-cities/]. AUTOMOTIVE LEADERS, CANADIAN INVESTORS JOIN WAABI ROUND Khosla Ventures and G2 Venture partners co-led Waabi’s Series C round, which the company said is the largest fundraise in Canadian history. “We invest in the companies that are leading the AI era,” said Vinod Khosla, founder of Khosla Ventures. “Waabi has developed a truly groundbreaking physical AI platform that represents a fundamental leap forward in how next-generation driverless technology is being developed.” Other strategic investors included NVentures (NVIDIA [https://www.therobotreport.com/tag/nvidia]’s venture capital arm), Volvo Group Venture Capital, and Porsche Automobil Holding SE. “Waabi is fundamentally changing the trajectory of autonomous transportation,” said Brook Porter, co-founder and partner at G2 Venture Partners. “Their simulation-first, end-to-end AI is a powerful enabler, accelerating commercial adoption while dramatically reducing capital needs to scale. Waabi is unlocking the potential for autonomy to drive vehicle efficiency and utilization, catalyzing the shift to a more sustainable transportation system.” In addition, funds and accounts managed by BlackRock, Radical Ventures, HarbourVest Partners, a wholly owned subsidiary of the Abu Dhabi Investment Authority (ADIA), Linse Capital, Incharge Capital, and others participated in the investment. Waabi noted that Canadian firms such as BDC Capital’s Thrive Venture Fund, Export Development Canada (EDC), TELUS Global Ventures, and BMO Global Asset Management were among its backers. Waabi’s Series C round joins other large robotaxi investments, including Waymo’s $5.6 billion [https://www.therobotreport.com/waymo-raises-5-6b-to-accelerate-self-driving-car-growth/] Series C in 2024, Cruise’s $2.75 billion [https://www.therobotreport.com/cruise-raises-2b-partners-microsoft-autonomous-vehicles/] round in 2021, and Aurora’s $820 million [https://ir.aurora.tech/news-events/press-releases/detail/79/aurora-announces-closing-of-820-million-upsized-public-offering-and-private-placement-of-class-a-common-stock] stock sale in 2024. In autonomous trucking, Einride raised $100 million [https://www.therobotreport.com/einride-raises-100m-to-scale-autonomous-freight-deployments/] in October 2025, while Plus Automation [https://www.therobotreport.com/autonomous-trucking-developer-plus-goes-public-via-spac/] and Kodiak Robotics [https://www.therobotreport.com/kodiak-robotics-autonomous-trucking-developer-goes-public-via-spac/] went public via special-purpose acquisition companies (SPACs) last year. The post Waabi raises $1B to advance autonomous trucks and robotaxis [https://www.therobotreport.com/waabi-raises-1b-to-advance-autonomous-trucks-and-robotaxis/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Waabi plans to apply its Physical AI Platform developed for autonomous trucks to robotaxis with Uber.\nThe post Waabi raises $1B to advance autonomous trucks and robotaxis appeared first on The Robot R"
      },
      {
        "title": "Robotiq brings sense of touch to physical AI with fingertips for 2F grippers",
        "link": "https://www.therobotreport.com/robotiq-brings-sense-touch-physical-ai-fingertips-2f-grippers/",
        "pubDate": "2026-01-27T15:44:03.000Z",
        "content": "Rending of Robotiq's TSF-85 tactile sensor fingertips on its 24-85 Adaptive Gripper. [https://www.therobotreport.com/wp-content/uploads/2026/01/Robotiq_gripper_sensor.jpg] Rending of the TSF-85 tactile sensor fingertips on the 2F-85 Adaptive Gripper. Source: Robotiq Robotic end effectors do not need to be humanoid to be more sensitive and effective, according to Robotiq Inc. The company today launched the TSF-85 tactile sensor fingertips for its 2F-85 Adaptive Gripper. It said that its integrated sensing enables robots to reliably feel, understand, and interact with the world at scale. “Physical AI demands more than clever algorithms—it demands reliable interaction with the real world,” stated Vincent Duchaine, chief technology officer for artificial intelligence at Robotiq. “By combining adaptive gripping with high-frequency tactile sensing, we’re giving robots the sense of touch and control they need to generalize across objects, tasks, and environments without the cost and complexity of anthropomorphic hands.” Founded in 2008, Robotiq said it built the TSF-85 on years of research and field experience, with more than 23,000 grippers [https://www.therobotreport.com/category/technologies/grippers-end-effectors/] deployed worldwide. The Lévis, Quebec-based company [https://www.therobotreport.com/tag/robotiq] said leading AI labs and manufacturers use its products to “bridge the gap between digital intelligence and physical reality.” ROBOTIQ COMMERCIALIZES UNIVERSITY R&D “Tactile sensing has been in academia for a long time,” noted Jennifer Kwiatkowski [https://www.linkedin.com/in/jenniferkwiatkowski/?originalSubdomain=ca], an AI specialist at Robotiq. “Robotics companies are realizing the limits of what vision can do for planning, navigation, and manipulation in terms of their robot perception stacks.” Robotiq’s tactile sensor came out of research that Kwiatkowski worked on at the École de technologie supérieure (ETS [https://www.etsmtl.ca/]) in Montreal, she told The Robot Report [https://www.therobotreport.com/]. “My research was on grasp stability prediction using these sensors and AI,” Kwiatkowski said. “There have been other experiments around object and texture recognition, as well as detecting if there’s a crack in a piece. If you’re squeezing different objects, can you recognize them?” “Robotiq recognized that we had something, having been involved with end-of-arm tooling [EOAT] for 17 years,” she added. “The gripper is the way that the robot interacts with the world, so all of your perception at the end of the arm is what makes your robot useful.” ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- 2F-85 GRIPPER DESIGNED FOR SENSITIVITY, SIMPLICITY While parallel grippers rely on precise positioning and rigid alignment, Robotiq said its 2F-85 EOAT offers both pinch and encompassing grips, with stroke lengths of 85 and 140 mm (3.3 and 5.5 in.). It said this allows the gripper to conform to an object’s shape, reduces grasp planning complexity, and minimizes reliance on unobstructed vision, making it suitable for handling a wide range of objects. Robotiq listed the following capabilities: * A 4×7 static taxel grid to monitor force distribution * Micro-slip detection at 1000 Hz for stable, precise manipulation * An integrated IMU (inertial measurement unit) for proprioceptive sensing and contact awareness “Unlike other sensing mechanisms — optical or magnetic — we’re capacitor-based, like a smartphone touchscreen, which is well-established,” explained Kwiatkowski. “We focus on having good integration of the sensor into the robotic hand, which has to be reliable in the warehouse.” The tactile-enabled 2F grippers integrate into existing robots using native RS-485 communication and a USB conversion board, said Robotiq. The tactile fingertips are designed to preserve grip mechanics with minimal impact on stroke and reach, and they feature robust cabling. ROBOTIQ TOUTS DURABILITY, SCALABILITY OF EOAT Robotiq said that thousands of its grippers are already operating in demanding research and industrial environments worldwide. It claimed that its grippers have a lower bill of materials (BOM) and replacement cost than anthropomorphic or DIY hands, reducing cost and providing “a practical path from lab prototype to large-scale robot fleets.” “We’re in the process of doing tests, but the threshold we’re looking to match is our grippers’ lifecycles, which is on the order of millions of cycles,” Kwiatkowski said. “Another interesting element of our design is scalability. These sensors are not too complex to manufacture, which is important to us if we’re getting orders for hundreds of thousands of grippers.” “Articulated hands are harder to control and have more failure modes,” she said. “Humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] hands are very cool, but they’re more of a 10-year play, while we’ll be ready to ship units in the coming months and iterate toward the best sensor that fits the most use cases.” Robotiq offers new tactile sensor fingertips for robot grippers. [https://www.therobotreport.com/wp-content/uploads/2026/01/Robotiq_tactile_sensors.jpg] The new tactile sensor fingertips are designed to be manufactured at scale. Source: Robotiq TSF-85 SENSORS TO PROVIDE DATA FOR PHYSICAL AI Robotiq said it shares best practices for tactile data handling, including guidance on bias management, normalization, and outlier detection, to help teams generate consistent, high-quality data for model training. It asserted that its sensors enable robots to understand contact geometry, detect incipient slip, and improve generalization across diverse objects, all of which are necessary for physical AI [https://www.therobotreport.com/category/design-development/ai-cognition/] datasets. “It’s important to make sure that you’re aligning your data collection to the appropriate tasks so that you’re not wasting time,” said Kwiatkowski. She cited three mechanisms of the human sense of touch [https://www.therobotreport.com/category/design-development/haptics/] — sustained or fast changes in pressure, vibration, and configuration of the hands. These shape perception of an object’s shape as well as grasping and manipulation capabilities. The lack of reliable data across sensor [https://www.therobotreport.com/category/technologies/sensors-sensing/] modalities has hindered the development and production of physical AI, according to Robotiq. The company said that its standardized hardware and tactile data provide a base for reinforcement learning, vision-language-action (VLA) models, and imitation learning. No matter how good simulations [https://www.therobotreport.com/category/software-simulation/] may be, developers still need real-world data as they build toward generalized systems, Kwiatkowski said. “At the Humanoids Summit, I heard how physical AI is growing into operational AI, but it needs 99.9% reliability,” she recalled. “To build physical AI that truly works, you need hardware that can sense, respond, and learn from every interaction,” said Aleksei Filippov, head of business development at Yango Tech Robotics [https://robotics.yango.com/]. “That’s why we chose Robotiq. With Robotiq precision force control and reliable feedback,",
        "excerpt": "Robotiq says it has combined adaptive gripping with high-frequency tactile sensing, enabling robots to generalize across objects.\nThe post Robotiq brings sense of touch to physical AI with fingertips "
      },
      {
        "title": "Big robot on campus: Starship finds 97% student approval rating",
        "link": "https://www.therobotreport.com/big-robot-campus-starship-finds-97-student-approval-rating/",
        "pubDate": "2026-01-27T14:01:02.000Z",
        "content": "Students on a college campus smiling with a delivery robot from Starship Technologies. [https://www.therobotreport.com/wp-content/uploads/2026/01/Students-smiling-copy.jpg] Students have rapidly accepted delivery robots on campus. Source: Jessica Foster, Starship Technologies While it may not be surprising that Generation Z is open to new technology, a survey of 5,000 students across 65 U.S. college campuses found that 95% “like” or “love” delivery robots from Starship Technologies Inc. The company called this a “generational shift in how humans coexist with autonomous helpers.” “Campuses have long been the birthplace of the world’s most transformative ideas, and today they’re once again leading the way,” said Ahti Heinla, co-founder and CEO of Starship. “What began as a convenient delivery option has grown into a new social standard” “This generation is proving that autonomous technology can coexist in our human communities, redefining the future of urban cities,” he stated. “We’re showing the world that it’s fully automated and frictionless.” Founded in 2014 by Heinla, former chief architect of Skype,and Janus Friis, co-founder of Skype, Starship claimed to be the world’s No. 1 autonomous delivery company [https://www.therobotreport.com/tag/starship-technologies/]. It said it has completed more than [https://www.therobotreport.com/starship-technologies-surpasses-8m-autonomous-deliveries/] 9 million deliveries [https://www.therobotreport.com/tag/delivery-robots/] and operates over 2,700 robots across 270+ locations in seven countries. Starship raised [https://www.therobotreport.com/starship-technologies-obtains-series-c-funding-for-autonomous-deliveries/] $50 million in funding in October 2025, bringing its total funding to more than $280 million. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- FAMILIARITY WITH ROBOTS BUILDS CAMPUS ACCEPTANCE Starship said the results from its 2025 Campus User Survey showed robots receiving one of the highest approval ratings for any AI-powered technology. In addition, 33% of students said they experienced the technology for the first time, which the company said suggested an “early adopter” mindset. A Starship robot at the Northern Arizona University campus. [https://www.therobotreport.com/wp-content/uploads/2026/01/NAU-robot-copy-238x300.jpg] A delivery robot at the Northern Arizona University campus. Source: Jessica Foster, Starship “The robots have become an essential part of everyday life in the U.S., completing almost 7 million orders and traveling nearly 8 million miles across U.S. campuses since 2019,” the company said. “In 2025, they completed 1.5 million miles on U.S. campuses — nearly six trips to the moon — reliably delivering in rain, snow, and freezing temperatures.” Almost three-quarters (72%) of the survey respondents described Starship’s robots as “friendly/cute,” and two thirds (65%) reported that their opinions became “more positive since seeing or using them.” Students often name, help, and treat the robots like mascots, said the company [https://www.starship.xyz/]. It asserted that the survey results show that autonomous technology is as much about culture as it is about technology. Oregon State has one of the largest Starship deployments, with 265,000 orders in 2025 and 1.2 million orders total since launch. STARSHIP TOUTS BENEFITS TO STUDENT HEALTH, STUDIES Beyond practicality, the sidewalk robots have delivered unexpected health benefits on college campuses, noted Starship. They can help students eat more regularly, avoid unsafe late-night walks, manage stress and illness, and access meals despite mobility or social-anxiety challenges. Nearly 40% of students surveyed said delivery robots improve food accessibility, while 1 in 4 reported feeling safer using contactless delivery, particularly during late-night study sessions or poor weather. Students also cited reduced stress around meal access when sick, injured, or overwhelmed during exam periods. Similar to last year’s survey results, over half of those surveyed said they can study more effectively (54%), avoid skipping meals (60%), and save time (51%). “Starship delivery robots have been extremely helpful on campus,” said Amelia Ott, a junior at Purdue University. “Whether I’m studying late, or rushing between classes, they’re an easy, reliable way to get meals. The robots are always on time and can navigate through any terrain or bad weather. They’ve become part of everyday campus life and are fun to see around campus too.” Starship operates on 65 U.S. campuses and plans to expand further in 2026. The San Francisco-based company [https://www.automatedwarehouseonline.com/tag/starship-technologies/] said its delivery robots are becoming a common sight on campuses and cities around the world rather than a novelty. The post Big robot on campus: Starship finds 97% student approval rating [https://www.therobotreport.com/big-robot-campus-starship-finds-97-student-approval-rating/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "As delivery robots become more common on campus, they are finding wide acceptance, found a Starship survey.\nThe post Big robot on campus: Starship finds 97% student approval rating appeared first on T"
      },
      {
        "title": "Multiply Labs partners with AstraZeneca to automate cell therapy manufacturing",
        "link": "https://www.therobotreport.com/multiply-labs-partners-astrazeneca-automate-cell-therapy-manufacturing/",
        "pubDate": "2026-01-27T14:00:53.000Z",
        "content": "Multiply Labs offers a modular approach to Cell Therapy automation that it says reduces manufacturing bottlenecks. [https://www.therobotreport.com/wp-content/uploads/2026/01/multiplylabs-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/multiplylabs-featured.jpg Multiply Labs said its modular approach to cell therapy automation reduces manufacturing bottlenecks. | Source: Multiply Labs Multiply Labs Inc. today announced an agreement with AstraZeneca to evaluate the potential for applying good manufacturing practice, or GMP-ready, robotic systems to commercial-scale cell therapy manufacturing. The collaboration will focus on automation of industry-standard instruments used in cell therapy production using Multiply Labs’ robotic biomanufacturing system. The goal is to enable scalable, high-throughput manufacturing [https://www.therobotreport.com/category/markets-industries/manufacturing/] while maintaining the rigorous quality and regulatory standards required for clinical and commercial use. “Cell therapies are among the most promising, yet complex medicines being developed today,” said Fred Parietti, Ph.D., CEO of Multiply Labs. “Our mission is to make these therapies more widely available by increasing manufacturing efficiency and scale.” “This agreement with AstraZeneca allows us to evaluate our multi-arm robotic clusters in a setting where we can combine some of the world’s best scientific and clinical expertise with our robotic platform to build the next generation of high-throughput, GMP-ready cell therapy manufacturing,” he added. FOUR-ARMED ROBOT MAXIMIZES PHARMACEUTICAL PRODUCTION Founded in 2016, Multiply Labs provides automation to the pharmaceutical [https://www.therobotreport.com/tag/pharmaceutical/] industry. It develops cloud-controlled robotics for the production of advanced therapies at scale. The San Francisco-based company [https://multiplylabs.com/]‘s newest systems [https://multiplylabs.com/products/cell-therapy/] use four robotic arms operating in parallel to run a broad range of cell therapy manufacturing instruments already in use by the industry. It said this architecture minimizes the need for process modifications while maximizing output in existing facilities, targeting higher productivity. Multiply Labs claimed that its customers include some of the largest pharmaceutical manufacturers in the world. The company asserted that its expertise is at the intersection of robotics and biopharma – its team includes mechanical engineers, electrical engineers, computer scientists, software engineers, and pharmaceutical scientists. MULTIPLY LABS BUILDS PARTNER NETWORK This isn’t the first partnership Multiply Labs has made in its efforts to automate cell therapy manufacturing. In September, it partnered [https://multiplylabs.com/2025/09/30/thermo-dynacellect-announcement/] with Thermo Fisher Scientific. The collaboration is focusing on integrating robots with Thermo Fisher’s automated Gibco CTS DynaCellect Magnetic Separation System. The Gibco CTS DynaCellect System is a closed, automated isolation, activation, and bead-removal system for cell therapy development and manufacturing. Together with fit-for-purpose consumables, it offers high cell purity, recovery, and viability. By combining the Gibco CTS DynaCellect System with Multiply Labs’ robotic automation expertise, the partners said they hope to accelerate critical steps in cell therapy production by reducing manual intervention and improving process consistency. In June, Multiply Labs announced [https://multiplylabs.com/2025/06/12/kyverna-announcement/] a pilot collaboration with Kyverna Therapeutics, a clinical-stage biopharmaceutical company focused on developing cell therapies for patients with autoimmune diseases. Kyverna planned to evaluate the automation of its cell therapy manufacturing processes using Multiply Labs’ robotic systems for KYV-102. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- The post Multiply Labs partners with AstraZeneca to automate cell therapy manufacturing [https://www.therobotreport.com/multiply-labs-partners-astrazeneca-automate-cell-therapy-manufacturing/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Multiply Labs' goal is to enable scalable, high-throughput manufacturing while maintaining rigorous quality and regulatory standards.\nThe post Multiply Labs partners with AstraZeneca to automate cell "
      },
      {
        "title": "Vention raises $110M to accelerate physical AI deployments in manufacturing",
        "link": "https://www.therobotreport.com/vention-raises-110m-to-accelerate-physical-ai-deployments-in-manufacturing/",
        "pubDate": "2026-01-27T12:01:41.000Z",
        "content": "Vention is developing physical AI to unify and accelerate deployment of robotic workcells such as this one. [https://www.therobotreport.com/wp-content/uploads/2026/01/Vention_motion-plan.jpg] Vention is developing physical AI to unify and accelerate deployment of robotic workcells. Source: Vention Physical AI, the union of artificial intelligence and robotics, is moving toward wider use. Vention Inc. today said it has raised $110 million ($150 million CAD) in Series D funding to advance its research, add capabilities to its software, expand its portfolio of pre-engineered applications, and grow its presence from North America to Europe. Traditional automation is complex and requires a lot of integration work, according to the Montreal-based company [https://www.therobotreport.com/tag/vention/]. Vention said its Zero-Shot Automation platform combines hardware, software, physical AI, and cloud connectivity to enable faster and more scalable automation of production in support of reshoring [https://www.therobotreport.com/tag/reshoring]. “Manufacturers no longer want automation that requires deep expertise and long commissioning cycles,” stated Etienne Lacroix, CEO of Vention. “They want automation that works as intuitively and reliably as modern software. Physical AI is allowing us to deliver exactly that.” VENTION PLAYBOOK INCLUDES FOUR TARGETS “We’re making automation more accessible by moving integration activities to our online platform,” Lacroix told The Robot Report [https://www.therobotreport.com/]. “We’re going to use the the funds to power four playbooks. The first is serving enterprise clients.” “Vention has become a standard within enterprises — 40% of our revenue comes from companies with $1 billion or more in revenue,” he said. “In the past, automation was decentralized, with separate integrators. Enterprises ended up with orphan machines with spaghetti code.” Many manufacturers [https://www.therobotreport.com/category/markets-industries/manufacturing/] want turnkey offerings, in which everything on the factory floor is governed by standardized platforms while retaining access to data models, digital twins, and code, explained Lacroix. Vention’s second playbook is to continue commercializing physical AI [https://www.therobotreport.com/category/design-development/ai-cognition/] from its roots in research and development. “We’ve always been an innovation powerhouse — that’s why NVIDIA joined as an investor; it saw how fast we’re moving in real industrial use cases,” said Lacroix. NVIDIA [https://www.therobotreport.com/tag/nvidia/] last year partnered [https://www.therobotreport.com/vention-nvidia-partner-bringing-automation-small-manufacturers/] with Vention on motion control [https://www.therobotreport.com/category/robot-components/motioncontrol/] and vision AI, and the partners plan to announce new products soon, he added. Vention has launched automated configuration tools, an AI agent for defining machine specifications, a robotic programming co-pilot, and AI-powered robotic applications. It claimed that these tools can reduce automation project timelines from months to days. The third playbook is to build depth in Vention’s “end-to-end” offering. “We’re the broadest industrial tech stack [https://www.therobotreport.com/vention-announces-full-stack-ai-automation-platform-expansions/],” Lacroix asserted. “We do design, programming, simulation [https://www.therobotreport.com/category/software-simulation/], ordering, deployments from the cloud and digital twin to the floor, plus the operating layer and teleoperation [https://www.therobotreport.com/tag/teleoperation/]. We’re not the deepest in all those verticals, but we’re continuing to build that depth.” The fourth playbook is Vention’s expansion in Europe [https://www.therobotreport.com/tag/europe]. The company has about 310 employees and has 30 open positions. In 2025, it added senior project managers and U.S.-based salespeople and customer success team members. Vention also invested into physical AI researchers last year. MANUFACTURERS DEMAND RELIABILITY, SPEED Vention serves 25 discrete manufacturing industries, said Lacroix. Promising segments this year include defense [https://www.therobotreport.com/category/markets-industries/defense-security/], data center and AI, and end-of-line automation [https://www.therobotreport.com/category/robots-platforms/industrial-robots/]. “We have people in the U.S. certified to serve heavily regulated defense clients,” he noted. “Defense is trying to move fast in Canada and Europe, and a whole class of startups is emerging. There are already a few winners in drones and submarines.” “We don’t manufacture hyperscalers, but it turns out that a lot of their cooling systems are now assembled in Vention-automated assembly lines,” added Lacroix. “We’re putting a lot of emphasis this year on physical AI, because we can figure out a way to take the technology in its current state and create a 99% reliable application at the speed of a human.” “If a manufacturer is operating two shifts, it needs to figure out a way to deploy physical AI for roughly $150,000,” Lacroix said. “Physical AI applied to manufacturing can grasp and position physical objects very precisely. We have good traction in consumer packaged goods and with Fortune 500 companies. They have mostly brownfield facilities but want modular automation that can be deployed quickly.” He said Vention’s tech stack is robot-agnostic, providing robot compatibility without the need to tweak robot drivers. The company [https://vention.io/] started 10 years ago with the Robot Operating System (ROS [https://www.therobotreport.com/tag/robot-operating-system]), which was well-built but slow, recalled Lacroix. “Every half-second counts,” he said. “We built a more efficient robot control layer to be 1:1 with robot planners. They can’t live in controllers, they have to be in the cloud. The systems on the factory floor need to run at the same speed as native planners.” ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- VENTION STAYS FOCUSED ON GENERALIZING ITS PLATFORM New and existing investors in Vention included Investissement Québec, Desjardins Capital, certain funds managed by Fidelity Investments Canada ULC, and NVentures (NVIDIA’s venture capital arm). “We’re quite excited to reach this milestone and have raised $260 million in total capital,” said Lacroix. While humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] robots and generalized AI have garnered a lot of attention lately, Vention is staying focused on robot arms and autonomous mobile robots (AMRs [https://www.therobotreport.com/category/robots-platforms/amrs/]). It has deployed more than 25,000 machines worldwide in over 4,000 factories. “We have discussions all the time about humanoids specific to manufacturing,” Lacroix acknowledged. “We looked at generalized platforms for fit-for-purpose workcells and said, ‘Let’s not generalize the robot, but generalize the platform to lead fit-for-purpose robots.'” “Vention’s AI pipeline uses robust models for real-time collision avoidance and pose estimation that we couldn’t have dreamed of just a year ago,” he said. “We’ll have a stream of announcements in the coming months, and we’ll be at GTC [https://www.therobotreport.com/tag/gtc/], Automate [https://www.therobotreport.com/tag/automate/], and IMTS, as well as Pack Expo and Hannover Messe.” The post Vention raises $110M to accelerate physical AI deployments in manufacturing [https://www.therobotreport.com/vention-raises-110m-to-accelerate-physical-ai-deployments-in-manufacturing/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Vention has raised Series D funding to continue commercializing its robot control platform and expand into Europe.\nThe post Vention raises $110M to accelerate physical AI deployments in manufacturing "
      },
      {
        "title": "AAA20 Group debuts cobot palletizer for food and protein processing",
        "link": "https://www.therobotreport.com/aaa20-group-debuts-cobot-palletizer-food-protein-processing/",
        "pubDate": "2026-01-26T21:24:23.000Z",
        "content": "AAA20 Group's new CP-66-WD wash-down collaborative palletizer. [https://www.therobotreport.com/wp-content/uploads/2026/01/AAA20-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/AAA20-featured.jpg AAA20 Group’s new CP-66-WD wash-down collaborative palletizer. | Source: AAA20 Group AAA20 Group plans to debut its new CP-66-WD wash-down collaborative palletizer at the International Production & Processing Expo, or IPPE, in Atlanta this week. The robot is a sanitary automation system for food manufacturers, particularly beef processors. “Space constraints and stringent sanitation requirements have made automation difficult to deploy in the food industry, particularly in finding solutions to address protein processing labor shortages,” said Marcus Kurle, co-founder of AAA20 Group. “The CP-66-WD was engineered to remove those barriers, giving processors a compact, compliant way to automate one of the most demanding jobs while preserving capital budgets and delivering high hygienic standards,” he noted. Founded in 2020, AAA20 Group specializes in smart robotic palletizing [https://www.therobotreport.com/tag/palletizing/] systems. The Las Vegas-based company [https://aaa20group.com/] uses a robot-as-a-service (RaaS [https://www.automatedwarehouseonline.com/category/raas/]) model, which it says removes traditional barriers to automation and enables rapid deployment. Its systems are in use across the food [https://www.therobotreport.com/tag/food/], beverage, and industrial sectors. COLLABORATIVE PALLETIZER DESIGNED FOR FAST DEPLOYMENT As seen during the COVID-19 pandemic, protein processors have been among the most affected by workforce constraints, which increase production costs and limit throughput. By palletizing with wash-down-ready force- and power-limited robots, producers can improve efficiency, reduce reliance on hard-to-fill labor roles, and help stabilize costs throughout the food supply chain, AAA20 Group said. Traditional automation systems require safety cages, extensive construction, and large capital expenditures. Unlike those, AAA20’s collaborative palletizers are designed for fast deployment and offered through a month-to-month leasing model. This approach allows food [https://www.therobotreport.com/category/markets-industries/food/] and beef processors to automate at a lower cost than manual labor while retaining the flexibility to scale as production needs change. Engineered for food and protein processing environments where cleanliness is critical, the CP-66-WD is a waterproof, IP69K-rated collaborative robot [https://www.therobotreport.com/category/robots-platforms/collaborative-robot/] that can be power-washed like other production equipment. The system enables manufacturers to automate palletizing while meeting sanitation regulations across beef, poultry, and other food operations. The robot is priced at approximately $5,000 per month. AAA20 PRIORITIZES LOW CAPITAL INVESTMENT AAA20 Group said the launch follows recent industry momentum for, including a recent visit to the company’s Las Vegas robotics facility by members of the U.S. Small Business Administration (SBA). The visit, which brought SBA Administrator Kelly Loeffler and representatives from Washington, D.C., underscored growing federal interest in automation that strengthens the U.S. food system and supports domestic protein production. At IPPE, AAA20 Group plans to show the CP-66-WD for facilities that require sanitary automation, minimal footprint integration, and rapid deployment within existing production lines. The company invited food manufacturers and beef processors attending IPPE to visit Booth A2745 to see the CP-66-WD in action and learn how wash-down robotics can help address labor challenges, maintain compliance, and control costs. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- The post AAA20 Group debuts cobot palletizer for food and protein processing [https://www.therobotreport.com/aaa20-group-debuts-cobot-palletizer-food-protein-processing/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "AAA20 Group's CP-66-WD is a waterproof, IP69K-rated collaborative robot that can be power-washed like other production equipment.\nThe post AAA20 Group debuts cobot palletizer for food and protein proc"
      },
      {
        "title": "State of robotics industry report 2026",
        "link": "https://www.therobotreport.com/state-of-robotics-industry-report-2026/",
        "pubDate": "2026-01-26T18:15:11.000Z",
        "content": "[https://www.therobotreport.com/wp-content/uploads/2026/01/ChatGPT-Image-Jan-26-2026-01_06_56-PM-e1769451153613.png] Imaged created using OpenAI’s ChatGPT. The robotics industry enters 2026 at an inflection point. After years of innovation, bold claims, and headline-grabbing demonstrations, the conversation is shifting from what robots could do to what they can reliably do in the real world. The Robot Report‘s State of Robotics Industry Report 2026 examines this transition in detail, offering a clear-eyed assessment of where the market stands today and where it’s headed. This coverage draws on reporting, interviews, and analysis from across the global robotics ecosystem, spanning industrial automation, mobile robots, humanoids, autonomous vehicles, investments, and emerging technologies and applications. It explores how advances in AI, perception, simulation, compute, and edge software are reshaping robot capabilities. It also highlights the challenges facing developers, integrators, and end users, including safety and reliability to cost, regulation, and deployment timelines. Readers will find insights and expert perspectives on the technologies shaping 2026, including foundation models, vision-language-action systems, simulation-first development, and the evolving role of humanoids. The report also examines how geopolitical pressures, supply-chain realities, labor shortages, and shifting investor expectations are influencing product roadmaps and commercialization strategies. The report offers informed predictions on where adoption will accelerate, where hype is likely to cool, and which segments are poised for meaningful progress over the next 12 to 24 months. Whether you’re building robots, deploying them, or investing in the companies behind them, the State of Robotics Industry Report 2026 provides essential context for understanding the state of robotics in 2026. Fill out the form below to download the report for free. The post State of robotics industry report 2026 [https://www.therobotreport.com/state-of-robotics-industry-report-2026/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "This report draws on reporting, interviews, and analysis from across the global robotics ecosystem, spanning industrial automation, mobile robots, humanoids, autonomous vehicles, investments, and emer"
      },
      {
        "title": "Hadrian raises funding for automated manufacturing, bringing valuation to $1.6B",
        "link": "https://www.therobotreport.com/hadrian-brings-in-additional-funding-bringing-its-valuation-to-1-6b/",
        "pubDate": "2026-01-25T13:30:38.000Z",
        "content": "A rendering of a planned Hadrian facility in Mesa, Ariz. [https://www.therobotreport.com/wp-content/uploads/2026/01/Hadrian-factory-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/Hadrian-factory-featured.jpg A rendering of one of Hadrian’s announced facilities in Mesa, Ariz. | Source: Hadrian Hadrian, which uses AI-powered automation and modern software to build manufacturing facilities for aerospace, defense, and emerging industrial programs, recently announced expanded capital. With the latest investment, the company is valued at $1.6 billion. Hadrian said it plans to use the funding to accelerate factory expansion and advance its automated manufacturing roadmap. “For decades, the United States separated design from production and assumed global supply chains would carry the load,” stated Chris Power, the founder and CEO of Hadrian. “That assumption no longer holds,” he said. “This capital accelerates our ability to build the industrial capacity America needs by pairing advanced automation with workforce training designed for the scale of the opportunity in front of us.” OPUS DESIGNED TO PROPEL RESHORING OF MANUFACTURING Demand for domestic manufacturing [https://www.therobotreport.com/category/markets-industries/manufacturing/] capacity across aerospace, defense, and critical infrastructure continues to grow, noted Hadrian. The company [https://www.hadrian.co/] said it is building advanced factories designed to produce mission-critical components, assemblies, and full product lines with speed, reliability, and scale. Hadrian said its platform pairs advanced automation with a rapidly trained workforce to meet the urgent need for a generational re-industrialization effort. This is part of its factories-as-a-service (FaaS) initiative. Opus, Hadrian’s proprietary software stack for production autonomy, powers its factories. It said its manufacturing facilities can go online in under six months. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- HADRIAN BUILDS ON FUNDING TO EXPAND DEFENSE PRODUCTION Hadrian said the latest capital will position it to move faster in scaling high-throughput American factories. The investment was led by accounts advised by T. Rowe Price Associates Inc. It included participation from Altimeter Capital, D1 Capital Partners, StepStone Group, 1789 Capital, Founders Fund, Lux Capital, a16z, Construct Capital, and existing investors. This funding builds on a $260 million round [https://www.prnewswire.com/news-releases/hadrian-raises-260m-to-build-ai-powered-factories-for-america-adds-full-product-manufacturing-opens-arizona-site-302508179.html] from July 2025. Hadrian used that investment to support nearly five football fields’ worth of new manufacturing space, expanded research and development capacity, and dedicated teams focused on shipbuilding and naval [https://www.therobotreport.com/category/maritime/] defense production. Hadrian said it plans to use the latest capital to: * Accelerate factory expansion * Scale workforce training programs * Continue investment in automation, AI [https://www.therobotreport.com/category/design-development/ai-cognition/]-driven tooling, and real-time manufacturing intelligence Last week, Hadrian launched [https://www.prnewswire.com/news-releases/hadrian-launches-additive-manufacturing-division-to-expand-us-defense-production-capacity-302668241.html] Hadrian Additive, a division dedicated to delivering scalable, production-ready additive manufacturing capacity for the U.S. defense [https://www.therobotreport.com/category/markets-industries/defense-security/] industrial base and allied partners. The new division expands Hadrian’s Opus platform to include 3D printing systems built for qualification, repeatability, and sustained throughput. It is intended to enable defense programs to move from validated designs into reliable, at-scale production. Initial additive manufacturing capacity is expected to come online in 2026 as part of Hadrian’s expanding U.S. factory footprint. The post Hadrian raises funding for automated manufacturing, bringing valuation to $1.6B [https://www.therobotreport.com/hadrian-brings-in-additional-funding-bringing-its-valuation-to-1-6b/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Hadrian plans to use the funding to accelerate factory expansion and advance the company's manufacturing roadmap.\nThe post Hadrian raises funding for automated manufacturing, bringing valuation to $1."
      },
      {
        "title": "1X launches world model enabling NEO robot to learn tasks by watching videos",
        "link": "https://www.therobotreport.com/1x-launches-world-model-enabling-neo-robot-to-learn-tasks-by-watching-videos/",
        "pubDate": "2026-01-24T13:30:47.000Z",
        "content": "A NEO robot from 1X dusting shelves. [https://www.therobotreport.com/wp-content/uploads/2026/01/1X_NEO-Home-Duster-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/1X_NEO-Home-Duster-featured.jpg 1X said its new world model puts it a step closer to a future where robots can teach themselves to do any task a human can do. | Source: 1X Technologies 1X Technologies AS last week announced its latest 1X World Model. The company said the AI update for NEO enables the humanoid robot to turn any request into an AI capability on demand, using a video model grounded in real-world physics. “After years of developing our World Model and making NEO’s [https://www.1x.tech/neo] design as close to human as possible, NEO can now learn from internet-scale video and apply that knowledge directly to the physical world,” said Bernt Børnich, founder and CEO of 1X. “With the ability to transform any prompt into new actions—even without prior examples—this marks the starting point of NEO’s ability to teach itself to master nearly anything you could think to ask.” With this update, 1X said NEO can use video data fine-tuned on robot data to perform AI tasks, even with objects and environments it has never encountered before. The Palo Alto, Calif.-based company [https://www.therobotreport.com/tag/1x-technologies/] said it has designed NEO for household [https://www.therobotreport.com/tag/household/] use. The humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] is available through 1X’s early access program for $20,000, which includes priority delivery in 2026. A subscription model will also be available for $499/month. NEO CAN DO OLD TASKS IN UNFAMILIAR PLACES, PLUS LEARN NEW ONES With this update, users can give NEO a simple voice or text prompt, and the robot uses what it’s looking at to generate visualizations of future actions. A built-in inverse dynamics model then translates these into precise movements for NEO to complete the request, 1X explained. “With the 1X World Model, you can turn any prompt into a fully autonomous robot action — even with tasks and objects NEO’s never seen before,” said Daniel Ho, an AI researcher at 1X. Demonstrations in 1X’s latest video (below) demonstrate NEO’s ability to generalize beyond training data. For simple prompts like packing a lunch box, it visualizes and executes, even with unfamiliar objects. The robot can also handle completely novel tasks, such as operating a toilet seat, opening a sliding door, ironing a shirt, brushing a human’s hair, and more, without any prior examples in its dataset, said 1X. This highlights the transfer of broad human knowledge through the World Model, 1X said. 1X SAYS ROBOT CAN LEARN ON ITS OWN Where traditional AI [https://www.therobotreport.com/category/design-development/ai-cognition/] models for humanoid robots have relied on data collected by human operators, the 1X World Model enables NEO to collect its own data and autonomously master new capabilities. This opens the door for robots to eventually teach themselves anything, 1X claimed. Where improvement in AI capabilities for humanoids has been bottlenecked by the speed in which robot data can be collected by human operators, 1X said its World Model doesn’t only self improve from NEO collecting it’s own data. It also benefits from the improvement of video models, given that the world model uses a video model at its core. Traditional models have historically struggled with changes in lighting, clutter, or chaos that are commonplace in the home. The 1X World Model applies human-like understanding to navigate extreme variability, maintaining composure amid rapid environmental shifts, according to the company. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- The post 1X launches world model enabling NEO robot to learn tasks by watching videos [https://www.therobotreport.com/1x-launches-world-model-enabling-neo-robot-to-learn-tasks-by-watching-videos/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "With this update, 1X Technologies' NEO uses internet-scale video data tuned on robot data to perform AI tasks.\nThe post 1X launches world model enabling NEO robot to learn tasks by watching videos app"
      },
      {
        "title": "Thomas Pilz on innovation and safety in robotics",
        "link": "https://www.therobotreport.com/thomas-pilz-on-innovation-and-safety-in-robotics/",
        "pubDate": "2026-01-23T20:50:56.000Z",
        "content": "The Robot Report Podcast [https://soundcloud.com/robot-report-podcast] · Thomas Pilz on Innovation and Safety in Robotics [https://soundcloud.com/robot-report-podcast/thomas-pilz-on-innovation-and-safety-in-robotics] In Episode 228 of The Robot Report Podcast, hosts Steve Crowe and Mike Oitzman recap the major robotics news of the week. headshot of thomas pilz. [https://www.therobotreport.com/wp-content/uploads/2026/01/thomas-pilz-headshot-250.jpg] Thomas Pilz, managing partner of Pilz GmbH & Co. KG Our guest on this show is Thomas Pilz, managing partner of Pilz GmbH & Co. KG [https://www.therobotreport.com/tag/pilz/] — a dynamic leader and innovator in robotics safety. He oversees IT, purchasing, research and development, quality management, and production. Pilz previously served as CEO of Pilz Automation Safety L.P. in Michigan, leading the company’s North American operations. He also contributes to several technical and scientific boards, including the VDMA [https://www.vdma.eu/en/] and various research associations in the fields of automation and microelectronics. During the interview, we touch on current concerns with the rapidly evolving robotic landscape. We gather Thomas’ insights into new cutting-edge technologies like those being developed by 3Laws [https://www.therobotreport.com/tag/3laws/]. ---------------------------------------- SHOW TIMELINE * 0:45 – John Deere Excavator demo recap * 10:21 – News of the week * 33:25 – Thomas Pilz, Managing Partner of Pilz GmbH & Co. KG ---------------------------------------- mike oitzman sits in the cab of a John Deere 230P excavator. [https://www.therobotreport.com/wp-content/uploads/2026/01/mike-on-a-jd-230p-excavator.jpg] Editor and cohost Mike Oitzman gets his first opportunity to operate a John Deere excavator. | Credit: The Robot Report NEWS OF THE WEEK IROBOT EMERGES FROM CHAPTER 11 AS RESTRUCTURED PICEA U.S. SUBSIDIARY [https://www.therobotreport.com/irobot-emerges-from-chapter-11-picea-u-s-subsidiary/] iRobot Corp. is back from bankruptcy with the completion of Shenzhen Picea Robotics Co.’s acquisition of the consumer robotics pioneer. iRobot said it has emerged from a pre-packaged Chapter 11 process “with an improved financial foundation and additional capacity to invest in the next generation of smart home robotics.” Last month, iRobot announced that it had entered into a restructuring support agreement with its creditor Santrum Hong Kong Co. and contract manufacturer Picea. The company had faced falling revenue, diversification challenges, and antitrust concerns that doomed its proposed acquisition by Amazon. However, it said it remains committed to continuing to produce and support its fleet of millions of robotic vacuum cleaners. SERVE ROBOTICS TO ACQUIRE HOSPITAL LOGISTICS PROVIDER DILIGENT ROBOTICS [https://www.therobotreport.com/serve-robotics-acquires-diligent-robotics/] Serve Robotics Inc., a developer of sidewalk delivery robots, is acquiring Diligent Robotics Inc. for $29 million. The companies said the acquisition will open up new opportunities. With the Diligent acquisition, Serve Robotics is making its first foray into indoor environments, mobile manipulation, and the healthcare industry. Diligent currently operates its Moxi robots in more than 25 hospitals, where robots have completed over 1.25 million deliveries of medical supplies. Serve Robotics said it has spent the past year rapidly expanding the number of robots it has operating in the world, doing real work. With this experience, CEO Ali Kashani said he hopes to use Serve’s scaling and manufacturing capabilities to help Diligent Robotics to do the same. MICROSOFT RESEARCH REVEALS RHO-ALPHA VISION-LANGUAGE-ACTION MODEL FOR ROBOTS [https://www.therobotreport.com/microsoft-research-reveals-rho-alpha-vision-language-action-model-for-robots/] Microsoft announced Rho-alpha, or ρα, the first robotics model derived from its Phi series of vision-language models (VLAs). The new models built on Phi are intended to make robots more adaptable and trustworthy, the company said. “Rho-alpha translates natural language commands into control signals for robotic systems performing bimanual manipulation tasks,” wrote Ashley Llorens, corporate vice president and managing director of the Microsoft Research Accelerator. “It can be described as a VLA+ model in that it expands the set of perceptual and learning modalities beyond those typically used by VLAs.” For perception, Rho-alpha adds tactile sensing, and Microsoft said it is working to include modalities such as force. For learning, the company claimed that Rho-alpha can continually improve with feedback provided by people. Microsoft said it is looking to work with robotics manufacturers, integrators, and end users to see how Rho-alpha and associated tooling can help them train, deploy, and adapt cloud-hosted physical AI with their own data. HYUNDAI MOTOR’S KOREAN UNION WARNS OF HUMANOID ROBOT PLAN, SEES THREAT TO JOBS While South Korea is a leading adopter of industrial and service robots, fears of job displacement exist there as well as in other regions. Hyundai Motor Group [https://www.therobotreport.com/tag/hyundai-motor-group/]‘s labor union warned the automaker not to deploy humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] robots without its approval because of concerns about “employment shocks,” reported [https://www.reuters.com/business/world-at-work/hyundai-motors-korean-union-warns-humanoid-robot-plan-sees-threat-jobs-2026-01-22/] Reuters. Hyundai is the owner of Boston Dynamics and has said it plans [https://www.therobotreport.com/hyundai-purchase-tens-of-thousands-boston-dynamics-robots/] to buy “tens of thousands” of robots in the coming years. Boston Dynamics [https://www.therobotreport.com/tag/boston-dynamics/] is also testing its Atlas humanoid with Hyundai. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- The post Thomas Pilz on innovation and safety in robotics [https://www.therobotreport.com/thomas-pilz-on-innovation-and-safety-in-robotics/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "The podcast guest this week is Thomas Pilz, managing partner of Pilz GmbH & Co. KG.\nThe post Thomas Pilz on innovation and safety in robotics appeared first on The Robot Report."
      },
      {
        "title": "Registration opens for Robotics Summit & Expo 2026",
        "link": "https://www.therobotreport.com/registration-opens-for-robotics-summit-expo-2026/",
        "pubDate": "2026-01-23T18:04:46.000Z",
        "content": "[https://www.therobotreport.com/wp-content/uploads/2026/01/robotics-summit-2026-registration.png] Registration is now open for the Robotics Summit & Expo [http://www.roboticssummit.com], the world’s leading technical event for commercial robotics developers. The event takes place May 27-28 in Boston at the Thomas M. Menino Convention and Exhibition Center. The Robotics Summit & Expo is produced by The Robot Report and parent company WTWH Media. The Robotics Summit & Expo brings together the brightest minds in commercial robotics development. Our conference programming will provide engineers the information they need to develop and scale the next generation of commercial robots. Buy your full conference pass before March 2 to save and gain full access to all keynotes, technical sessions, networking receptions and special events. Discounts are also available for academia, associations, and corporate groups. Please email events[at]wtwhmedia.com for more details about our discount programs. MassRobotics [http://www.massrobotics.org], the world’s largest independent robotics hub dedicated to accelerating robotics innovation, commercialization and adoption, is the strategic partner of the Robotics Summit & Expo. ROBOTICS SUMMIT & EXPO CONFERENCE PROGRAMMING The Robotics Summit & Expo will have more than 50 sessions in tracks on artificial intelligence, design and development, enabling technologies, healthcare, and logistics. The Engineering Theater on the show floor will also feature presentations by industry experts. There will be 70-plus speakers confirmed from companies such as Agility Robotics, Amazon Robotics, ASTM International, AWS, Brain Corp, Fictiv, GM, Harmonic Drive, maxon, PickNik Robotics, QNX, RealSense, Robotics and AI Institute, Robust AI, Tesla, Toyota Research Institute, Universal Robots, and more. We will announce the full agenda soon. Stay tuned. The event will feature the following keynotes: What Makes a Robot Worthy? Speaker: Mikell Taylor, director of robotics strategy, GM The robotics industry is at an inflection point of opportunity, but continued growth relies on more than just novelty and excitement about advanced technology. Real impact will depend on robots being worthy of trust and adoption into environments with incredibly high bars for safety, uptime, performance, and results. Mikell draws on her experience at startups developing technology for customers big and small, as well as experience in leadership roles at companies like General Motors and Amazon, to urge the industry to focus on areas of robotics that can unlock exponential growth while avoiding stagnation in “pilot purgatory.” Building Reliable Robots at Scale — Safety, Determinism, and Real-Time Performance Speakers: QNX and industry-leading robotics manufacturers In this keynote panel, executives will share how they architect performance across control, perception, and networking, and how certified processes can compress time to market for safety-relevant systems. We’ll cover practical trade-offs when combining ROS 2 with hard real-time workloads. Attendees will leave with concrete patterns for building and validating reliable robotic systems. The Rise of Large Behavior Models in Robotics Speaker: Russ Tedrake, senior vice president of large behavior models, Toyota Research Institute Russ Tedrake will present TRI’s latest advances in large behavior models and their role in scaling robot intelligence. The keynote will explore how these models enable more robust, adaptive, and high-performance robotic systems across complex real-world tasks. Interview with Noland Arbaugh, world’s first Neuralink user In this keynote interview, I will sit down with Noland Arbaugh, the world’s first Neuralink user, for a firsthand discussion of brain–computer interface (BCI) technology. The conversation will explore how the system works, the impact it has had on Noland’s daily life, and what this breakthrough signals for the future of BCI-enabled robotics and human–machine interaction. INTERACTIVE EXPO FLOOR The expo hall at the Robotics Summit will have more than 250 exhibitors [https://rsedtb2026.mapyourshow.com/8_0/floorplan/index.cfm] showcasing the latest enabling technologies, products, and services that can help robotics engineers throughout their development journeys. Tennibot will be returning to allow attendees to play tennis on the showfloor with the help of a robotic trainer. The RBR50 Showcase returns for its third year to highlight winners of the annual RBR50 Robotics Innovation Awards. The Robotics Summit also offers numerous networking opportunities, a career fair, a robotics development challenge, and much more. CO-LOCATED EVENTS The Robotics Summit will be co-located with DeviceTalks Boston [https://boston.devicetalks.com/], the premier industry event for medical technology professionals, currently in its ninth year. Both events attract engineering and business professionals from a broad range of healthcare and medical technology backgrounds. SPONSORSHIP OPPORTUNITIES For information about sponsorship and exhibition opportunities, download the prospectus [https://www.roboticssummit.com/sponsorship-opportunities/]. Questions regarding sponsorship opportunities should be directed to Colleen Sepich at csepich[AT]wtwhmedia.com. The post Registration opens for Robotics Summit & Expo 2026 [https://www.therobotreport.com/registration-opens-for-robotics-summit-expo-2026/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Agility Robotics, Amazon Robotics, ASTM International, AWS, Brain Corp, GM, Harmonic Drive, maxon, PickNik Robotics, QNX, RealSense, Robust AI, Tesla, Toyota Research Institute, Universal Robots, and "
      },
      {
        "title": "iRobot emerges from Chapter 11 as restructured Picea U.S. subsidiary",
        "link": "https://www.therobotreport.com/irobot-emerges-from-chapter-11-picea-u-s-subsidiary/",
        "pubDate": "2026-01-23T16:49:28.000Z",
        "content": "The iRobot Roomba i7+ with Imprint Smart Mapping gives users control to choose which rooms are cleaned and when. Picea Robotics has made iRobot a wholly owned subsidiary. [https://www.therobotreport.com/wp-content/uploads/2026/01/Roombai7withImp.jpg] iRobot and Picea have set up a U.S. subsidiary to safeguard customer data. Source: iRobot iRobot Corp. is back from bankruptcy with the completion of Shenzhen Picea Robotics Co.’s acquisition of the consumer robotics pioneer. iRobot said it has emerged from a pre-packaged Chapter 11 process “with an improved financial foundation and additional capacity to invest in the next generation of smart home robotics.” Last month, iRobot announced [https://www.therobotreport.com/irobot-to-enter-chapter-11-acquired-chinese-creditor/] that it had entered into a restructuring support agreement with its creditor [https://www.therobotreport.com/irobot-debt-acquired-by-contract-manufacturer-as-bankruptcy-looms/] Santrum Hong Kong Co. and contract manufacturer Picea. The company [https://www.therobotreport.com/tag/irobot/] had faced falling revenue [https://www.therobotreport.com/irobot-revenue-drops-again-with-no-sources-of-additional-capital/], diversification challenges [https://www.therobotreport.com/looking-back-irobot-scrapped-retired-robots-through-years/], and antitrust concerns that doomed [https://www.therobotreport.com/irobot-terminates-deal-with-amazon-laying-off-31-of-staff/] its proposed acquisition by Amazon. However, it remained committed to continuing to produce and support its fleet of millions of robotic vacuum cleaners. “It has been a privilege to lead iRobot through this pivotal period, and I’m incredibly proud of the team’s resilience, focus, and commitment to our customers,” stated Gary Cohen, CEO of iRobot. “Building on iRobot’s strong foundation, I look forward to working with the Picea leadership team as we enter our next chapter with renewed momentum.” “My conviction in iRobot has never been stronger, and together with Picea, we are focused on delivering reliable, high-quality products, supporting our customers, protecting consumer data, and operating with discipline as we move forward,” he added. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- IROBOT RESTRUCTURES, FORMS UNIT TO PROTECT U.S. DATA Three engineers in MIT’s Artificial Intelligence Lab founded iRobot in 1990. Since introducing [https://www.therobotreport.com/a-look-at-irobot-35-year-robotics-journey/] its first Roomba robot vacuum in 2002, the company said it has sold more than 50 million devices to date. iRobot has also added features for cleaning, mapping, and navigation to its portfolio, which it plans to continue developing. In 2022, iRobot’s plan to map homes with its vacuums sparked criticism [https://www.therobotreport.com/irobot-addresses-privacy-concerns-pending-amazon-deal/] from privacy advocates. However co-founder and then-CEO Colin Angle said at the time that “iRobot does not – and will not – sell customers’ personal information. Our customers control the personal information they provide us, and we use that information to improve robot performance and the customer’s ability to directly control a mission.” As part of its restructuring plan, iRobot said it has implemented “structural, legal, and governance safeguards to protect U.S. and other global consumer data and connected devices.” They include the creation of iRobot Safe Corp., a U.S.-based subsidiary responsible for the protection of U.S. consumer data. iRobot Safe and the controls are intended to maintain a clear separation between iRobot’s Chinese [https://www.therobotreport.com/tag/china/] ownership and its U.S. and other global consumer data. iRobot Safe will be governed by an independent board composed of U.S. citizens and will include an independent U.S.-based data security officer and an iRobot Safe CEO, each subject to strict eligibility requirements. The company said these safeguards should give regulators, consumers, and partners confidence that its data governance framework to protect consumer data remains transparent, enforceable, and effective following the transaction. Some of iRobot’s founders and other industry experts had expressed concerns [https://www.therobotreport.com/robotics-industry-reacts-to-irobot-bankruptcy/] about potential Chinese ownership of a leading provider of robots to U.S. households. PICEA PROVIDES BUSINESS, CONSUMER CONTINUITY Picea, which has research and development and manufacturing facilities in China and Vietnam, said it holds more than 1,300 intellectual property rights worldwide. The company [https://www.piceacorp.com/] noted that it has manufactured, sold, and serviced over 20 million robotic vacuum cleaners [https://www.therobotreport.com/tag/cleaning]. Picea also said it has more than 7,000 employees and relationships with leading global enterprises. As a longtime contract manufacturer and contract lender for iRobot, Picea provided liquidity and operational support during the restructuring process. It said it helped [https://www.therobotreport.com/this-time-irobot-needs-help-cleaning-up-a-mess/] ensure continuity for customers, employees, suppliers, and global partners. Following the transaction, iRobot is now a privately held company wholly owned by Picea. iRobot will continue to be based in Bedford, Mass., with engineering, product development, marketing, and other corporate functions across the U.S. iRobot said it will advance its long-term innovation strategy, “focused on delivering trusted robotics and smart home devices, enhancing customer experiences, and investing in future product development.” The company did not specify whether it will move beyond floor-cleaning robots or if it would rehire any of the staffers laid off [https://www.therobotreport.com/irobot-terminates-deal-with-amazon-laying-off-31-of-staff/] in recent years. The post iRobot emerges from Chapter 11 as restructured Picea U.S. subsidiary [https://www.therobotreport.com/irobot-emerges-from-chapter-11-picea-u-s-subsidiary/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "iRobot has restructured after its bankruptcy proceedings and purchase by Chinese manufacturer Picea, adding a data security unit in the U.S.\nThe post iRobot emerges from Chapter 11 as restructured Pic"
      },
      {
        "title": "Zipline raises over $600M in funding, surpasses 2M commercial drone deliveries",
        "link": "https://www.therobotreport.com/zipline-raises-over-600m-in-funding-surpasses-2m-commercial-drone-deliveries/",
        "pubDate": "2026-01-22T20:59:03.000Z",
        "content": "A Zipline drone making a delivery. [https://www.therobotreport.com/wp-content/uploads/2026/01/ziplinedelivery-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/ziplinedelivery-featured.jpg Zipline said it beat its Q3 daily delivery volume target by about 30% and hit its Q4 target six weeks early. | Source: Zipline Zipline International Inc. this week announced it has surpassed 2 million commercial deliveries and raised more than $600 million. The company is also expanding operations to Houston and Phoenix in early 2026, followed by more metropolitan areas later in the year. The latest round of funding brings Zipline’s valuation to $7.6 billion. The milestone comes as the company [https://www.therobotreport.com/tag/zipline/] continues to grow across the U.S., delivering food [https://www.therobotreport.com/tag/food], retail [https://www.therobotreport.com/tag/retail], and healthcare [https://www.therobotreport.com/category/markets-industries/biotechnology-medical-healthcare/] products directly to customers’ homes in minutes. “Autonomous logistics has been maturing for more than a decade, and the last year has made it unmistakably clear that when deliveries are faster, cleaner, safer, and cheaper, demand isn’t just high; it grows exponentially,” stated Keller Cliffton, co-founder and CEO of Zipline. “In 2026, autonomous logistics will become an everyday staple for people across several states in the U.S.,” he added. “That transformation starts with Houston and my hometown of Phoenix, which we’ll begin serving early this year, and then expand to even more places across the country throughout the year.” In Houston and Phoenix, eligible customers will soon be able to order tens of thousands of items through the Zipline app [https://apps.apple.com/us/app/zipline-drone-delivery/id6472111511], with deliveries arriving in as little as 10 minutes via the company’s drones. Zipline claimed that its U.S. deliveries [https://www.therobotreport.com/tag/delivery-robots/] have grown by approximately 15% week over week for the past seven months, making it one of the fastest-growing AI [https://www.therobotreport.com/category/design-development/ai-cognition/] and robotics companies in the world. “As new markets come online, autonomous on-demand delivery is quickly moving from early adoption to everyday infrastructure,” it said. “This latest expansion is yet another sign that autonomous on-demand drone delivery will soon be the norm in America.” ZIPLINE PLANS TO USE LATEST FUNDING TO EXPAND TO 4 NEW STATES Zipline’s latest funding round included participation from several existing and new investors, including Fidelity Management & Research Co., Baillie Gifford, Valor Equity Partners, and Tiger Global. The company said it plans to use the capital to accelerate its expansion into at least four new states this year. Houston and Phoenix are the first two markets on that journey. “In the next five to 10 years, deliveries made by autonomous aircraft will become standard. That revolution is going to be led by Zipline,” said Antonio Gracias, founder, chief investment officer, and CEO of Valor Equity Partners. “There’s no better team, company, and product positioned to lead the charge than Zipline.” To date, Zipline said its zero-emission aircraft have flown more than 125 million autonomous commercial miles, delivering more than 20 million items without a serious injury. The company said it designed its system for speed and reliability at scale. Zipline added that its median flight time is three minutes. Customers routinely cite saving time as a reason why drone [https://www.therobotreport.com/category/robots-platforms/uav-drones/] deliveries quickly becomes part of their daily lives. Since August, Zipline said it has launched its delivery service to new areas weekly, unlocking thousands of customers every time. Each additional site ramps up faster than the last, according to the company Zipline’s first location in Dallas [https://www.therobotreport.com/faa-allows-wing-and-zipline-to-do-bvlos-flights-over-dallas/] took 10 weeks to reach 100 deliveries per day, and new sites reached that same volume in just two days. DRONE DELIVERY TAKES OFF ACROSS THE U.S. Zipline isn’t the only company working to make drone deliveries a regular part of life. Earlier this year, Wing Aviation LLC and Walmart Inc. announced [https://www.therobotreport.com/wing-brings-drone-delivery-to-150-more-walmart-stores/] they will expand drone deliveries to 150 more Walmart stores over the next year. Walmart [https://www.therobotreport.com/tag/walmart/] and Wing [https://www.therobotreport.com/tag/wing/] plan to establish a network of over 270 drone delivery locations by 2027, stretching from Los Angeles to Miami. The new service will launch in major hubs including Los Angeles, St. Louis, Cincinnati, and Miami, with others to be announced later. Zipline is also working with Walmart. Last year, the company launched [https://corporate.walmart.com/news/2025/06/05/walmart-takes-flight-with-drone-delivery-expansion-to-5-new-cities-redefining-fast-flexible-retail] its first-ever P2 drone site at Walmart stores in Mesquite and Waxahachie, Texas, offering a delivery experience that’s quieter than the average delivery truck, it claimed. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2026/01/Early-Bird-Promo-Inline-Ad-RSE-2.png]https://www.roboticssummit.com/ ---------------------------------------- The post Zipline raises over $600M in funding, surpasses 2M commercial drone deliveries [https://www.therobotreport.com/zipline-raises-over-600m-in-funding-surpasses-2m-commercial-drone-deliveries/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "In Houston and Phoenix, eligible customers will soon be able to order tens of thousands of items through the Zipline app.\nThe post Zipline raises over $600M in funding, surpasses 2M commercial drone d"
      }
    ],
    "arxiv": [
      {
        "id": "2601.18923",
        "title": "DeFM: Learning Foundation Representations from Depth for Robotics",
        "authors": [
          "Manthan Patel",
          "Jonas Frey",
          "Mayank Mittal",
          "Fan Yang",
          "Alexander Hansson",
          "Amir Bar",
          "Cesar Cadena",
          "Marco Hutter"
        ],
        "abstract": "Title:\n          DeFM: Learning Foundation Representations from Depth for Robotics\n        Comments:\n          Under review, 19 pages, 15 Figures, 9 Tables\n        \n          Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: this https URL",
        "link": "https://arxiv.org/abs/2601.18923",
        "submittedDate": "2026-01-28T23:09:46.603Z"
      },
      {
        "id": "2601.18963",
        "title": "Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot",
        "authors": [
          "Fauna Robotics: Diego Aldarondo",
          "Ana Pervan",
          "Daniel Corbalan",
          "Dave Petrillo",
          "Bolun Dai",
          "Aadhithya Iyer",
          "Nina Mortensen",
          "Erik Pearson",
          "Sridhar Pandian Arunachalam",
          "Emma Reznick",
          "David Weis",
          "Jacob Davison",
          "Samuel Patterson",
          "Tess Carella",
          "Michael Suguitan",
          "David Ye",
          "Oswaldo Ferro",
          "Nilesh Suriyarachchi",
          "Spencer Ling",
          "Erik Su",
          "Daniel Giebisch",
          "Peter Traver",
          "Sam Fonseca",
          "Mack Mor",
          "Rohan Singh",
          "Sertac Guven",
          "Kangni Liu",
          "Yaswanth Kumar Orru",
          "Ashiq Rahman Anwar Batcha",
          "Shruthi Ravindranath",
          "Silky Arora",
          "Hugo Ponte",
          "Dez Hernandez",
          "Utsav Chaudhary",
          "Zack Walker",
          "Michael Kelberman",
          "Ivan Veloz",
          "Christina Santa Lucia",
          "Kat Casale",
          "Helen Han",
          "Michael Gromis",
          "Michael Mignatti",
          "Jason Reisman",
          "Kelleher Guerin",
          "Dario Narvaez",
          "Christopher Anderson",
          "Anthony Moschella",
          "Robert Cochran",
          "Josh Merel"
        ],
        "abstract": "Title:\n          Fauna Sprout: A lightweight, approachable, developer-ready humanoid robot\n        \n          Recent advances in learned control, large-scale simulation, and generative models have accelerated progress toward general-purpose robotic controllers, yet the field still lacks platforms suitable for safe, expressive, long-term deployment in human environments. Most existing humanoids are either closed industrial systems or academic prototypes that are difficult to deploy and operate around people, limiting progress in robotics. We introduce Sprout, a developer platform designed to address these limitations through an emphasis on safety, expressivity, and developer accessibility. Sprout adopts a lightweight form factor with compliant control, limited joint torques, and soft exteriors to support safe operation in shared human spaces. The platform integrates whole-body control, manipulation with integrated grippers, and virtual-reality-based teleoperation within a unified hardware-software stack. An expressive head further enables social interaction -- a domain that remains underexplored on most utilitarian humanoids. By lowering physical and technical barriers to deployment, Sprout expands access to capable humanoid platforms and provides a practical basis for developing embodied intelligence in real human environments.",
        "link": "https://arxiv.org/abs/2601.18963",
        "submittedDate": "2026-01-28T23:09:46.603Z"
      },
      {
        "id": "2601.19098",
        "title": "SimTO: A simulation-based topology optimization framework for bespoke soft robotic grippers",
        "authors": [
          "Kurt Enkera",
          "Josh Pinskier",
          "Marcus Gallagher",
          "David Howard"
        ],
        "abstract": "Title:\n          SimTO: A simulation-based topology optimization framework for bespoke soft robotic grippers\n        Comments:\n          12 pages, 8 figures. Submitted to Structural and Multidisciplinary Optimization\n        \n          Soft robotic grippers are essential for grasping delicate, geometrically complex objects in manufacturing, healthcare and agriculture. However, existing grippers struggle to grasp feature-rich objects with high topological variability, including gears with sharp tooth profiles on automotive assembly lines, corals with fragile protrusions, or vegetables with irregular branching structures like broccoli. Unlike simple geometric primitives such as cubes or spheres, feature-rich objects lack a clear \"optimal\" contact surface, making them both difficult to grasp and susceptible to damage when grasped by existing gripper designs. Safe handling of such objects therefore requires specialized soft grippers whose morphology is tailored to the object's features. Topology optimization offers a promising approach for producing specialized grippers, but its utility is limited by the requirement for pre-defined load cases. For soft grippers interacting with feature-rich objects, these loads arise from hundreds of unpredictable gripper-object contact forces during grasping and are unknown a priori. To address this problem, we introduce SimTO, a framework that enables high-resolution topology optimization by automatically extracting load cases from a contact-based physics simulator, eliminating the need for manual load specification. Given an arbitrary feature-rich object, SimTO produces highly customized soft grippers with fine-grained morphological features tailored to the object geometry. Numerical results show our designs are not only highly specialized to feature-rich objects, but also generalize to unseen objects.",
        "link": "https://arxiv.org/abs/2601.19098",
        "submittedDate": "2026-01-28T23:09:46.603Z"
      },
      {
        "id": "2601.19234",
        "title": "iFAN Ecosystem: A Unified AI, Digital Twin, Cyber-Physical Security, and Robotics Environment for Advanced Nuclear Simulation and Operations",
        "authors": [
          "Youndo Do",
          "Chad Meece",
          "Marc Zebrowitz",
          "Spencer Banks",
          "Myeongjun Choi",
          "Xiaoxu Diao",
          "Kai Tan",
          "Michael Doran",
          "Jason Reed",
          "Fan Zhang"
        ],
        "abstract": "Title:\n          iFAN Ecosystem: A Unified AI, Digital Twin, Cyber-Physical Security, and Robotics Environment for Advanced Nuclear Simulation and Operations\n        \n          As nuclear facilities experience digital transformation and advanced reactor development, AI integration, cyber-physical security, and other emerging technologies such as autonomous robot operations are increasingly developed. However, evaluation and deployment is challenged by the lack of dedicated virtual testbeds. The Immersive Framework for Advanced Nuclear (iFAN) ecosystem is developed, a comprehensive digital twin framework with a realistic 3D environment with physics-based simulations. The iFAN ecosystem serves as a high-fidelity virtual testbed for plant operation, cybersecurity, physical security, and robotic operation, as it provides real-time data exchange for pre-deployment verification. Core features include virtual reality, reinforcement learning, radiation simulation, and cyber-physical security. In addition, the paper investigates various applications through potential operational scenarios. The iFAN ecosystem provides a versatile and secure architecture for validating the next generation of autonomous and cyber-resilient nuclear operations.",
        "link": "https://arxiv.org/abs/2601.19234",
        "submittedDate": "2026-01-28T23:09:46.604Z"
      },
      {
        "id": "2601.19275",
        "title": "Tactile Memory with Soft Robot: Robust Object Insertion via Masked Encoding and Soft Wrist",
        "authors": [
          "Tatsuya Kamijo",
          "Mai Nishimura",
          "Cristian C. Beltran-Hernandez",
          "Nodoka Shibasaki",
          "Masashi Hamaya"
        ],
        "abstract": "Title:\n          Tactile Memory with Soft Robot: Robust Object Insertion via Masked Encoding and Soft Wrist\n        Comments:\n          This work has been submitted to the IEEE for possible publication\n        \n          Tactile memory, the ability to store and retrieve touch-based experience, is critical for contact-rich tasks such as key insertion under uncertainty. To replicate this capability, we introduce Tactile Memory with Soft Robot (TaMeSo-bot), a system that integrates a soft wrist with tactile retrieval-based control to enable safe and robust manipulation. The soft wrist allows safe contact exploration during data collection, while tactile memory reuses past demonstrations via retrieval for flexible adaptation to unseen scenarios. The core of this system is the Masked Tactile Trajectory Transformer (MAT$^\\text{3}$), which jointly models spatiotemporal interactions between robot actions, distributed tactile feedback, force-torque measurements, and proprioceptive signals. Through masked-token prediction, MAT$^\\text{3}$ learns rich spatiotemporal representations by inferring missing sensory information from context, autonomously extracting task-relevant features without explicit subtask segmentation. We validate our approach on peg-in-hole tasks with diverse pegs and conditions in real-robot experiments. Our extensive evaluation demonstrates that MAT$^\\text{3}$ achieves higher success rates than the baselines over all conditions and shows remarkable capability to adapt to unseen pegs and conditions.",
        "link": "https://arxiv.org/abs/2601.19275",
        "submittedDate": "2026-01-28T23:09:46.604Z"
      },
      {
        "id": "2601.19376",
        "title": "Teaching Machine Learning Fundamentals with LEGO Robotics",
        "authors": [
          "Viacheslav Sydora",
          "Guner Dilsad Er",
          "Michael Muehlebach"
        ],
        "abstract": "Title:\n          Teaching Machine Learning Fundamentals with LEGO Robotics\n        Comments:\n          10 pages, 8 figures\n        \n          This paper presents the web-based platform Machine Learning with Bricks and an accompanying two-day course designed to teach machine learning concepts to students aged 12 to 17 through programming-free robotics activities. Machine Learning with Bricks is an open source platform and combines interactive visualizations with LEGO robotics to teach three core algorithms: KNN, linear regression, and Q-learning. Students learn by collecting data, training models, and interacting with robots via a web-based interface. Pre- and post-surveys with 14 students demonstrate significant improvements in conceptual understanding of machine learning algorithms, positive shifts in AI perception, high platform usability, and increased motivation for continued learning. This work demonstrates that tangible, visualization-based approaches can make machine learning concepts accessible and engaging for young learners while maintaining technical depth. The platform is freely available at this https URL, with video tutorials guiding students through the experiments at this https URL.",
        "link": "https://arxiv.org/abs/2601.19376",
        "submittedDate": "2026-01-28T23:09:46.604Z"
      },
      {
        "id": "2601.19406",
        "title": "Sim-and-Human Co-training for Data-Efficient and Generalizable Robotic Manipulation",
        "authors": [
          "Kaipeng Fang",
          "Weiqing Liang",
          "Yuyang Li",
          "Ji Zhang",
          "Pengpeng Zeng",
          "Lianli Gao",
          "Jingkuan Song",
          "Heng Tao Shen"
        ],
        "abstract": "Title:\n          Sim-and-Human Co-training for Data-Efficient and Generalizable Robotic Manipulation\n        \n          Synthetic simulation data and real-world human data provide scalable alternatives to circumvent the prohibitive costs of robot data collection. However, these sources suffer from the sim-to-real visual gap and the human-to-robot embodiment gap, respectively, which limits the policy's generalization to real-world scenarios. In this work, we identify a natural yet underexplored complementarity between these sources: simulation offers the robot action that human data lacks, while human data provides the real-world observation that simulation struggles to render. Motivated by this insight, we present SimHum, a co-training framework to simultaneously extract kinematic prior from simulated robot actions and visual prior from real-world human observations. Based on the two complementary priors, we achieve data-efficient and generalizable robotic manipulation in real-world tasks. Empirically, SimHum outperforms the baseline by up to $\\mathbf{40\\%}$ under the same data collection budget, and achieves a $\\mathbf{62.5\\%}$ OOD success with only 80 real data, outperforming the real only baseline by $7.1\\times$. Videos and additional information can be found at \\href{this https URL}{project website}.",
        "link": "https://arxiv.org/abs/2601.19406",
        "submittedDate": "2026-01-28T23:09:46.604Z"
      },
      {
        "id": "2601.19496",
        "title": "Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots",
        "authors": [
          "Jie Gu",
          "Hongrun Gao",
          "Zhihao Xia",
          "Yirun Sun",
          "Chunxu Tian",
          "Dan Zhang"
        ],
        "abstract": "Title:\n          Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots\n        \n          For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility.",
        "link": "https://arxiv.org/abs/2601.19496",
        "submittedDate": "2026-01-28T23:09:46.604Z"
      },
      {
        "id": "2601.19499",
        "title": "Reinforcement Learning Goal-Reaching Control with Guaranteed Lyapunov-Like Stabilizer for Mobile Robots",
        "authors": [
          "Mehdi Heydari Shahna",
          "Seyed Adel Alizadeh Kolagar",
          "Jouni Mattila"
        ],
        "abstract": "Title:\n          Reinforcement Learning Goal-Reaching Control with Guaranteed Lyapunov-Like Stabilizer for Mobile Robots\n        \n          Reinforcement learning (RL) can be highly effective at learning goal-reaching policies, but it typically does not provide formal guarantees that the goal will always be reached. A common approach to provide formal goal-reaching guarantees is to introduce a shielding mechanism that restricts the agent to actions that satisfy predefined safety constraints. The main challenge here is integrating this mechanism with RL so that learning and exploration remain effective without becoming overly conservative. Hence, this paper proposes an RL-based control framework that provides formal goal-reaching guarantees for wheeled mobile robots operating in unstructured environments. We first design a real-time RL policy with a set of 15 carefully defined reward terms. These rewards encourage the robot to reach both static and dynamic goals while generating sufficiently smooth command signals that comply with predefined safety specifications, which is critical in practice. Second, a Lyapunov-like stabilizer layer is integrated into the benchmark RL framework as a policy supervisor to formally strengthen the goal-reaching control while preserving meaningful exploration of the state action space. The proposed framework is suitable for real-time deployment in challenging environments, as it provides a formal guarantee of convergence to the intended goal states and compensates for uncertainties by generating real-time control signals based on the current state, while respecting real-world motion constraints. The experimental results show that the proposed Lyapunov-like stabilizer consistently improves the benchmark RL policies, boosting the goal-reaching rate from 84.6% to 99.0%, sharply reducing failures, and improving efficiency.",
        "link": "https://arxiv.org/abs/2601.19499",
        "submittedDate": "2026-01-28T23:09:46.604Z"
      },
      {
        "id": "2601.19510",
        "title": "ALRM: Agentic LLM for Robotic Manipulation",
        "authors": [
          "Vitor Gaboardi dos Santos",
          "Ibrahim Khadraoui",
          "Ibrahim Farhat",
          "Hamza Yous",
          "Samy Teffahi",
          "Hakim Hacid"
        ],
        "abstract": "Title:\n          ALRM: Agentic LLM for Robotic Manipulation\n        \n          Large Language Models (LLMs) have recently empowered agentic frameworks to exhibit advanced reasoning and planning capabilities. However, their integration in robotic control pipelines remains limited in two aspects: (1) prior \\ac{llm}-based approaches often lack modular, agentic execution mechanisms, limiting their ability to plan, reflect on outcomes, and revise actions in a closed-loop manner; and (2) existing benchmarks for manipulation tasks focus on low-level control and do not systematically evaluate multistep reasoning and linguistic variation. In this paper, we propose Agentic LLM for Robot Manipulation (ALRM), an LLM-driven agentic framework for robotic manipulation. ALRM integrates policy generation with agentic execution through a ReAct-style reasoning loop, supporting two complementary modes: Code-asPolicy (CaP) for direct executable control code generation, and Tool-as-Policy (TaP) for iterative planning and tool-based action execution. To enable systematic evaluation, we also introduce a novel simulation benchmark comprising 56 tasks across multiple environments, capturing linguistically diverse instructions. Experiments with ten LLMs demonstrate that ALRM provides a scalable, interpretable, and modular approach for bridging natural language reasoning with reliable robotic execution. Results reveal Claude-4.1-Opus as the top closed-source model and Falcon-H1-7B as the top open-source model under CaP.",
        "link": "https://arxiv.org/abs/2601.19510",
        "submittedDate": "2026-01-28T23:09:46.605Z"
      },
      {
        "id": "2601.19529",
        "title": "Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion",
        "authors": [
          "Jie Gu",
          "Yirui Sun",
          "Zhihao Xia",
          "Tin Lun Lam",
          "Chunxu Tian",
          "Dan Zhang"
        ],
        "abstract": "Title:\n          Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion\n        \n          In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module's stable reconfiguration ability, as well as its positional and docking accuracy.",
        "link": "https://arxiv.org/abs/2601.19529",
        "submittedDate": "2026-01-28T23:09:46.605Z"
      },
      {
        "id": "2601.19634",
        "title": "AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation",
        "authors": [
          "Wenda Yu",
          "Tianshi Wang",
          "Fengling Li",
          "Jingjing Li",
          "Lei Zhu"
        ],
        "abstract": "Title:\n          AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation\n        \n          Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success.",
        "link": "https://arxiv.org/abs/2601.19634",
        "submittedDate": "2026-01-28T23:09:46.605Z"
      },
      {
        "id": "2601.19643",
        "title": "Enhancing Worker Safety in Harbors Using Quadruped Robots",
        "authors": [
          "Zoe Betta",
          "Davide Corongiu",
          "Carmine Tommaso Recchiuto",
          "Antonio Sgorbissa"
        ],
        "abstract": "Title:\n          Enhancing Worker Safety in Harbors Using Quadruped Robots\n        \n          Infrastructure inspection is becoming increasingly relevant in the field of robotics due to its significant impact on ensuring workers' safety. The harbor environment presents various challenges in designing a robotic solution for inspection, given the complexity of daily operations. This work introduces an initial phase to identify critical areas within the port environment. Following this, a preliminary solution using a quadruped robot for inspecting these critical areas is analyzed.",
        "link": "https://arxiv.org/abs/2601.19643",
        "submittedDate": "2026-01-28T23:09:46.605Z"
      },
      {
        "id": "2601.19761",
        "title": "Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications",
        "authors": [
          "Jin Huang",
          "Fethiye Irmak Doğan",
          "Hatice Gunes"
        ],
        "abstract": "Title:\n          Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications\n        Comments:\n          HRI 2026\n        \n          Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users' immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields.",
        "link": "https://arxiv.org/abs/2601.19761",
        "submittedDate": "2026-01-28T23:09:46.605Z"
      },
      {
        "id": "2601.19826",
        "title": "Whether We Care, How We Reason: The Dual Role of Anthropomorphism and Moral Foundations in Robot Abuse",
        "authors": [
          "Fan Yang",
          "Renkai Ma",
          "Yaxin Hu",
          "Lingyao Li"
        ],
        "abstract": "Title:\n          Whether We Care, How We Reason: The Dual Role of Anthropomorphism and Moral Foundations in Robot Abuse\n        \n          As robots become increasingly integrated into daily life, understanding responses to robot mistreatment carries important ethical and design implications. This mixed-methods study (N = 201) examined how anthropomorphic levels and moral foundations shape reactions to robot abuse. Participants viewed videos depicting physical mistreatment of robots varying in humanness (Spider, Twofoot, Humanoid) and completed measures assessing moral foundations, anger, and social distance. Results revealed that anthropomorphism determines whether people extend moral consideration to robots, while moral foundations shape how they reason about such consideration. Qualitative analysis revealed distinct reasoning patterns: low-progressivism individuals employed character-based judgments, while high-progressivism individuals engaged in future-oriented moral deliberation. Findings offer implications for robot design and policy communication.",
        "link": "https://arxiv.org/abs/2601.19826",
        "submittedDate": "2026-01-28T23:09:46.605Z"
      },
      {
        "id": "2601.19832",
        "title": "Information-Theoretic Detection of Bimanual Interactions for Dual-Arm Robot Plan Generation",
        "authors": [
          "Elena Merlo",
          "Marta Lagomarsino",
          "Arash Ajoudani"
        ],
        "abstract": "Title:\n          Information-Theoretic Detection of Bimanual Interactions for Dual-Arm Robot Plan Generation\n        \n          Programming by demonstration is a strategy to simplify the robot programming process for non-experts via human demonstrations. However, its adoption for bimanual tasks is an underexplored problem due to the complexity of hand coordination, which also hinders data recording. This paper presents a novel one-shot method for processing a single RGB video of a bimanual task demonstration to generate an execution plan for a dual-arm robotic system. To detect hand coordination policies, we apply Shannon's information theory to analyze the information flow between scene elements and leverage scene graph properties. The generated plan is a modular behavior tree that assumes different structures based on the desired arms coordination. We validated the effectiveness of this framework through multiple subject video demonstrations, which we collected and made open-source, and exploiting data from an external, publicly available dataset. Comparisons with existing methods revealed significant improvements in generating a centralized execution plan for coordinating two-arm systems.",
        "link": "https://arxiv.org/abs/2601.19832",
        "submittedDate": "2026-01-28T23:09:46.605Z"
      },
      {
        "id": "2601.19839",
        "title": "HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs",
        "authors": [
          "Jeanne Malécot",
          "Hamed Rahimi",
          "Jeanne Cattoni",
          "Marie Samson",
          "Mouad Abrini",
          "Mahdi Khoramshahi",
          "Maribel Pino",
          "Mohamed Chetouani"
        ],
        "abstract": "Title:\n          HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs\n        \n          Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.",
        "link": "https://arxiv.org/abs/2601.19839",
        "submittedDate": "2026-01-28T23:09:46.605Z"
      },
      {
        "id": "2601.19856",
        "title": "Estimating Trust in Human-Robot Collaboration through Behavioral Indicators and Explainability",
        "authors": [
          "Giulio Campagna",
          "Marta Lagomarsino",
          "Marta Lorenzini",
          "Dimitrios Chrysostomou",
          "Matthias Rehm",
          "Arash Ajoudani"
        ],
        "abstract": "Title:\n          Estimating Trust in Human-Robot Collaboration through Behavioral Indicators and Explainability\n        \n          Industry 5.0 focuses on human-centric collaboration between humans and robots, prioritizing safety, comfort, and trust. This study introduces a data-driven framework to assess trust using behavioral indicators. The framework employs a Preference-Based Optimization algorithm to generate trust-enhancing trajectories based on operator feedback. This feedback serves as ground truth for training machine learning models to predict trust levels from behavioral indicators. The framework was tested in a chemical industry scenario where a robot assisted a human operator in mixing chemicals. Machine learning models classified trust with over 80\\% accuracy, with the Voting Classifier achieving 84.07\\% accuracy and an AUC-ROC score of 0.90. These findings underscore the effectiveness of data-driven methods in assessing trust within human-robot collaboration, emphasizing the valuable role behavioral indicators play in predicting the dynamics of human trust.",
        "link": "https://arxiv.org/abs/2601.19856",
        "submittedDate": "2026-01-28T23:09:46.605Z"
      },
      {
        "id": "2601.18975",
        "title": "HumanoidTurk: Expanding VR Haptics with Humanoids for Driving Simulations",
        "authors": [
          "DaeHo Lee",
          "Ryo Suzuki",
          "Jin-Hyuk Hong"
        ],
        "abstract": "Title:\n          HumanoidTurk: Expanding VR Haptics with Humanoids for Driving Simulations\n        Comments:\n          14 pages, 7 figures. To appear in CHI 2026\n        \n          We explore how humanoid robots can be repurposed as haptic media, extending beyond their conventional role as social, assistive, collaborative agents. To illustrate this approach, we implemented HumanoidTurk, taking a first step toward a humanoid-based haptic system that translates in-game g-force signals into synchronized motion feedback in VR driving. A pilot study involving six participants compared two synthesis methods, leading us to adopt a filter-based approach for smoother and more realistic feedback. A subsequent study with sixteen participants evaluated four conditions: no-feedback, controller, humanoid+controller, and human+controller. Results showed that humanoid feedback enhanced immersion, realism, and enjoyment, while introducing moderate costs in terms of comfort and simulation sickness. Interviews further highlighted the robot's consistency and predictability in contrast to the adaptability of human feedback. From these findings, we identify fidelity, adaptability, and versatility as emerging themes, positioning humanoids as a distinct haptic modality for immersive VR.",
        "link": "https://arxiv.org/abs/2601.18975",
        "submittedDate": "2026-01-28T23:09:46.605Z"
      },
      {
        "id": "2601.19462",
        "title": "Physical Human-Robot Interaction: A Critical Review of Safety Constraints",
        "authors": [
          "Riccardo Zanella",
          "Federico Califano",
          "Stefano Stramigioli"
        ],
        "abstract": "Title:\n          Physical Human-Robot Interaction: A Critical Review of Safety Constraints\n        \n          This paper aims to provide a clear and rigorous understanding of commonly recognized safety constraints in physical human-robot interaction, i.e. ISO/TS 15066, by examining how they are obtained and which assumptions support them. We clarify the interpretation and practical impact of key simplifying assumptions, show how these modeling choices affect both safety and performance across the system, and indicate specific design parameters that can be adjusted in safety-critical control implementations. Numerical examples are provided to quantify performance degradation induced by common approximations and simplifying design choices. Furthermore, the fundamental role of energy in safety assessment is emphasized, and focused insights are offered on the existing body of work concerning energy-based safety methodologies.",
        "link": "https://arxiv.org/abs/2601.19462",
        "submittedDate": "2026-01-28T23:09:46.607Z"
      },
      {
        "id": "2601.19851",
        "title": "How Does Delegation in Social Interaction Evolve Over Time? Navigation with a Robot for Blind People",
        "authors": [
          "Rayna Hata",
          "Masaki Kuribayashi",
          "Allan Wang",
          "Hironobu Takagi",
          "Chieko Asakawa"
        ],
        "abstract": "Title:\n          How Does Delegation in Social Interaction Evolve Over Time? Navigation with a Robot for Blind People\n        Comments:\n          Pre-print of paper accepted into CHI 2026\n        \n          Autonomy and independent navigation are vital to daily life but remain challenging for individuals with blindness. Robotic systems can enhance mobility and confidence by providing intelligent navigation assistance. However, fully autonomous systems may reduce users' sense of control, even when they wish to remain actively involved. Although collaboration between user and robot has been recognized as important, little is known about how perceptions of this relationship change with repeated use. We present a repeated exposure study with six blind participants who interacted with a navigation-assistive robot in a real-world museum. Participants completed tasks such as navigating crowds, approaching lines, and encountering obstacles. Findings show that participants refined their strategies over time, developing clearer preferences about when to rely on the robot versus act independently. This work provides insights into how strategies and preferences evolve with repeated interaction and offers design implications for robots that adapt to user needs over time.",
        "link": "https://arxiv.org/abs/2601.19851",
        "submittedDate": "2026-01-28T23:09:46.607Z"
      },
      {
        "id": "2412.05197",
        "title": "A Riemannian Take on Distance Fields and Geodesic Flows in Robotics",
        "authors": [
          "Yiming Li",
          "Jiacheng Qiu",
          "Sylvain Calinon"
        ],
        "abstract": "Title:\n          A Riemannian Take on Distance Fields and Geodesic Flows in Robotics\n        Comments:\n          27 pages, 20 figures. International Journal of Robotics Research (IJRR)\n        \n          Distance functions are crucial in robotics for representing spatial relationships between a robot and its environment. They provide an implicit, continuous, and differentiable representation that integrates seamlessly with control, optimization, and learning. While standard distance fields rely on the Euclidean metric, many robotic tasks inherently involve non-Euclidean structures. To this end, we generalize Euclidean distance fields to more general metric spaces by solving the Riemannian eikonal equation, a first-order partial differential equation whose solution defines a distance field and its associated gradient flow on the manifold, enabling the computation of geodesics and globally length-minimizing paths. We demonstrate that geodesic distance fields, the classical Riemannian distance function represented as a global, continuous, and queryable field, are effective for a broad class of robotic problems where Riemannian geometry naturally arises. To realize this, we present a neural Riemannian eikonal solver (NES) that solves the equation as a mesh-free implicit representation without grid discretization, scaling to high-dimensional robot manipulators. Training leverages a physics-informed neural network (PINN) objective that constrains spatial derivatives via the PDE residual and boundary and metric conditions, so the model is supervised by the governing equation and requires no labeled distances or geodesics. We propose two NES variants, conditioned on boundary data and on spatially varying Riemannian metrics, underscoring the flexibility of the neural parameterization. We validate the effectiveness of our approach through extensive examples, yielding minimal-length geodesics across diverse robot tasks involving Riemannian geometry.",
        "link": "https://arxiv.org/abs/2412.05197",
        "submittedDate": "2026-01-28T23:09:46.607Z"
      },
      {
        "id": "2508.18443",
        "title": "PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing",
        "authors": [
          "Ruohan Zhang",
          "Uksang Yoo",
          "Yichen Li",
          "Arpit Agarwal",
          "Wenzhen Yuan"
        ],
        "abstract": "Title:\n          PneuGelSight: Soft Robotic Vision-Based Proprioception and Tactile Sensing\n        Comments:\n          16 pages, 12 figures, International Journal of Robotics Research (accepted), 2025\n        \n          Soft pneumatic robot manipulators are popular in industrial and human-interactive applications due to their compliance and flexibility. However, deploying them in real-world scenarios requires advanced sensing for tactile feedback and proprioception. Our work presents a novel vision-based approach for sensorizing soft robots. We demonstrate our approach on PneuGelSight, a pioneering pneumatic manipulator featuring high-resolution proprioception and tactile sensing via an embedded camera. To optimize the sensor's performance, we introduce a comprehensive pipeline that accurately simulates its optical and dynamic properties, facilitating a zero-shot knowledge transition from simulation to real-world applications. PneuGelSight and our sim-to-real pipeline provide a novel, easily implementable, and robust sensing methodology for soft robots, paving the way for the development of more advanced soft robots with enhanced sensory capabilities.",
        "link": "https://arxiv.org/abs/2508.18443",
        "submittedDate": "2026-01-28T23:09:46.607Z"
      },
      {
        "id": "2601.18639",
        "title": "Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation",
        "authors": [
          "Ojasva Mishra",
          "Xiaolong Wu",
          "Min Xu"
        ],
        "abstract": "Title:\n          Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation\n        Comments:\n          Pending IEEE Transactions on Robotics Publication\n        \n          The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($\\tau=1.0$~s, $\\Delta t=0.01$~s, $u\\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\\%$. In simulation-only tuning, the certification screen rejects $11.6\\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments.",
        "link": "https://arxiv.org/abs/2601.18639",
        "submittedDate": "2026-01-28T23:09:46.608Z"
      }
    ]
  },
  "total_count": 39
}