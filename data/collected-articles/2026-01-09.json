{
  "fetch_date": "2026-01-09T00:35:13.854Z",
  "week_range": "Jan 2–8",
  "articles": {
    "ieee": [],
    "robotreport": [
      {
        "title": "Neocis’ Yomi platform completes over 100,000 osteotomies",
        "link": "https://www.therobotreport.com/neocis-yomi-platform-completes-over-100000-osteotomies/",
        "pubDate": "2026-01-08T20:00:34.000Z",
        "content": "The Yomi S system. [https://www.therobotreport.com/wp-content/uploads/2026/01/Yomi-S-Rendering-full-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/Yomi-S-Rendering-full-featured.jpg Yomi S has a smaller footprint than Neocis’ first system and features a longer arm to make it more flexible in tight spaces. | Source: Neocis Neocis Inc. yesterday said that clinicians have completed more than 100,000 osteotomies using the Yomi platform. The company also confirmed that the first clinical cases using Yomi S, its second-generation robotic system, have now been successfully completed by Dr. Jay Neugarten, DDS, MD, FACS. “Reaching 100,000 osteotomies is more than just a statistical achievement — it represents clinicians across the country choosing to integrate robotics into their daily surgical workflow,” said Alon Mozes, co-founder and CEO of Neocis. “This level of usage demonstrates trust in the technology and reinforces our belief that robotics has set a new standard of care.” Founded in 2009, Neocis said it aims to bring robotics to the field of dental surgery. The Miami-based company [https://www.therobotreport.com/tag/neocis/] collaborates with clinicians to develop technologies that help advance patient care and improve quality of life for physicians and patients. NEOCIS SYSTEM HELPS DENTISTS STAY ON TRACK The Yomi platform combines real-time haptic guidance, pre-operative digital planning, and intra-operative adaptability to assist clinicians during implant cases, said Neocis. Since its introduction, Yomi has been used in a wide variety of cases, from single-tooth to full-arch implant procedures. The system includes YomiPlan [https://www.neocis.com/products-and-services/yomiplan-software/] AI [https://www.therobotreport.com/category/design-development/ai-cognition/] to help plan dental procedures. It uses proprietary machine learning algorithms that automate the segmentation of critical anatomy, like nerves and sinuses, from cone beam computer tomography (CBCT) scans. “Clinicians are increasingly seeking technologies that integrate seamlessly into existing workflows while delivering tangible procedural value,” said Dennis Moses, chief technology officer of Neocis. “The scale reflected in 100,000 osteotomies validates both the robustness of the platform and its proven ability to perform consistently across real-world clinical environments.” The U.S. Food and Drug Administration (FDA) has cleared [https://www.therobotreport.com/yomi-robot-neocis-gets-fda-nod-full-arch-dental-implants/] Yomi [https://www.neocis.com/products-and-services/yomi-robot/] for robot-assisted dental surgery. Neocis said it was the first company to achieve FDA-clearance for a robotic system for dental implant surgery. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- YOMI S DELIVERS ENHANCED DEXTERITY Neocis unveiled [https://www.therobotreport.com/neocis-upgrades-yomi-s-robotic-system-dental-implants/] Yomi S in November 2025. The company said the system uses YomiPlan to deliver surgical precision, streamlined workflows, and high-quality patient outcomes. The system has a smaller footprint, enhanced dexterity, and improved visibility compared with its predecessor. “So you can plan the case in software using our AI-enabled planning. We can automatically find the nerve for you,” Moses told The Robot Report [https://www.therobotreport.com/] when Neocis released Yomi S. “We can automatically find the sinus and get it so that you can plan the case as close to perfect as possible. Then the robot guides them to get exactly what they planned in software.” “It works with the dentist hand in hand, using physical guidance with haptics as well as kind of visual and audio cues on the screen,” he added. “It’s kind of like Lane Assist; they have to color inside the lines. They can only drill according to the plan they made preoperatively.” Neocis said it continues to invest in expanding Yomi’s capabilities through software enhancements, workflow optimization, and advanced planning tools designed to support efficient case execution and broaden clinical applications. The post Neocis’ Yomi platform completes over 100,000 osteotomies [https://www.therobotreport.com/neocis-yomi-platform-completes-over-100000-osteotomies/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "The Yomi platform combines real-time haptic guidance, pre-operative digital planning, and intra-operative adaptability.\nThe post Neocis’ Yomi platform completes over 100,000 osteotomies appeared first"
      },
      {
        "title": "Doosan Bobcat unveils RX3 autonomous concept loader",
        "link": "https://www.therobotreport.com/doosen-bobcat-unveils-latest-rx3-concept-loader/",
        "pubDate": "2026-01-08T19:31:11.000Z",
        "content": "hero image of the bobcat rx3 prototype. [https://www.therobotreport.com/wp-content/uploads/2026/01/bobcat-rx3-featured.jpg] The design of RX3 has patents pending that cold shape Bobcat’s future lineup. | Credit: Doosan Bobcat At CES 2026 this week, Doosan Bobcat Inc. introduced a suite of AI-powered, autonomous, and electrified technologies such as the RX3 concept loader designed to simplify operations and increase productivity for construction equipment. The electric-powered Bobcat RogueX3 [https://www.bobcat.com/na/en/equipment/future-products/roguex] (RX3) is in its third generation. Doosan Bobcat designed it to be compact, quiet, autonomous, and with a footprint similar to existing Bobcat (manned) machines. The robot is equipped with tracks that provide traction across a variety of work surfaces. The modular design of the RX3 allows for interchangeable components: cab or no cab, wheels or tracks, configurable lift arms, and more. Users can tailor the machine to specific tasks, and it could be built and powered in multiple ways, including electric, diesel, hybrid, or even hydrogen, said the company [https://www.doosanbobcat.com/en]. “For nearly 70 years, Bobcat has led the compact equipment industry by solving real problems for real people,” said Scott Park, vice chairman and CEO of Doosan Bobcat. “Today, as workforce needs change and jobsites become more complex, we’re responding with intelligent systems that empower people to accomplish more, faster, and smarter. These innovations aren’t concepts for the distant future; they’re advancements that are shaping how work gets done right now.” Bobcat is also a technology partner of Agtonomy [https://www.therobotreport.com/tag/agtonomy/], which provides the intelligence, perception stack, and fleet management for custom-fitted Bobcat tractors for agriculture [https://www.therobotreport.com/category/markets-industries/ag/]. DOOSAN BOBCAT OFFERS AI SUPPORT FOR CONSTRUCTION EQUIPMENT Also at CES [https://www.therobotreport.com/tag/ces/], Doosan Bobcat announced the Bobcat Jobsite Companion [https://www.bobcat.com/na/en/equipment/future-products/jobsite-companion], an AI [https://www.therobotreport.com/category/design-development/ai-cognition/]-enabled feature for compact construction [https://www.therobotreport.com/tag/construction/] equipment. It said new features improve the overall functionality and safety of the system while offering increasing autonomy for operations. The onboard AI feature uses a proprietary large language model (LLM) to provide real-time voice and display support, automating more than 50 functions without requiring cloud connectivity, said the company. “Jobsite Companion lowers the barrier to entry for new operators while helping experienced professionals work faster and more precisely,” said Joel Honeyman, vice president of global innovation at Doosan Bobcat. “It’s not just smarter technology; it’s a smarter experience that puts expert-level guidance directly in the cab.” ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- OTHER RX3 INNOVATIONS ON DISPLAY AT CES In addition, Doosan Bobcat announced the following offerings: * Collision warning and avoidance system: A new system for compact machinery that uses imaging radar [https://www.therobotreport.com/tag/radar/] to track surroundings and that can automatically slow or stop the machine to prevent accidents. * Advanced display technology: A transparent MicroLED display integrated into cab windows. It overlays 360-degree camera views, performance data, and asset tracking directly onto the operator’s field of vision. * Bobcat Standard Unit Pack (BSUP): A modular, stackable, and rugged battery [https://www.therobotreport.com/category/technologies/batteries-power-supplies/] system designed for the harsh environments of construction sites. It is scalable across the Bobcat lineup and available to other manufacturers. * Service.AI: A support platform that gives technicians instant access to repair manuals, diagnostic guidance, and historical data to accelerate troubleshooting and minimize downtime. While these technologies are currently showcased as prototypes or concepts, they represent Bobcat’s shift toward integrating AI, autonomy, and electrification into the modern jobsite, said the company. Doosan Bobcat displayed its innovations at Booth 5840 [https://exhibitors.ces.tech/8_0/exhibitor/exhibitor-details.cfm?exhid=0013A00001W4W4dQAF] in the West Hall of the Las Vegas Convention Center. In Las Vegas, Doosan Robotics Inc. also demonstrated [https://www.automatedwarehouseonline.com/doosan-launches-systems-depalletizing-scan-go-repair-finishing/] its AI-Powered Depalletizing system, its Scan & Go Autonomous Robotic Solution for composites repair, and a partnership with Maple Advanced Robotics Inc. for finishing large structures. The post Doosan Bobcat unveils RX3 autonomous concept loader [https://www.therobotreport.com/doosen-bobcat-unveils-latest-rx3-concept-loader/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Bobcat unveiled the RX3 and other AI-powered and electric technologies at CES 2026 to simplify operations and boost jobsite productivity.\nThe post Doosan Bobcat unveils RX3 autonomous concept loader a"
      },
      {
        "title": "Qualcomm introduces general-purpose architecture for robotics",
        "link": "https://www.therobotreport.com/qualcomm-introduces-general-purpose-architecture-for-robotics/",
        "pubDate": "2026-01-08T15:46:31.000Z",
        "content": "Three different robot embodiments that Qualcomm can work with. [https://www.therobotreport.com/wp-content/uploads/2026/01/PhysicalAI-qualcomm-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/PhysicalAI-qualcomm-featured.jpg Qualcomm said its architecture can enable capabilities in service robots, industrial mobile robots, humanoids, and more. | Source: Qualcomm Qualcomm Technologies Inc. this week introduced its next-generation robotics architecture stack, which integrates hardware, software, and compound artificial intelligence. At CES, the company also unveiled its latest high-performance robotics processor for industrial mobile robots and full-size humanoids, the Qualcomm Dragonwing IQ10 Series. “As pioneers in energy efficient, high–performance physical AI [https://www.therobotreport.com/category/design-development/ai-cognition/] systems, we know what it takes to make even the most complex robotics systems perform reliably, safely, and at scale,” said Nakul Duggal, executive vice president and group general manager for automotive, industrial and embedded IoT [https://www.therobotreport.com/tag/iot/] and robotics at Qualcomm. “By building on our strong foundational low-latency, safety-grade, high-performance technologies, ranging from sensing, perception to planning and action, we’re redefining what’s possible with physical AI by moving intelligent machines out of the labs and into real-world environments,” he asserted. Qualcomm has more than 40 years of experience in developing technology. Its portfolio includes edge AI [https://www.therobotreport.com/category/design-development/ai-cognition/]; high-performance, low-power computing; and connectivity [https://www.therobotreport.com/category/technologies/networking-connectivity]. The San Diego, Calif.-based company [https://www.therobotreport.com/tag/qualcomm/] offers its Snapdragon platform for consumers, while its Dragonwing line is geared towards businesses and industries. ROBOTICS ARCHITECTURE MOVES FROM CONCEPT TO DEVELOPMENT A Figure robot pouring water from a pitcher in a kitchen. [https://www.therobotreport.com/wp-content/uploads/2025/09/figureai-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2025/09/figureai-featured.jpg Figure is targeting a range of applications in industrial and home environments with its humanoid. | Source: Figure AI Today, the Dragonwing industrial processor [https://www.therobotreport.com/category/technologies/microprocessors-socs/] roadmap powers an assortment of general-purpose form factors, including humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] robots from Booster [https://www.booster.tech/], VinMotion [https://vinmotion.net/], and other global robotics providers. “Figure’s mission is to develop general-purpose humanoid robots powered by advanced AI to eliminate unsafe and undesirable jobs, boost productivity across industries, and create economic abundance that enables happier, more purposeful lives for humanity,” stated Brett Adcock, the founder and CEO of Figure AI [https://therobotreport.com/tag/Figure-AI]. “Qualcomm Technologies’ platform, with its combination of exceptional compute capabilities and energy efficiency, is a valuable building block in enabling Figure to turn our vision into reality.” Qualcomm added that new architecture supports advanced perception [https://www.therobotreport.com/category/technologies/sensors-sensing/] and motion [https://www.therobotreport.com/category/robot-components/motioncontrol/] planning with “end-to-end” AI models such as vision-language-action models (VLAs) and vision-language models (VLMs), enabling generalized manipulation capabilities and human-robot interaction. The company said Dragonwing IQ10 [https://www.qualcomm.com/internet-of-things/products/iq10-series] helps it “take a significant step toward practical, real‑world deployment across industrial applications.” In addition, Qualcomm said it is in discussions with KUKA Robotics [https://www.therobotreport.com/tag/kuka/] for its next-generation robotic system. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- QUALCOMM BUILDS AN INTEGRATED, SUPPORTED STACK The general-purpose robotics architecture and the Dragonwing IQ10 combine heterogeneous edge computing, edge AI, mixed-criticality systems, software, machine learning operations, and an AI “data flywheel,” Qualcomm said. The company claimed that its approach enables robots to easily reason and adapt to the spatial and temporal environment. It is optimized to scale across various robotic form factors with industrial-grade reliability, according to Qualcomm. In addition, Qualcomm’s technologies are supported by a growing partner ecosystem and complemented by a suite of developer tools. This collaborative network can accelerate robotics development and deployment, solving the last-mile challenges and enabling faster, more scalable innovation across industries, it said. VinMotion’s Motion 2 [https://vinmotion.net/product/motion-2] humanoid, powered by the Dragonwing IQ9 Series, is on display at Booth 5001 [https://www.qualcomm.com/company/events/ces] during CES [https://www.therobotreport.com/tag/CES]. Qualcomm’s booth also features Booster’s K1 Geek [https://www.booster.tech/booster-k1/], which it said demonstrates Qualcomm’s leadership in edge AI and commitment to advancing physical AI. Qualcomm is also showing Advantech’s [https://www.therobotreport.com/advantech-qualcomm-partner-on-edge-ai-applications/] commercially available robotics development kit for rapid, multi‑application development and deployment. Separately, the booth features an in-depth look into teleoperation [https://www.therobotreport.com/tag/teleoperation] tooling and an AI data flywheel for collection, training, and deployment to continuously add new skills across robotic form factors. The post Qualcomm introduces general-purpose architecture for robotics [https://www.therobotreport.com/qualcomm-introduces-general-purpose-architecture-for-robotics/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Qualcomm said its new architecture supports advanced perception and motion planning with end-to-end AI models.\nThe post Qualcomm introduces general-purpose architecture for robotics appeared first on "
      },
      {
        "title": "Lyte brings in $107M to build perception systems for AI-enabled robots",
        "link": "https://www.therobotreport.com/lyte-brings-in-107m-to-build-perception-systems-for-ai-enabled-robots/",
        "pubDate": "2026-01-08T14:29:09.000Z",
        "content": "Lyte said its LyteVision is a vertically integrated sensing block that fuses 4D vision, RGB, and IMU at the edge. [https://www.therobotreport.com/wp-content/uploads/2026/01/lyte-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/lyte-featured.jpg LyteVision fuses 4D vision, RGB, and IMU sensing at the edge. | Source: Lyte AI Lyte AI this week emerged from stealth with $107 million in aggregate funding. The startup plans to use the investment to build a perception foundation for autonomous machines. The Mountain View, Calif.-based company [https://lyte.ai/home] said its core product, LyteVision, vertically integrates advanced 4D vision [https://www.therobotreport.com/category/technologies/cameras-imaging-vision/], RGB imaging, and motion sensing [https://www.therobotreport.com/category/technologies/sensors-sensing/] into a single platform to deliver unified spatial and visual data through one connection. LyteVision enables a wide range of physical AI [https://www.therobotreport.com/category/design-development/ai-cognition/] platforms, including autonomous mobile robots, robotic arms, quadrupeds, robotaxis, and humanoids, it claimed. “Physical AI will change how the world works, but only if robots can see it clearly,” stated Alexander Shpunt, co-founder and CEO of Lyte AI. “After helping shape how billions of people interact with technology, we’ve assembled an extraordinary team to build the perception layer that enables robots to operate safely and reliably at scale.” Shpunt, Arman Hajati, and Yuval Gerson, who previously worked on Apple’s depth-sensing and perception technologies, founded Lyte in 2021. Shpunt previously co-founded and served as chief technology officer of PrimeSense, the 3D sensing company that powered Microsoft Kinect. That company later became the foundation of Apple’s depth platform after its 2013 acquisition. LYTE AI ADDRESS A ‘STRUCTURAL PROBLEM’ IN ROBOTICS The AI robotics market will reach $125 billion by 2030, projected [https://www.grandviewresearch.com/industry-analysis/artificial-intelligence-ai-robotics-market-report] Grand View Research. However, more than 60% of industrial companies lack the internal capability to implement robotic automation, including sensor integration, according to [https://www.mckinsey.com/industries/industrials/our-insights/unlocking-the-industrial-potential-of-robotics-and-automation] McKinsey & Co. Lyte AI asserted that its approach addresses a “structural problem” in robotics. “Teams today assemble perception from multiple vendors, then spend months calibrating sensors, writing fusion software, and debugging integration failures,” it said. “Lyte’s vertically integrated stack eliminates that cycle.” The company said it has engineered its platform for safety, reliability, and performance from the hardware up. Lyte AI has paired that with an AI-driven operating layer that continuously advances alongside breakthroughs in vision, language, and action models. This enables physical AI systems to perceive, reason, and act with increasing intelligence over time, it said. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- CHAIRMAN IS A SEMICONDUCTOR ENTREPRENEUR Avigdor Willenz, a semiconductor entrepreneur, is Lyte’s founding investor and chairman of the board. The funding included participation from his group, Fidelity Management & Research Co., Atreides Management, Exor Ventures, Key1 Capital, and Venture Tech Alliance. “Lyte is building at the right layer, at the right moment,” said Willenz. “I’ve seen how foundational technologies unlock entire industries. What stands out here is the depth of the team and the discipline to solve perception as a system — where lasting value is created.” Lyte’s perception platform was recognized at CES [https://www.therobotreport.com/tag/CES] 2026 with a Best of Innovation Award in Robotics and as an Honoree in Vehicle Tech and Advanced Mobility, selected from a record 3,600 submissions. “Lyte is building core infrastructure for physical AI: a perception platform that helps robots safely understand and interact with the real world,” said Gavin Baker, managing partner at Atreides Management. “The founders already pioneered one era of 3D sensing and are among the select few with the credibility and technical depth to usher in a new frontier defined by coherent 4D vision and full-stack perception.” The post Lyte brings in $107M to build perception systems for AI-enabled robots [https://www.therobotreport.com/lyte-brings-in-107m-to-build-perception-systems-for-ai-enabled-robots/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Lyte AI said it engineered its platform, LyteVision, for safety, reliability, and performance from the hardware up.\nThe post Lyte brings in $107M to build perception systems for AI-enabled robots appe"
      },
      {
        "title": "ATDev develops Reflex to bring robotics and AI to rehabilitation",
        "link": "https://www.therobotreport.com/atdev-develops-reflex-to-bring-robotics-and-ai-to-rehabilitation/",
        "pubDate": "2026-01-07T16:56:19.000Z",
        "content": "ATDev has developed the Reflex wearable robot for rehabilitation. [https://www.therobotreport.com/wp-content/uploads/2026/01/ATDev_Reflex.jpg] Assistive Technology Development has developed the Reflex wearable robot for rehabilitation. Source: ATDev The time is right to apply robotics and artificial intelligence to mobility and manipulation challenges, according to the founders of Assistive Technology Development Inc., or ATDev. The company is working on the next generation of systems to help people with rehabilitation and daily life. ATDev co-founders Owen Kent and Todd Roberts met as roommates at the University of California, Berkeley [https://www.therobotreport.com/tag/uc-berkeley]. “I came to find out I was actually mentoring an engineering course designing for the human body that Todd was taking as part of his master’s program in exoskeleton design and mechanical engineering. So that was pretty serendipitous. And then I came to find out that we really shared a lot of similar passions for building assistive technology,” recalled Kent. “I’ve been a wheelchair user my whole life. I have a type of muscular dystrophy, and it’s a necessity rather than invention.” Their idea of using robotics for physical therapy began as a class project and turned into a company [https://assistivetech.dev/] with research funding from the National Science Foundation and the National Institutes of Health. “We got our first venture capital check in 2022 from UC Berkeley’s Skydeck program. It’s sort of a modified version of the Y Combinator program that Berkeley runs,” said Roberts. “They cut us a $200,000 check that let us get a little bit more money into developing some prototypes directly in line with what we wanted to build with Reflex.” “And then in 2023, we closed our pre-seed round with some investment from a small hardware-focused VC firm based out of Orange County called Solutions,” he added. “We were in their lab space for the last almost three years and raised a seed round back in 2025 that aligned with our U.S. launch of the Reflex product.” REFLEX PROVIDES CONSISTENT REHAB HELP Reflex [https://assistivetech.dev/reflex] is a robotic device that straps on to a user’s leg for orthopedic knee rehabilitation [https://www.therobotreport.com/tag/rehabilitation]. Weighing less than 5 lb. (2.2 kg), it includes sensors for remote monitoring and guided exercises. This allows for more sustained use than sessions with a human therapist, said ATDev. “This isn’t a tool to replace physical therapists; this is a tool to make them 10x more effective,” said Kent. ATDev has already proven that a physical therapy device can obtain insurance reimbursements, and it plans to show that its systems can provide support over longer timeframes. The Orange, Calif.-based company works with NVIDIA [https://www.therobotreport.com/tag/nvidia/], Amazon Web Services [https://www.therobotreport.com/tag/aws/], Kinova Robotics [https://www.therobotreport.com/tag/kinova-robotics/], and LUCI Mobility. It also has partnerships with the University of Pittsburgh, Carnegie Mellon University, Cornell University, Northeastern University, and Purdue University. RAAMP PROGRAM PROVIDES ATDEV FUNDING In November, ATDev announced [https://www.prnewswire.com/news-releases/atdev-joins-41-million-arpa-h-initiative-to-build-the-future-of-robotic-mobility-and-independence-302606603.html] that it was a subcontractor in the Robotic Assistive Mobility and Manipulation Platform (RAMMP [https://arpa-h.gov/explore-funding/awardees]) program. The Advanced Research Projects Agency for Health (ARPA-H) had awarded up to $41 million to the University of Pittsburgh for the program. RAMMP aims to create a new generation of open-source [https://www.therobotreport.com/tag/open-source] robotic technologies to provide mobility and manipulation assistance to people with disabilities so they can live more independently. ARPA-H was modeled after the Defense Advanced Research Projects Agency (DARPA [https://www.therobotreport.com/tag/darpa]) as part of the Biden administration’s “cancer moonshot,” noted Roberts. ATDev submitted its proposal to address the difficult problems of totally autonomous navigation and safe feeding for people with quadriplegia in April 2024. Getting federal funding can be a challenge with the change of administrations, Roberts acknowledged, but in early 2025, “the new administration picked up the baton and said, hey, we really like this. We wanna support our veteran community primarily through this technology, but [also] the aging population more broadly.” “There was a bunch of negotiations over the summer, because it’s not a traditional grant where you write the proposal, and they say yes or no, and then you go do it. It’s a contract for the government,” he said. “So you’re negotiating milestones, you’re negotiating total amount of funding, you’re negotiating intellectual property rights. And then finally, that all got over the finish line on the last possible day of the 2025 fiscal year. So Sept. 25, it was signed by the government, and we were greenlit before the shutdown. We kicked the project off in October and are off and rolling.” The Robotic Assistive Mobility and Manipulation Platform team at the announcement of its ARPA-H award. [https://www.therobotreport.com/wp-content/uploads/2026/01/ATDev_TEAM-scaled.jpg] The RAMMP team at the press conference announcing its ARPA-H award. Source: ATDev ROBOTIC WHEELCHAIR DEVELOPER GIVES USERS A SEAT AT THE TABLE Most powered wheelchair users are stuck with decades-old technology, such as lead-acid batteries and motors without encoders to know whether or not they’re moving, noted Kent. “As a lifelong wheelchair user, it’s been so frustrating for me to see a $50 robotics kit on Amazon that has better technology than my $40,000 wheelchair,” he said. “We’ve done probably over 200 customer interviews of people that use wheelchairs and have a lot of spreadsheets about things they like, things they don’t like, and so we really feel like we’ve done a lot of the legwork around the user needs and design requirements.” “Our vision for this company is to really have the end users have an active seat at the table,” said Kent. “I’m obviously the co-founder of the company, but we really plan to build out extensive advisory boards of end users that are able to actively help design and shape the technology.” ATDev also plans to eventually employ end users with disabilities as remote support technicians so they can bring their own experiences to understanding how the technology works, he said. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- ATDEV WORKS TO BRING ITS SYSTEMS TO MARKET With one ATDev division focusing on getting Reflex to market, a separate team is working on technical milestones for its robotic wheelchair. “One of our next most immediate milestones is an initial",
        "excerpt": "Assistive Technology Development, or ATDev, is working with people with disabilities to develop systems for rehab and daily tasks.\nThe post ATDev develops Reflex to bring robotics and AI to rehabilita"
      },
      {
        "title": "Mobileye to acquire Mentee Robotics for $900M in bid to dominate physical AI",
        "link": "https://www.therobotreport.com/mobileye-to-acquire-mentee-robotics-for-900m-bid-dominate-physical-ai/",
        "pubDate": "2026-01-07T15:41:33.000Z",
        "content": "hero image of a mentee humanoid robot. [https://www.therobotreport.com/wp-content/uploads/2026/01/mentee-featured.jpg] Mentee says it is developing an AI-first humanoid for real-world usefulness and adaptability. | Credit: Mentee Robotics Mobileye Global Inc. yesterday said it will acquire Mentee Robotics Ltd., which is developing a third-generation, vertically integrated humanoid robot. This transaction would combine Mobileye’s artificial intelligence technology and global production expertise with Mentee’s humanoid platform and AI talent with the goal of creating a global leader in physical AI across two markets: autonomous driving and humanoid robotics. Jerusalem-based Mobileye said it plans to pay $900 million, including about $612 million in cash and up to 26.2 million shares of Class A stock, subject to the vesting of any Mentee options before closing. It noted that the transaction is subject to customary closing conditions and that it expected to close it in the first quarter of 2026. Mobileye develops computer vision [https://www.therobotreport.com/category/technologies/cameras-imaging-vision/] and machine learning technology for the automotive [https://www.therobotreport.com/category/markets-industries/automotive/] industry. The company [https://www.therobotreport.com/tag/mobileye/] provides hardware and software for advanced driver-assistance systems (ADAS) and autonomous driving [https://www.therobotreport.com/category/robots-platforms/self-driving-vehicles/]. Its product lineup includes the EyeQ family of system-on-chips (SoCs [https://www.therobotreport.com/category/technologies/microprocessors-socs]), which process visual data to support vehicle safety features. Additional offerings include SuperVision for hands-off operation, Chauffeur for eyes-off autonomous driving, and the Drive platform for mobility [https://www.therobotreport.com/category/design-development/mobility-navigation/] services. To support these systems, the Mobileye uses Road Experience Management technology to generate maps through data collected from fleets of vehicles from global automakers. At a CES 2026 [https://www.therobotreport.com/tag/ces2026/] press conference, Prof. Amnon Shashua, co-founder and CEO of Mobileye, said he was impressed with the “Real2Sim2Real” technology at the core of Mentee Robotics’ intellectual property. The acquisition shouldn’t come as too much of a surprise, since he is also a co-founder of Mentee Robotics. The Herzliya, Israel-based company [https://www.menteebot.com/] emerged from stealth [https://www.therobotreport.com/mentee-robotics-de-cloaks-launches-ai-driven-humanoid-robot/] in 2024. Shashua abstained from voting on matters related to the transaction, which was proposed by a strategic committee that included independent directors and Intel Corp. [https://www.therobotreport.com/tag/intel], Mobileye’s largest shareholder. screenshot from amnon shashua's CES 2026 keynote. [https://www.therobotreport.com/wp-content/uploads/2026/01/mobileye-preso1.jpg] Mobileye is investing in Mentee’s Real2Sim2Real pipeline and AI training IP. | Credit: Mobileye “Today marks a new chapter for robotics and automotive AI, and the beginning of Mobileye 3.0,” said Shashua. “By combining Mentee’s breakthroughs in humanoid robotics with Mobileye’s expertise in automotive autonomy and its proven ability to productize advanced AI, we have a unique opportunity to lead the evolution of physical AI across robotics and autonomous vehicles on a global scale.” Prof. Lior Wolf, CEO of Mentee Robotics, said: “I am immensely proud of what Mentee’s multidisciplinary team has accomplished in just four years. We set out to build a platform that combines cutting-edge AI with deeply integrated hardware to make humanoid robots truly useful in real-world environments. Joining forces with Mobileye gives us access to unparalleled AI infrastructure and commercialization expertise, accelerating our mission to bring scalable, safe, and cost-effective humanoid solutions to market.” group shot of the mentee robotics cofounders. [https://www.therobotreport.com/wp-content/uploads/2024/04/mentee-robotics-founders.jpg] Mentee Robotics co-founders include Lior Wolf, CEO (L); Amnon Shashua, chairman (M); Shai Shalev-Shwartz, chief scientist (R). | Credit: Mentee Robotics MENTEE BUILDS HUMANOID WITH ENVIRONMENTAL AWARENESS Mentee Robotics said its system integrates scene understanding and natural command execution, can autonomously process tasks without remote control, offers reliable positional movement and navigation, and can safely manipulate rigid objects. Using simulation, the company is working on “few-shot generalization” technology to enable its robot to quickly learn and execute new skills and tasks with a small number of human demonstrations. In addition, MenteeBot’s integrated technologies include a proprietary actuator, high-precision motor drivers, a robotic arm equipped with motor-driven tactile sensing, and a hot-swappable battery. “Both autonomous driving and humanoid robotics face the same fundamental challenge: achieving reliable operation and demonstrating practical value in a human-dominated physical world,” said Mobileye. “Success requires meeting stringent performance requirements, achieving verifiable safety levels, operating efficiently on edge computing platforms, and enabling cost-effective, large-scale deployment.” a menteebot humanoid robot picks up a tote. [https://www.therobotreport.com/wp-content/uploads/2026/01/mentee-software.jpg] MenteeBot demonstrates autonomous task execution, encompassing scene perception, navigation and locomotion, detecting and localizing objects, grasping, and manipulating objects. | Credit: Mentee Robotics MOBILEYE CLAIMS ACQUISITION WILL CATALYZE PHYSICAL AI The companies said the acquisition will advance physical AI across robotics and autonomous vehicles. They cited the following technical synergies: * Enhanced autonomy stack: Mentee’s advancements in vision-language-action (VLA) models and large-scale simulation [https://www.therobotreport.com/category/software-simulation/] with novel Sim2Real transfer techniques complement Mobileye’s autonomy stack. The companies said these capabilities will strengthen autonomous driving systems through improved generalization of long-tail scenarios, faster adaptation to new environments, and more efficient development and validation cycles. * Safety [https://www.therobotreport.com/tag/safety] leadership for humanoids [https://www.therobotreport.com/category/robots-platforms/humanoids/]: Humanoid robots operating near humans, other machines, and dynamic environments require a level of verifiable safety that goes beyond reactive collision avoidance, acknowledged Mobileye. Unlike fixed automation, humanoids must reason in real time about human behavior, shared spaces, movable objects, and fragile surroundings, while producing predictable and auditable outcomes. Mobileye brings a safety-first approach developed for autonomous driving, including formal models such as Responsibility-Sensitive Safety (RSS), mathematically grounded decision-making under uncertainty, and system-level redundancy architectures validated at scale. It said these technologies provide a foundation for defining, verifying, and enforcing safe behavior to build the trust, reliability, and regulatory readiness required for humanoids to become economically viable at scale. * Accelerated commercialization: Mobileye cited its two decades of expertise in bringing advanced technologies to market with tools and infrastructure that adhere to strict safety standards. The company also has AI [https://www.therobotreport.com/category/design-development/ai-cognition/] training infrastructure and relationships with high-volume precision manufacturers that it said will accelerate deployment of humanoids in factories, warehouses, and industrial environments globally. MENTEEBOT TO ROLL OUT FOR INDUSTRY IN 2028, INTO HOMES IN 2030 Shashua was measured in his assessment of the go-to-market for the MenteeBot system, which is prudent given the current state of humanoid technology. He said the company plans to begin production in 2027 with partner Aumovio [https://www.aumovio.com/en.html], with a goal of beginning household [https://www.therobotreport.com/tag/household/] deployments in 2030. timeline showing the go to market for menteebot humanoid robot between 2026 and",
        "excerpt": "Mobileye is acquiring Mentee Robotics for $900 million to pursue leadership in both autonomous driving and humanoid robotics.\nThe post Mobileye to acquire Mentee Robotics for $900M in bid to dominate "
      },
      {
        "title": "Universal Robots partners with Robotiq and Siemens for smart palletizing",
        "link": "https://www.therobotreport.com/universal-robots-partners-with-robotiq-and-siemens-for-smart-palletizing/",
        "pubDate": "2026-01-07T13:30:36.000Z",
        "content": "A Robotiq gripper and a Universal Robots arm holding a box of chips or crisps for palletizing. [https://www.therobotreport.com/wp-content/uploads/2026/01/Siemens_UniversalRobots_Robotiq-featured.jpg] CES attendees can see the integration of a Universal Robots UR20 arm, Robotiq’s palletizing cell, and Siemens’ digital twin software. Source: Universal Robots Universal Robots A/S and Robotiq Inc. yesterday unveiled a robotic palletizing system at CES 2026. The companies created the system in collaboration with Siemens AG. The system combines Robotiq’s [https://www.therobotreport.com/tag/robotiq/] PAL Ready palletizing [https://www.therobotreport.com/tag/palletizing/] cell with Universal Robots’ [https://www.therobotreport.com/tag/universal-robots] UR20 force- and power-limited robot arm [https://www.therobotreport.com/category/robots-platforms/collaborative-robot/]. They are integrated into Siemens’ automation hardware and the Digital Twin Composer software it launched at the event. “Our work with Robotiq and Universal Robots exemplifies what can happen when you mix together advanced automation and real-time digital twins with the power of industrial AI,” stated Stuart McCutcheon, global vice president of sales and customer success at Siemens Digital Industries. “Siemens is proud to collaborate with Universal Robots and Robotiq to bring the industrial metaverse to life at CES and demonstrate how manufacturers can truly innovate faster, optimize operations, and unlock new efficiencies,” he said. SYSTEM USES DIGITAL TWINS TO IMPROVE REAL-WORLD EFFICIENCY Designed to support a company’s operational needs, the system palletizes boxes of chips and beverages. It uses digital twin analytics to optimize gripper [https://www.therobotreport.com/category/technologies/grippers-end-effectors/] performance and suction points dynamically. “This collaboration with Universal Robots and Siemens demonstrates how lean palletizing, combined with cutting-edge digital twin technology, can help companies boost efficiency and adapt quickly to changing demands,” said Samuel Bouchard, CEO of Lévis, Canada-based Robotiq. With data captured using Siemens’ Industrial Edge [https://x1.siemens.com/en-US/industrial-edge/] hardware and then streamed to its Insights Hub Copilot [https://blogs.sw.siemens.com/insights-hub/2025/01/30/production-copilot-ai-assistant-for-production/], the demonstrator provides real-time insights into cell behavior. The companies said this presents the scenario in a real-time photorealistic environment built using Siemens’ new Digital Twin Composer. “The seamless integration of our heavy-duty UR20 [https://www.universal-robots.com/products/ur20/] robot with Robotiq’s palletizing cell and Siemens’ advanced automation and software shows how digital and physical innovation can work hand in hand to transform production environments and deliver measurable ROI,” said Jean-Pierre Hathout, president of Teradyne Robotics [https://www.therobotreport.com/tag/teradyne-robotics/], the parent company of Universal Robots. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- SIEMENS SAYS DIGITAL TWIN COMPOSER APPLIES AI AT SCALE Siemens [https://www.therobotreport.com/tag/siemens] said its Digital Twin Composer builds industrial metaverse environments at scale. The company said the software empowers organizations to apply industrial AI [https://www.therobotreport.com/category/design-development/ai-cognition/], simulation [https://www.therobotreport.com/category/software-simulation/], and real-time physical data to make decisions virtually, at speed, and at scale. Digital Twin Composer enables industrial companies to combine 2D and 3D data from Siemens’ comprehensive digital twin with physical real-time information. It does this in a managed, secure, photorealistic visual scene, built using NVIDIA [https://www.therobotreport.com/tag/nvidia] Omniverse libraries. With Digital Twin Composer, Siemens said companies can rapidly build and maintain this global environment, containing all aspects of their product or production data (both virtual and physical). It does this in a secure, managed high-fidelity 3D experience throughout the lifecycle of the product, process, or facility. Digital Twin Composer provides contextualized, real-time insights enabling companies to visualize, interact with, and iterate on any product, process, or factory in its real-world context before physical design or construction, added Siemens. CES [https://www.therobotreport.com/tag/ces] attendees can see the demonstration at Siemens’ booth, No. 8725 in the North Hall of the Las Vegas Convention Center. Siemens’ Digital Twin Composer builds Industrial Metaverse environments at scale. [https://www.automatedwarehouseonline.com/wp-content/uploads/2026/01/siemens-1024x576.jpg] The new Digital Twin Composer from Siemens builds Industrial Metaverse environments at scale. | Source: Siemens The post Universal Robots partners with Robotiq and Siemens for smart palletizing [https://www.therobotreport.com/universal-robots-partners-with-robotiq-and-siemens-for-smart-palletizing/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Universal Robots has integrated its robot arm with Robotiq's palletizing cell and Siemens' digital twin software to make palletizing more efficient.\nThe post Universal Robots partners with Robotiq and"
      },
      {
        "title": "Oshkosh acquires core technology developed by Canvas",
        "link": "https://www.therobotreport.com/oshkosh-acquires-core-technology-developed-by-canvas/",
        "pubDate": "2026-01-07T13:00:04.000Z",
        "content": "The 1200CX can reduce work at height, cut 70% of repetitive motion, and capture 99% of dust, claims Canvas. [https://www.therobotreport.com/wp-content/uploads/2024/07/Canvas_finisher.jpg]https://www.therobotreport.com/wp-content/uploads/2024/07/Canvas_finisher.jpg The 1200CX can reduce work at height, cut 70% of repetitive motion, and capture 99% of dust, claims Canvas. | Source: Canvas Construction job sites are dynamic, unpredictable, and inherently complex. Oshkosh Corp. hopes to transform job sites with an intelligent and connected ecosystem. The company yesterday took a step forward in its goals by acquiring the core technology developed by Canvas Construction Inc. Oshkosh didn’t provide any financial details of the transaction. The Robot Report [https://www.therobotreport.com/] reached out to Oshkosh for more details but has not heard back as of press time. This story will be updated with additional details if they become available. San Francisco-based Canvas was founded in 2017. Its flagship construction [https://www.therobotreport.com/category/markets-industries/construction-demolition/] robot was 1200CX, a worker-controlled drywall finishing robot. The robot precisely sprayed layers of joint compounds onto walls in a single step. This sped up the finishing process from four or five days to just one day of applying the material and one day of sanding, according to the company [https://www.therobotreport.com/tag/canvas/]. Canvas and Oshkosh have been working together for six years, since Canvas selected the JLG platform to develop its robot. Since then, Oshkosh said the technology has advanced to automate repetitive tasks and enable consistent, high-quality results. “This addition adds to Oshkosh’s portfolio of intelligent technologies to address job-site challenges and support those who do tough work,” it said. OSHKOSH SHOWS OFF ROBOTS FOR REFUSE COLLECTION AT CES In addition to the acquisition, Oshkosh introduced and updated HARR-E, or the Hailable Autonomous Refuse Robot, Electric. The company [https://www.therobotreport.com/tag/OSHKOSH/] designed HARR-E for on-demand refuse collection. It enables residents to request a pickup using a smartphone app or a virtual at-home assistant. Then, HARR-E makes its way to the designated pickup point. The new version features a two-piece design that makes it easy to lift and transfer waste into a central dumpster or collection point, said Oshkosh. It measures the volume and weight of waste at each pickup, notifying waste companies when a Dumpster or central container is approaching capacity, and it uses AI-optimized routes to serve multiple requests efficiently. Oshkosh said the robot is suitable for planned communities, campuses, and corporate parks, event venues and stadiums, and even indoor environments, like malls and senior living facilities. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- MODULAR MOBILE ROBOTS HEAD TO AIRPORTS Oshkosh is also hoping to bring more robots to airports. At CES [https://www.therobotreport.com/tag/CES/], it showed a fleet of modular, autonomous robots that stand ready to assist ground support equipment (GSE) workers on the tarmac. Each robot is equipped to handle multiple ramp and airfield operational tasks. These intelligent machines are intended tackle repetitive jobs in inclement weather, day or night, and to aid the departure and arrival of flights and move [https://www.therobotreport.com/category/transportation/] passengers efficiently to their destinations. Oshkosh said they can help airlines manage costs, uptime, and performance. The company’s modular, commercial robotic platform is purpose-built to take on multiple jobs across the tarmac. Originally designed and used in the defense [https://www.therobotreport.com/category/markets-industries/defense-security/] industry, this system combines autonomous mobility, AI-driven perception, and task-configurable hardware to promote flexibility, utilization, and return on investment (ROI) in airport operations, said Oshkosh. The post Oshkosh acquires core technology developed by Canvas [https://www.therobotreport.com/oshkosh-acquires-core-technology-developed-by-canvas/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Canvas creates worker-controlled drywall finishing robots that provide consistent wall finishes faster than human workers. \nThe post Oshkosh acquires core technology developed by Canvas appeared first"
      },
      {
        "title": "Orbbec releases two new Gemini stereo cameras for robotics",
        "link": "https://www.therobotreport.com/orbbec-releases-two-new-gemini-stereo-cameras-for-robotics/",
        "pubDate": "2026-01-06T23:00:24.000Z",
        "content": "A hand holding the Gemini 305. [https://www.therobotreport.com/wp-content/uploads/2026/01/orbbec-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/orbbec-featured.jpg Gemini 305 measures 42 × 42 × 23mm and weighs just 65g, purpose-built for robotic wrist mounting. | Source: Orbbec Orbbec today unveiled two new products at CES 2026: the Gemini 305 and Gemini 345Lg. It also announced full platform compatibility between its flagship Gemini series cameras and NVIDIA Jetson Thor, as well as dual manufacturing capabilities in Vietnam and China. The Gemini 305 is a compact stereo 3D camera engineered for robotic wrist mounting. It features 4 cm (1.5 in.) ultra-close depth and color imaging capability, an expansive field of view, and comprehensive preset imaging modes. These enable it to provide visual data for recognition and grasping in collaborative robot [https://www.therobotreport.com/category/robots-platforms/collaborative-robot/] arms, humanoid [https://www.therobotreport.com/category/robots-platforms/collaborative-robot/] robot hands, and industrial [https://www.therobotreport.com/category/robots-platforms/industrial-robots/] flexible manipulation scenarios. The Gemini 345Lg is a rugged stereo vision camera built to thrive in demanding environments. It features IP67-rated protection and can operate in temperatures ranging from -20°C to 65°C (-4°F to 149°F). Orbbec said the Gemini 345Lg maintains stable performance in extreme heat, cold, and vibration. Founded in 2013, Orbbec offers products spanning structured light, stereo vision [https://www.therobotreport.com/category/technologies/cameras-imaging-vision/], ToF, and lidar technologies. The Shenzhen, China-based company [https://www.therobotreport.com/tag/orbbec/] said its sensors [https://www.therobotreport.com/category/technologies/sensors-sensing/] power robots and manufacturing [https://www.therobotreport.com/category/markets-industries/manufacturing/], logistics [https://www.automatedwarehouseonline.com/], retail, 3D scanning, healthcare [https://www.therobotreport.com/category/markets-industries/biotechnology-medical-healthcare/], and fitness systems. With in-house research and development, a state-of-the-art factory, supply chain management, and global support, Orbbec also offers OEM engagements for custom and embedded designs. GEMINI 305 PROVIDES WRIST-MOUNTED 3D VISION FOR ROBOTIC ARMS The Gemini 305 camera from Orbbec. [https://www.therobotreport.com/wp-content/uploads/2026/01/Gemini-305.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/Gemini-305.jpg Beyond the standard “Depth + Color” stream, Gemini 305 adds an “RGB + RGB” dual-stream mode with one-click switching between presets. | Source: Orbbec At the critical 15 cm (5.9 in.) “fingertip-class” working distance, the Gemini 305 [https://www.orbbec.com/gemini-305/] achieves sub-millimeter depth accuracy across its full 4 to 100 cm (1.5 to 39.3 in.) imaging range, said Orbbec. Beyond the 88° × 65° depth field of view, its single-channel color field of view (FOV) reaches 94° × 68°. This means it can capture objects measuring 8.6 × 5.4 cm (3.3 x 2.1 in.) at a distance of 20 cm (7.8 in.). At the pivotal wrist position, every extra degree of visual coverage translates directly to more agile robot performance, Orbbec claimed. According to the company, what sets Gemini 305 apart is its independent configuration of color and depth resolutions, delivering true on-demand decoupling. Traditional architectures, constrained by image signal processing (ISP) performance, force color and depth streams to share the same resolution. Gemini 305 overcomes this limitation, allowing each stream to be set independently while remaining spatially and temporally aligned, asserted Orbbec. This eliminates unnecessary trade-offs and improves data efficiency in robotic vision [https://www.therobotreport.com/category/technologies/cameras-imaging-vision/] pipelines. To further support diverse workloads, Gemini 305 includes multiple preset data-stream modes, with one-click switching between “color + depth” and “dual RGB” outputs. The former balances spatial perception with edge computing efficiency, while the latter strengthens semantic and visual understanding. Orbbec said this flexibility ensures that every frame delivers exactly what the algorithm needs, resolving data efficiency constraints in complex task flows. The camera streams real-time color and depth output at up to 1280×800 resolution and 60 fps. With direct depth output and latency as low as 60 ms, it can lighten the computational load on host processors dramatically, said Orbbec. Multi-camera sync uses both high-precision hardware and flexible software timestamp synchronization, while trigger-enabled capture allows scenario-based coordination—providing robust infrastructure for multi-robot collaborative operations. ORBBEC HOPES TO CLOSE THE HUMANOID ROBOTICS GAP The Gemini 305 and 305g sensors from Orbbec. [https://www.therobotreport.com/wp-content/uploads/2026/01/Gemini-305-and-Gemini-305g-scaled.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/Gemini-305-and-Gemini-305g-scaled.jpg The Gemini 305 and 305g variant. | Source: Orbbec Today’s mainstream wrist-mounted 3D cameras typically suffer from inadequate close-range performance, cramped fields of view, and rigid imaging modes. One leading product, for instance, bottoms out at 7 cm minimum working distance with roughly 87°×58° depth FOV and no flexible mode switching. “By contrast, Gemini 305 shrinks the minimum imaging distance to 4 cm—slashing the perception blind zone by 43%,” said Mike McSweeney, vice president of sales at Orbbec. “Paired with its expansive 88°×65° depth FOV, this ‘close-range + wide-angle’ combination substantially widens the perceptual envelope during intimate precision tasks.” “With one-click switching between ‘Depth + Color’ and ‘Dual RGB’ data streams, Gemini 305 delivers decisive advantages across the board—making it an ideal visual solution for small-part recognition, flexible grasping, and humanoid robots,” he added. To meet the rigorous demands of high-frequency dynamic routing on robotic arms, Orbbec also launched the Gemini 305g variant, equipped with GMSL2 serializer and FAKRA connector plus highly flexible, low-loss cabling. The company said GMSL2/FAKRA connectivity grants the Gemini 305g more stable and consistent data transmission than USB cameras, mitigating mechanical vibration, electromagnetic interference, and other environmental adversities. GEMINI 345LG PROVIDES VISION FOR OUTDOOR ROBOTICS The Gemini 345Lg. [https://www.therobotreport.com/wp-content/uploads/2026/01/345Lg-KV-.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/345Lg-KV-.jpg Orbbec’s new Gemini 345Lg. | Source: Orbbec Whether in pitch-black nighttime conditions or under intense midday sunlight exceeding 100 klux, Orbbec said the Gemini 345Lg [https://www.orbbec.com/345lg/] reliably outputs high-quality depth data. It provides dependable 3D vision for commercial lawn mowers [https://www.therobotreport.com/tag/mowing/], inspection [https://www.therobotreport.com/tag/inspection/] robots, outdoor logistics [https://www.therobotreport.com/category/markets-industries/logistics-warehousing-asrs/] vehicles, and other scenarios. The Gemini 345Lg features a wide operating temperature range and environmental adaptability, making it one of the few stereo vision cameras in the robotics field capable of easily handling extreme environments, Orbbec claimed. It offers dual-mode depth FOV of up to 104° × 87°/91° × 78°, RGB FOV of 137° × 71°, and IR FOV of 130° × 95°, providing vision coverage for environmental perception in outdoor robotics. With a GMSL2 serializer and FAKRA connector, Orbbec said the Gemini 345Lg delivers high-bandwidth depth data and color data reliably over long cable distances in demanding environments. GEMINI SERIES IS COMPATIBLE WITH NVIDIA JETSON THOR At CES [https://www.therobotreport.com/tag/CES] 2026, Orbbec announced that its Gemini 330 series stereo 3D cameras have completed full system-level adaptation and validation with the NVIDIA [https://www.therobotreport.com/tag/nvidia/] Jetson Thor system-on-module (SoM). Robot manufacturers deploying Orbbec cameras can now more easily access the sensor-processing capabilities of the Thor platform, while flexibly adopting diverse carrier board",
        "excerpt": "Orbbec announced new vision products, partnerships, and expanded manufacturing capabilities at CES in Las Vegas. \nThe post Orbbec releases two new Gemini stereo cameras for robotics appeared first on "
      },
      {
        "title": "Amazon acquires Rightbot, adds to Robotics Delivery and Packaging Innovation team",
        "link": "https://www.therobotreport.com/amazon-acquires-rightbot-adds-robotics-delivery-packaging-innovation-team/",
        "pubDate": "2026-01-06T19:25:37.000Z",
        "content": "Rightbot developed a system for unloading trucks and trailers. [https://www.therobotreport.com/wp-content/uploads/2023/12/rightbot-featured.jpg] Rightbot’s container unloading robot with a suction gripper. | Source: Rightbot Truck loading and unloading is a task ripe for automation because it is strenuous and happens in all kinds of weather. However, competition is stiff. Amazon.com Inc. has acquired Rightbot Technologies Inc., which developed a robot for unstructured load handling. The companies did not specify the terms of the transaction. Amazon [https://www.therobotreport.com/tag/amazon] has added a team of Rightbot’s employees to one of its own teams, an Amazon spokesman confirmed today to The Robot Report [https://www.therobotreport.com/]. “We are always evaluating opportunities to create efficiencies in our operations that ultimately benefit our customers and improve the safety of our employees,” he said. “We are excited to bring a group of innovative team members from Rightbot to Amazon’s Robotics Delivery and Packaging Innovation (RDPI) team and look forward to accomplishing much together.” AMAZON PREVIOUSLY INVESTED IN RIGHTBOT Amazon’s Industrial Innovation Fund led an investment [https://www.therobotreport.com/rightbot-unloading-robots-pick-6-25m-amazon-more/] of $4 million into Bergen, N.J.-based Rightbot as it emerged from stealth in 2023. Other backers included SOSV, Walmart’s Flipkart unit, and Entrepreneur First. “Our technology is not just an incremental improvement, but [also] a radical reimagining of how robotics can enhance operational efficiency in complex environments,” Abhinav Warrier, co-founder and chief technology officer of Rightbot, asserted at the time. “We’re not just developing robots; we’re crafting a new paradigm in handling unstructured loads.” Newark, N.J.-based Righbot used a suction gripper to move loads that could be varied and unpredictable. The company was an exhibitor at ProMat [https://www.therobotreport.com/tag/promat/] 2025, but its website was down as of today. MULTIPLE COMPANIES BUILD ROBOTS TO UNLOAD TRUCKS Several companies have taken a variety of approaches to automating the loading and unloading [https://www.automatedwarehouseonline.com/tag/loading/] of containers, trucks, and trailers. Dextrous Robotics shut down [https://www.therobotreport.com/dextrous-robotics-shuts-down/] in January 2024 after developing a chopstick-like robot. Slip Robotics [https://www.therobotreport.com/tag/slip-robotics] has built SlipBot, a mobile platform that unloads an entire trailer at once. Mujin [https://www.therobotreport.com/tag/mujin/]and Honeywell [https://www.therobotreport.com/honeywell-robotic-unloaded-distribution/] have used a mobile conveyor to reach into a truck to pull out boxes. Most truck-unloading robots use suction cups. For instance, Boston Dynamics [https://www.therobotreport.com/tag/boston-dynamics/]‘ Stretch uses suction cups and is more stable than its Handle predecessor. Gap [https://www.automatedwarehouseonline.com/gap-integrates-boston-dynamics-stretch-robot-to-unload-trailers/], DHL [https://www.automatedwarehouseonline.com/from-case-unloading-picking-beyond-dhl-boston-dynamics/], and Lidl [https://www.automatedwarehouseonline.com/lidl-unload-containers-boston-dynamics-stretch-robots-2026/] are among Stretch’s users, with DHL ordering [https://www.therobotreport.com/dhl-buying-1000-stretch-robots-from-boston-dynamics/] more than 1,000 robots. Pickle Robot [https://www.automatedwarehouseonline.com/tag/pickle-robot/] has also gotten commercial traction, with customers such as UPS and Randa Apparel [https://www.automatedwarehouseonline.com/randa-apparel-deploys-pickle-robot-to-unload-trucks/]. UPS plans [https://www.automatedwarehouseonline.com/ups-to-spend-120m-on-400-pickle-truck-unloading-robots/] to spend $120 million for 400 robots from Pickle. Anyware Robotics [https://www.therobotreport.com/tag/anyware-robotics/] said its Pixmo system is easy to deploy, while Contoro Robotics [https://www.therobotreport.com/tag/contoro-robotics/] said it uses artificial intelligence and teleoperation for unloading operations. Beckoff and Dexterity [https://www.therobotreport.com/tag/dexterity/] have partnered [https://www.automatedwarehouseonline.com/beckhoff-usa-dexterity-collaborate-develop-mech-robots/] to develop the two-armed Mech [https://www.automatedwarehouseonline.com/dexterity-launches-mech-dual-armed-mobile-manipulator-for-truck-unloading/] mobile manipulator [https://www.automatedwarehouseonline.com/tag/mobile-manipulation/]. In addition, autonomous forklift [https://www.automatedwarehouseonline.com/category/autonomous-forklifts/] providers such as Fox Robotics [https://www.automatedwarehouseonline.com/tag/fox-robotics/], Humro [https://www.automatedwarehouseonline.com/arapl-humro-unit-enters-u-s-market-truck-loading-unloading-robot/], and Multiway [https://www.mw-r.com/company-news/27] also offer to load and unload trucks. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- The post Amazon acquires Rightbot, adds to Robotics Delivery and Packaging Innovation team [https://www.therobotreport.com/amazon-acquires-rightbot-adds-robotics-delivery-packaging-innovation-team/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Amazon was an investor in Rightbot, which developed a suction-based robot for unloading trucks and trailers.\nThe post Amazon acquires Rightbot, adds to Robotics Delivery and Packaging Innovation team "
      },
      {
        "title": "AGIBOT launches Genie Sim 3.0 robot simulation platform",
        "link": "https://www.therobotreport.com/agibot-launches-genie-sim-3-0-robot-simulation-platform/",
        "pubDate": "2026-01-06T18:05:19.000Z",
        "content": "An overview of how Genie Sim works. [https://www.therobotreport.com/wp-content/uploads/2026/01/genieplatform-featured.jpg]https://www.therobotreport.com/wp-content/uploads/2026/01/genieplatform-featured.jpg An overview of how Genie Sim works. | Source: AGIBOT AGIBOT Innovation Technology Co. today introduced Genie Sim 3.0, a next-generation robot simulation platform powered by NVIDIA Isaac Sim, at CES [https://www.therobotreport.com/tag/CES/] 2026. The company said it delivers a unified, open simulation workflow that brings together digital asset generation, scene generalization, data collection, automated evaluation, and physics-based simulation in a single toolchain. AGIBOT said it designed Genie Sim’s core module, Genie Sim Benchmark, as a standardized evaluation system to establish an accurate and authoritative benchmarks for embodied intelligence. Genie Sim 3.0 draws from more than 10,000 hours of synthetic datasets, including real-world robot operation scenarios. The system integrates 3D reconstruction with visual generation to create a high-fidelity simulation [https://www.therobotreport.com/category/software-simulation/] environment. It also uses large language model (LLM) technology to generate scenes and evaluation metrics. The evaluation system covers more than 200 tasks across over 100,000 scenarios to build a comprehensive capability profile for models. AGIBOT said the platform can accelerate model development, reduce reliance on physical hardware, and empower innovation in embodied intelligence. The Shanghai, China-based company [https://www.agibot.com/] was established in February 2023. AGIBOT offers a range of humanoid [https://www.therobotreport.com/category/robots-platforms/humanoids/] robots and mobile robots [https://www.therobotreport.com/category/robots-platforms/amrs/] for cleaning and service settings. In December, AGIBOT released [https://www.prnewswire.com/news-releases/agibot-makes-debut-at-fortune-event-with-full-size-humanoid-robot-agibot-a2-as-special-guest-302630428.html] its updated humanoid, AGIBOT A2, in December. Later that month, the company rolled out [https://www.prnewswire.com/apac/news-releases/agibot-announces-the-rollout-of-its-5-000th-mass-produced-humanoid-robot-302635107.html] its 5,000th mass-produced humanoid robot at its factory. GENIE SIM 3.0 PROVIDES A FULL-STACK PIPELINE FOR TRAINING AND EVALUATION How Genie Sim's LLM-driven scene generation works. [https://www.therobotreport.com/wp-content/uploads/2026/01/image3.png]https://www.therobotreport.com/wp-content/uploads/2026/01/image3.png How Genie Sim’s LLM-driven scene generation works. | Source: AGIBOT AGIBOT said its integrated technologies enable it to deliver real-time, high-fidelity, and high-precision simulation environments. The system captures real-world environments with Skyland Innovation’s MetaCam handheld 3D laser scanner [https://skylandx.com/metacam-air/]. This scanner combines high-resolution RGB images, 360° lidar point clouds, and centimeter-level RTK (real-time kinematic) positioning. Interactable objects can be converted into simulation-ready assets from a single 60-second orbital video, significantly accelerating scene construction, said the company. A defining feature of Genie Sim 3.0 is its LLM-driven natural-language scene generation, AGIBOT claimed. Users can describe environments conversationally, and the system automatically produces structured scenes, visual previews, and thousands of semantic variations without manual logic coding. Vision-language models further refine and tune these scenes to meet specification-level needs, enabling rapid adaptation and strong model generalization, it claimed. In addition, Genie Sim 3.0 integrates real industrial-scene datasets into both training and evaluation pipelines. This approach significantly shortens algorithm-validation cycles and reduces dependence on hardware infrastructure. AGIBOT PROVIDES OVER 10,000 HOURS OF SYNTHETIC DATA An overview of how Genie Sim handles multi-dimentional evaluation. [https://www.therobotreport.com/wp-content/uploads/2026/01/image2.png]https://www.therobotreport.com/wp-content/uploads/2026/01/image2.png An overview of how Genie Sim handles multi-dimensional evaluation. | Source: AGIBOT AGIBOT has also released over 10,000 hours of open-source synthetic data covering more than 200 tasks with multi-sensor modalities such as RGB-D, stereo vision, and whole-body kinematics. An intelligent data-collection toolkit supports both low-latency teleoperation and automated task programming. With auto-annotation and a recovery mechanism that resumes collection after task failures, it can reduce the cost and time of dataset production. For evaluation, Genie Sim 3.0 provides more than 100,000 simulation scenarios, which AGIBOT said moves beyond single-metric benchmarks to construct full-spectrum capability profiles for embodied models. Using LLMs, the platform can auto-generate executable evaluation workflows across diverse semantic, spatial-reasoning, and manipulation dimensions. The company said it outlines model strengths, limitations, and optimization pathways. All simulation assets, datasets, and automated evaluation source code are fully open-source. Developers and researchers can access AGIBOT Genie Sim on GitHub [https://github.com/AgibotTech/genie_sim]. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- The post AGIBOT launches Genie Sim 3.0 robot simulation platform [https://www.therobotreport.com/agibot-launches-genie-sim-3-0-robot-simulation-platform/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Genie Sim 3.0 draws from more than 10,000 hours of synthetic dataset, including real-world robot operation scenarios.\nThe post AGIBOT launches Genie Sim 3.0 robot simulation platform appeared first on"
      },
      {
        "title": "ANELLO launches compact navigation system for resilient drone operations",
        "link": "https://www.therobotreport.com/anello-launches-compact-navigation-system-resilient-drone-operations/",
        "pubDate": "2026-01-06T16:00:58.000Z",
        "content": "A drone helicopter for defense or surveillance missions. ANELLO Aerial INS supports accurate navigation in GNSS-denied environments. [https://www.therobotreport.com/wp-content/uploads/2026/01/Anello_aviation.jpg] ANELLO Aerial INS supports precise navigation for UAVs and autonomous VTOLs. Source: ANELLO Reliable navigation is especially important in GPS-denied or contested environments such as battlefields. ANELLO Photonics today launched ANELLO Aerial INS, a compact, high-performance inertial navigation system built around the company’s Silicon Photonic Optical Gyroscope, or SiPhOG technology. “Customers flying real missions need resilient navigation when GNSS isn’t reliable,” stated Dr. Mario Paniccia, co-founder and CEO of ANELLO Photonics. “By combining our SiPhOGs with our airborne-optimized sensor-fusion algorithms and integrated multi-band GNSS, the ANELLO Aerial INS delivers fiber-optic gyro performance in a cost-effective SWaP [size, weight, and power]-friendly package.” “This allows UAVs to hold course through jamming, multipath, spoofing or outages and complete the mission safely and successfully,” he said. Santa Clara, Calif.-based ANELLO developed SiPhOG [https://www.anellophotonics.com/siphog] based on integrated photonic system-on-chip (SoC [https://www.therobotreport.com/category/microprocessors_soc]) technology. The company [https://www.therobotreport.com/anello-photonics-secures-funding-inertial-navigation-gps-denied-environments/] has more than 40 issued patents, with 40 pending. ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- ANELLO AERIAL INS DESIGNED TO PROVIDE GUIDANCE “Today’s consumer-grade gyros are low accuracy and are all MEMS [micro-electromechanical systems]-based. They’re very sensitive to vibration and EMI [electromagnetic interference],” Paniccia told The Robot Report [https://www.therobotreport.com/]. “But then you get to industrial/defense-grade systems, which are high accuracy and more expensive. These are the big, bulky systems in missiles, fighter jets, and submarines. SiPhOG is the world’s smallest optical gyro, with no moving parts and high-volume assembly.” The fiber gyro splits two beams of light into a coil and then combines them to use the phase shift to determine the rotation rate, explained Gerhard Boiciuc, vice president of business development and partnerships at ANELLO Photonics. It can be put on a 1-in. sq. chip, which is fabricated in the U.S. The technology could work in everything from farm [https://www.anellophotonics.com/applications/agriculture] tractors to drones [https://www.anellophotonics.com/applications/unmanned-aerial-systems], and it was tested last summer in maritime [https://www.anellophotonics.com/products/maritime-ins] settings, said Paniccia. ANELLO won [https://www.anellophotonics.com/us-navy-awards-anello-photonics-contract-phase-ii] a U.S. Navy Small Business Innovation Research (SBIR) Phase II contract in September 2025. The company said it designed ANELLO INS with direct customer input for stringent performance requirements for air, land, and sea applications. The system is integrated with multi-band GNSS receivers and mission-ready platforms. The ANELLO Aerial INS uses an EKF (Extended Kalman Filter)-based sensor-fusion engine and the company’s flight-profile-tuned algorithms, consistently delivering >98% navigation accuracy without cameras or fiber-optic cables. ANELLO said the unit can maintain accurate navigation and control through high-dynamics and GNSS jamming, spoofing, or occlusion. Key features of the ANELLO Aerial INS include: * High-precision three-axis SiPhOG optical gyros with <0.5º/hr. unaided heading drift for reliable dead-reckoning during GNSS outages * Dual triple-frequency, all-constellation GNSS receivers with static heading capability, ready for RTK/PPP (precise point positioning) corrections * ANELLO Advanced Sensor Fusion Engine with GNSS spoofing detection for resilient holdover in GPS-denied or spoofed conditions * Flight-stack integration: PX4 and ArduPilot drivers; standard interfaces (Ethernet, RS-232, RS-422, CAN) and timing (PPS Out/PPS Sync In) * NMEA-compliant GNSS Interface outputs National Marine Electronics Association (NMEA) navigation packets for seamless integration as a drop-in replacement for conventional GNSS receivers * Flight-profile optimization: Algorithms calibrated for beyond visual line-of-sight (BVLOS [https://www.therobotreport.com/tag/bvlos]), intelligence, surveillance, and reconnaissance (ISR), vertical takeoff and landing (VTOL), and other autonomous aerial vehicles for accurate navigation [https://www.therobotreport.com/tag/navigation] * Rugged, compact, and lightweight: 4.4 x 3.4 x 1.9 in. (11.1 x 8.6 x 4.8 cm) package at 1 lb. (0.4 kg); low power consumption of <6W; and vibration-tolerant design for multirotor, fixed-wing, and VTOL * IP68: Waterproof, resistant to corrosion, salt spray, and chemicals Diagram of ANELLO Photonics' aerial inertial navigation system. [https://www.therobotreport.com/wp-content/uploads/2026/01/Anello_Aerial_INS_02.jpg] The new navigation system is designed for portability and accuracy. Source: ANELLO Photonics ANELLO ADDS TO PROVEN NAVIGATION PORTFOLIO ANELLO said its navigation [https://www.therobotreport.com/tag/navigation] solutions provide assured GNSS-denied operation in over-water/desert corridors, urban canyons, night/low-light scenarios, and fog/cloud conditions. This provides reliable, accurate guidance without GPS and improves warfighter [https://www.therobotreport.com/category/markets-industries/defense-security/] effectiveness and survivability, claimed the company. It added that it has designed its systems for demanding aerial platforms, including BVLOS uncrewed aerial systems (UAS [https://www.therobotreport.com/category/robots-platforms/uav-drones/]), maritime [https://www.therobotreport.com/tag/maritime]/shipborne VTOL UAS, ISR/special-mission aircraft, heavy-lift and cargo drones, and other drones. “Our road map is to continue shrink and integrate electronics and photonics for less weight and power,” Paniccia said. “We eventually want to bring high-precision navigation to everything.” In addition, ANELLO asserted that the introduction of ANELLO Aerial INS alongside its proven ground and maritime [https://www.anellophotonics.com/vatn-systems-launches-ins-powered-by-anello] systems extends its lead in GPS-denied navigation across domains, as validated through multiple U.S. Department of War [https://www.therobotreport.com/tag/u-s-department-of-defense] operational test events. “The ANELLO SiPhOG technology is a game changer for our warfighters,” said Dan Magy, CEO of Firestorm [https://www.launchfirestorm.com/], which is developing low-cost drones for combat and ISR applications. “The ability to navigate in GPS-denied or spoofed environments without cameras or fiber-optic cables — in small, lightweight systems — is essential for future combat missions.” “The ANELLO team has developed an aerial solution that seamlessly integrates into existing avionics with minimal effort,” he added. “This type of capability is essential in today’s conflict areas where our adversaries actively disrupt GPS, making ANELLO a powerful upgrade for all modern aerial platforms.” The ANELLO Aerial INS is now available for evaluation, with production shipments beginning in the second quarter of 2026. Evaluation kits include the ANELLO Aerial INS, cabling, drivers for PX4/ArduPilot, and a quick-start integration guide. The company is exhibiting at CES this week at Booth 10077 in the North Hall of the Las Vegas Convention Center. The post ANELLO launches compact navigation system for resilient drone operations [https://www.therobotreport.com/anello-launches-compact-navigation-system-resilient-drone-operations/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "ANELLO has developed an inertial navigation system based on its SiPhOG technology for reliable navigation in GNSS-denied environments.\nThe post ANELLO launches compact navigation system for resilient "
      },
      {
        "title": "NVIDIA releases new physical AI models, plus autonomous vehicle tools",
        "link": "https://www.therobotreport.com/nvidia-releases-new-physical-ai-models-plus-autonomous-vehicle-tools/",
        "pubDate": "2026-01-06T13:00:40.000Z",
        "content": "NVIDIA partners such as CAterpillar, LEM Surgical, AGIBOT, and Franka Robotics have used NVIDIA technologies to power physical AI. [https://www.therobotreport.com/wp-content/uploads/2026/01/Physical-AI_NVIDIA_CES26.jpg] NVIDIA partners such as Caterpillar (top left), LEM Surgical (top right), AGIBOT (bottom right), and Franka Robotics (bottom left) have used NVIDIA physical AI technologies to power autonomous machines ranging from industrial humanoids to surgical robots. Source: NVIDIA LAS VEGAS — To understand and operate in dynamic environments, physical AI needs to be able to learn, reason, and plan, according to NVIDIA Corp. The company yesterday announced new open models, frameworks, simulation tools, datasets, and artificial intelligence infrastructure for robotics and self-driving vehicle developers. “The ChatGPT moment for robotics is here,” said Jensen Huang, founder and CEO of NVIDIA. “Breakthroughs in physical AI — models that understand the real world, reason and plan actions — are unlocking entirely new applications.” “NVIDIA’s full stack of Jetson robotics processors, CUDA, Omniverse, and open physical AI models empowers our global ecosystem of partners to transform industries with AI-driven robotics,” he added. NVIDA OFFERS MODELS FOR ‘GENERALIST-SPECIALIST’ ROBOTS “Today, most robots are specialists. They are excellent at one single task, but they cannot adapt to anything else,” said Rev Lebaredian, vice president of Omniverse and simulation technology at NVIDIA. “Now, we are seeing generalist robots. Like someone with a bachelor’s degree, they can handle different situations. However, they lack the expert skills for complex jobs.” “The future belongs to the generalist-specialist,” he asserted. “Think of them as the Ph.D.s of the robot world. They combine broad knowledge with deep expertise, making them versatile and reliable. Building these advanced robots requires an open development platform. Perception alone is not enough, which is why we’re offering the new Cosmos models for reasoning and advanced foundation models with world generation and understanding.” Today’s machines are single-task and hard to program, said NVIDIA. Making them more capable typically requires enormous capital and expertise to build foundation models, but the Santa Clara, Calif.-based company [https://www.therobotreport.com/tag/nvidia] claimed that its open models allow developers to bypass resource-intensive pretraining. The new models, all available on Hugging Face, include: * NVIDIA Cosmos Transfer 2.5 [https://github.com/nvidia-cosmos/cosmos-transfer2.5] and NVIDIA Cosmos Predict 2.5 [https://github.com/nvidia-cosmos/cosmos-predict2.5]: Fully customizable world models that enable physically based synthetic data generation and robot policy evaluation in simulation for physical AI * NVIDIA Cosmos Reason 2 [https://github.com/nvidia-cosmos/cosmos-reason2]: A reasoning vision language model (VLM) that NVIDIA said enables machines to see, understand, and act in the physical world like humans * NVIDIA Isaac GR00T N1.6 [https://github.com/NVIDIA/Isaac-GR00T]: A reasoning vision-language-action (VLA) model, designed to unlock full body control for humanoids [https://www.therobotreport.com/category/robots-platforms/humanoids/] so they can move and handle objects simultaneously; it uses NVIDIA Cosmos Reason for reasoning and contextual understanding ---------------------------------------- SITE AD for the 2026 Robotics Summit save the date. [https://www.therobotreport.com/wp-content/uploads/2025/11/RSE26_LinkedInEVENT_STD_Vs1.jpg]https://www.roboticssummit.com/ ---------------------------------------- SIMULATION, COMPUTE FRAMEWORKS DESIGNED FOR ROBOTICS Scalable simulation is essential for training and evaluating robots, but current workflows remain fragmented and difficult to manage, said NVIDIA. Benchmarking is often manual and hard to scale, while end-to-end pipelines require complex orchestration across disparate compute resources. The company today released new open-source frameworks on GitHub to simplify these pipelines and accelerate the transition from research to real-world use cases. NVIDIA Isaac Lab-Arena [https://developer.nvidia.com/isaac/lab-arena] provides a system for robot policy evaluation and benchmarking in simulation, with the evaluation and task layers designed with Lightwheel [https://lightwheel.ai/media/robofinals-isaac-lab]. The company said it standardizes testing and ensures that robot skills are robust and reliable before they are deployed to physical hardware. “Isaac Lab-Arena is the world’s first collaborative system for large-scale robot policy evaluation and benchmarking to address this critical gap,” said Lebaredian. “It unifies assets, tasks, training scripts, and the most important robotics community benchmarks, such as Libero and Robocasa. As the community’s single source of truth, Isaac Lab-Arena offers the scaffolding needed to benchmark skills before real-world release.” NVIDIA OSMO [https://developer.nvidia.com/osmo] is a cloud-native orchestration framework that unifies robotic development into a single command center. The company said it lets developers define and run workflows such as synthetic data generation [https://www.nvidia.com/en-us/glossary/synthetic-data-generation/], model training, and software-in-the-loop testing. OSMO works across different compute environments — from workstations to mixed cloud instances — speeding up development cycles, NVIDIA said. OSMO is now available and used by robot developers such as Hexagon [https://www.therobotreport.com/hexagon-launches-aeon-humanoid-robot-for-industrial-applications/], and it is integrated into the Microsoft Azure Robotics Accelerator toolchain. Icons of NVIDIA's new open models, including the NVIDIA Nemotron family for agentic AI, the NVIDIA Cosmos platform for physical AI, the new NVIDIA Alpamayo family for autonomous vehicle development, NVIDIA Isaac GR00T for robotics and NVIDIA Clara for biomedical — will empower companies with the tools to develop real-world AI systems. [https://www.therobotreport.com/wp-content/uploads/2026/01/NVIDIA_open_models_CES26.jpg] NVIDIA’s new open models, including the Nemotron family for agentic AI, the Cosmos platform for physical AI, the Alpamayo family for AV development, Isaac GR00T for robotics, and Clara for biomedical, are intended as tools to help develop real-world AI systems. Source: NVIDIA NVIDIA, HUGGING FACE TEAM TO SPEED PHYSICAL AI DEVELOPMENT “NVIDIA and Hugging Face are teaming up to unite our communities [https://aiworld.eu/story/how-nvidia-is-powering-robotics-with-open-models-and-data], connecting 2 million NVIDIA robotic experts with 13 million Hugging Face AI builders,” said Lebaredian. “NVIDIA’s Isaac and GR00T technologies [https://huggingface.co/blog/nvidia/generalist-robotpolicy-eval-isaaclab-arena-lerobot] are now built into Hugging Face LeRobot library. This gives developers instant access to models like GR00T N1.6 and simulation frameworks like Isaac Labyrinth for evaluating robot skills. “On the hardware side, everything just works,” he said. “The open-source [https://www.therobotreport.com/tag/open-source/] Reachy 2 [https://www.therobotreport.com/hugging-face-bridges-gap-between-ai-physical-world-pollen-robotics-acquisition/] humanoid will run seamlessly on NVIDIA Jetson Thor [https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-thor/], letting developers deploy advanced AI models right on the robot. And for desktop projects, the Reachy Mini [https://www.therobotreport.com/hugging-face-launches-reachy-mini-robot-as-embodied-ai-platform/] pairs with DGX Spark to run custom AI, voice, and vision models locally.” COMPANIES ACROSS INDUSTRIES USE GR00T Several companies are already using GR00T [https://www.therobotreport.com/?s=gr00t&page=1]-enabled workflows to simulate, train and validate new behaviors for their robots. LEM Surgical [https://lemsurgical.com/lem-surgical-builds-humanoid-surgical-robotics-powered-by-nvidia-holoscan/] uses NVIDIA Isaac for Healthcare [https://developer.nvidia.com/isaac/healthcare] and Cosmos Transfer to train the autonomous arms of its Dynamis surgical robot, powered by NVIDIA Jetson AGX Thor [https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-thor/] and Holoscan. Several exhibitors at CES [https://www.therobotreport.com/tag/ces/] this week are using Jetson Thor to meet",
        "excerpt": "NVIDIA released Cosmos, GR00T, and Alpamayo open models to help developers build reasoning machines including robots and vehicles.\nThe post NVIDIA releases new physical AI models, plus autonomous vehi"
      },
      {
        "title": "Boston Dynamics, Google reunite on next-gen Atlas humanoid",
        "link": "https://www.therobotreport.com/boston-dynamics-google-reunite-on-next-gen-atlas-humanoid/",
        "pubDate": "2026-01-06T03:40:48.000Z",
        "content": "You can sell Boston Dynamics, but you can’t fully quit it. Nearly a decade after Google sold the robotics company to SoftBank [https://www.therobotreport.com/finally-google-sells-boston-dynamics-to-softbank/], the two are working together again, announcing at CES 2026 that DeepMind will help make the Atlas humanoid smarter with its Gemini Robotics foundation models. The event also featured Boston Dynamics [http://www.therobotreport.com/tag/boston-dynamics]’ first-ever public demonstration of Atlas, the debut of a productized version of the humanoid, plans for a robot training center, and a roadmap to deploy Atlas in Hyundai factories worldwide. FIRST PUBLIC SHOWING OF ATLAS The teleoperated Atlas demo was low-key but served as a fitting CES [https://www.therobotreport.com/tag/ces] opener, giving a glimpse into the robot’s future. The version demoed was the electric prototype introduced [https://www.therobotreport.com/boston-dynamics-debuts-electric-version-of-atlas-humanoid-robot/] in early 2024. “For the first time ever in public, please welcome Atlas to the stage,” said Zack Jackowski, general manager of Atlas at Boston Dynamics. It stood, walked, waved, spun, bent, and waved again before exiting. Later, a sleek, blue, product-ready version was revealed behind a curtain. You can watch it above. Boston Dynamics is running additional demos, both autonomous and teleoperated, at its CES booth throughout the week. Marc Raibert, founder of Boston Dynamics and executive director of the Robotics and AI Institute, attended the event. His presence, unusual for CES, highlighted the importance of the announcement, which was attended by executives from Hyundai Motor Group [https://www.therobotreport.com/tag/hyundai-motor-group/], Boston Dynamics’ parent company since its $880 million acquisition in December 2020. [https://www.therobotreport.com/hyundai-acquires-boston-dynamics-for-921m/] Atlas humanoids on stage at CES. [https://www.therobotreport.com/wp-content/uploads/2026/01/69742-on-site-photo-4.jpeg] The prototype Atlas (left) helped unveil the new, productized version of Atlas at CES 2026. | Credit: Hyundai BOSTON DYNAMICS PARTNERS WITH GOOGLE DEEPMIND ON AI While the display stole the show, the partnership with Google DeepMind [https://www.therobotreport.com/tag/google-deepmind] is more important. For humanoids [https://www.therobotreport.com/category/robots-platforms/humanoids/] to fulfill their promise as multi-purpose solutions, they must move beyond pre-programmed routines. Advances in AI [https://www.therobotreport.com/category/design-development/ai-cognition/] and robot learning could give Atlas the intelligence to adapt to a variety of tasks and environments. Boston Dynamics has increasingly relied on teleoperation [https://www.therobotreport.com/tag/teleoperation], imitation learning, and simulation to train Atlas rather than hand-coded programming. Carolina Parada, head of robotics at Google DeepMind, said the industry is just scratching the surface of what’s possible with AI. “Tasks that require certain sensors the robot doesn’t have are still limited,” she said. “But the robot can learn almost anything you can consistently demonstrate through teleoperation.” Aaron Saunders stepped down [https://www.therobotreport.com/agility-robotics-boston-dynamics-see-leadership-changes/] as chief technology officer of Boston Dynamics in November 2025. He spent nearly two decades at the company. In November 2025, he announced he was joining Google DeepMind as vice president of robotics hardware engineering. Boston Dynamics recently completed an Atlas proof of concept for Hyundai on parts sequencing [https://www.therobotreport.com/boston-dynamics-shows-atlas-humanoid-working-georgia-hyundai-plant/]. At CES, Hyundai outlined potential applications beyond automotive, including construction, energy, and facilities management. Collecting real-world data is expensive, Parada said, so DeepMind is exploring methods that allow Atlas to learn without needing to see every object in advance. To support this, Boston Dynamics and Hyundai announced the Robot Metaplant Application Center (RMAC), opening in 2026. Data collected from Hyundai factories will feed the RMAC, creating a controlled environment for Atlas to train on complex tasks. “No company has more real-world data than Hyundai Motor Group,” said Wong Jae Lee, vice president of the manufacturing division at Hyundai. Boston Dynamics also collaborates on AI with Toyota Research Institute [https://www.therobotreport.com/boston-dynamics-tri-use-large-behavior-models-train-atlas-humanoid/] and the Robotics and AI Institute [https://www.therobotreport.com/tag/ai-institute/], the latter funded by Hyundai. At press time, it was unclear how the DeepMind partnership will affect these relationships, but changes may be coming. “The work left to automate is difficult because the tasks vary so much,” said Boston Dynamics CEO Robert Playter. “That’s where AI comes in.” [https://www.therobotreport.com/wp-content/uploads/2026/01/G98R5-tbcAISNsW.jpg] Comparison of the old electric Atlas (left) and new production version. | Credit: Humanoid Hub SCALING THE NEW ATLAS The new productized Atlas [https://bostondynamics.com/products/atlas/] introduced at CES is 6.2 ft. (1.8 m) tall, has a 7.5-ft. (2.2 m) reach, fully rotational joints, and can lift up to 110 lb. (49.8 kg). It features 56 degrees of freedom, an IP67 rating, a new four-fingered hand, and a standard battery life of four hours, with the ability to swap batteries autonomously. Atlas can be operated autonomously, teleoperated via VR headset, or controlled with a tablet. Boston Dynamics said it focused on simplifying the design of the new version. “We limited the number of motors used throughout the robot to eliminate complexity,” it said. “Moreover, all of Atlas’ limbs can be replaced in the field in under five minutes, for ease of service and repair. From head to toe, every piece of Atlas’ hardware was built to reduce failure points and enable easy upgrades. There will also be training and certification paths to empower your onsite maintenance teams to maintain your fleet.” “This generation of Atlas significantly reduces the amount of unique parts in the robot, and every component has been designed for compatibility with automotive supply chains,” said the Waltham, Mass.-based company. “With Hyundai Motor Group’s backing, we will achieve the best reliability and economies of scale in the industry.” All units for 2026 are already committed, shipping to the RMAC and Google DeepMind, with additional customers planned for 2027. Hyundai aims to manufacture up to 30,000 humanoids annually by 2028. It said Atlas will be performing high-precision sequencing at scale in 2028 and complex assembly tasks slated for 2030. “Our new Atlas is the most production-friendly robot we’ve ever designed,” said Jackowski. “This generation of Atlas significantly reduces the amount of unique parts in the robot, and every component has been designed for compatibility with automotive supply chains. With Hyundai Motor Group’s backing, we will achieve the best reliability and economies of scale in the industry.” “This is the best robot we have ever built,” declared Playter. HUMANOID DEVELOPERS MUST GET PAST THE HYPE Playter noted the rapid proliferation of humanoids in the past three to five years, but said the industry’s success will depend on commercial maturity. “We have a team for integration, service, and repair,”",
        "excerpt": "At CES, Boston Dynamics reunited with Google, demoed Atlas for the first time in public, debuted a new production version of the humanoid, and announced a roadmap to deploy Atlas in Hyundai factories "
      },
      {
        "title": "Top 10 most popular robotics stories of 2025",
        "link": "https://www.therobotreport.com/top-10-most-popular-robotics-stories-of-2025/",
        "pubDate": "2026-01-05T18:29:37.000Z",
        "content": "In 2025, we saw big changes in the robotics industry. The fates of some well-known players in the ecosystem are uncertain, while other prominent companies are charting their own paths for the first time. Between advances in humanoid robotics, exciting events, and large robot orders, there was no shortage of news to cover this past year. Here are the top 10 most popular stories on The Robot Report [http://therobotreport.com/] in 2025. Subscribe to The Robot Report Newsletter [https://www.therobotreport.com/the-robot-report-enewsletter/] or listen to The Robot Report Podcast [https://www.therobotreport.com/category/podcast/] to stay up to date on the robotics developments you need to know about. ---------------------------------------- The Digit humanoid from Agility Robotics was in trials at GXO. [https://www.therobotreport.com/wp-content/uploads/2025/02/digit-raas-3-300x195.jpg]https://www.therobotreport.com/wp-content/uploads/2025/02/digit-raas-3.jpg10. HUMANOID ROBOTS PROMISE A MULTI-TRILLION-DOLLAR MARKET, BUT POSE CHALLENGES As an early investor in Agility Robotics, Karthee Madasamy has watched humanoids evolve from a fascinating concept to reality over the past five years. As a result, he wasn’t surprised when NVIDIA CEO Jensen Huang said, “Robotics will be a multi-trillion-dollar market.” Read more [https://www.therobotreport.com/humanoid-robots-promise-multi-trillion-dollar-market-but-pose-challenges/]. ---------------------------------------- Guardian Agriculture's SC1 drone on a farm. [https://www.therobotreport.com/wp-content/uploads/2025/09/GetStoredImage-300x225.png]https://www.therobotreport.com/wp-content/uploads/2025/09/GetStoredImage.png9. DRONE STARTUP GUARDIAN AGRICULTURE SHUTS DOWN Guardian Agriculture which developed large drones for aerial spraying, shut down after failing to secure additional funding. The Woburn, Mass.-based startup said it had gotten FAA approval for the first commercially authorized electric vertical take-off and landing (eVTOL) system in the U.S. Read more [https://www.therobotreport.com/drone-startup-guardian-agriculture-shuts-down/]. ---------------------------------------- Robotics Startup Radar 2025. [https://www.therobotreport.com/wp-content/uploads/2025/07/Robotics-Startup-Radar-300x195.jpg]https://www.therobotreport.com/wp-content/uploads/2025/07/Robotics-Startup-Radar.jpg8. 100 ROBOTICS STARTUPS TO WATCH IN 2025 The inaugural Startup Radar is an in-depth report that shines a spotlight on the next generation of robotics. The Startup Radar profiled 100 robotics startups, each five years old or younger, that are working to shape the future of robotics. Read more [https://www.therobotreport.com/100-robotics-startups-to-watch/]. ---------------------------------------- [https://www.therobotreport.com/wp-content/uploads/2025/12/zebra-winding-down-robotics-group-300x226.jpg]https://www.therobotreport.com/wp-content/uploads/2025/12/zebra-winding-down-robotics-group.jpg7. ZEBRA TECHNOLOGIES WINDS DOWN FETCH-BASED MOBILE ROBOT GROUP Zebra Technologies is winding down the autonomous mobile robot division that it built around its $290 million acquisition of Fetch Robotics in 2021. The move marks a strategic retreat from the robotics push Zebra launched to expand its warehouse automation capabilities. Read more [https://www.therobotreport.com/zebra-technologies-winding-down-fetch-based-mobile-robot-group/]. ---------------------------------------- ur headquarters building. [https://www.therobotreport.com/wp-content/uploads/2024/05/UR_HQ_new-300x193.jpg]https://www.therobotreport.com/wp-content/uploads/2024/05/UR_HQ_new.jpg6. TERADYNE ROBOTICS GROUP LAYS OFF 10% OF GLOBAL STAFF Economic uncertainty has taken a toll on the robotics industry, including leading vendors. Teradyne’s robotics group laid off 10% of its global workforce. Teradyne owns Universal Robots A/S and Mobile Industrial Robots ApS, leading developers of collaborative robot arms and autonomous mobile robots, respectively. Read more [https://www.therobotreport.com/teradyne-robotics-group-lays-off-10-percent-global-staff/]. ---------------------------------------- iRobot is offering the Roomba Plus 405 Combo robot + AutoWash dock at 50% off for Cyber Week. [https://www.therobotreport.com/wp-content/uploads/2025/12/iRobot_model_2025-300x193.jpg]https://www.therobotreport.com/wp-content/uploads/2025/12/iRobot_model_2025.jpg5. IROBOT DEBT ACQUIRED BY CONTRACT MANUFACTURER AS BANKRUPTCY LOOMS The fortunes of one of the few household names in robotics continued to sink in 2025. In a filing with the U.S. Securities and Exchange Commission, iRobot said that a Chinese company had acquired its debt and that it was still looking for alternatives to bankruptcy. Read more [https://www.therobotreport.com/irobot-debt-acquired-by-contract-manufacturer-as-bankruptcy-looms/]. ---------------------------------------- Automate 2025 included numerous robotics exhibitors. [https://www.therobotreport.com/wp-content/uploads/2025/05/Automate_floor-300x194.jpg]https://www.therobotreport.com/wp-content/uploads/2025/05/Automate_floor.jpg4. 10 ROBOTICS TRENDS SPOTTED AT AUTOMATE 2025 At the height of trade show season, Automate 2025 provided opportunities to see the state of commercial and industrial automation. From robot arms and mobile platforms to grippers, sensors, and software, as well as educational and technical workshops, there was something for everyone at the event. Read more [https://www.therobotreport.com/10-robotics-trends-spotted-at-automate-2025/]. ---------------------------------------- [https://www.therobotreport.com/wp-content/uploads/2025/12/rbr50-promo-homepage-300x227.jpg]https://www.therobotreport.com/wp-content/uploads/2025/12/rbr50-promo-homepage.jpg3. 50 MOST INNOVATIVE ROBOTICS COMPANIES We are passionate about the impact robotics can have on the world. That’s why for 14 years, the RBR50 Robotics Innovation Awards have honored the most innovative robotics companies, technologies, and applications from around the world. Read more [https://www.therobotreport.com/rbr50-2025/]. ---------------------------------------- Intel RealSense D421 Depth Module [https://www.therobotreport.com/wp-content/uploads/2024/09/intel-depth-module-featured-300x184.jpg]https://www.therobotreport.com/wp-content/uploads/2024/09/intel-depth-module-featured.jpg2. INTEL SPINS OUT REALSENSE AS A STANDALONE COMPANY We were a little surprised in mid-September 2024 with the launch of the entry-level Intel RealSense Depth Module D421. With the recent financial turmoil and changes at the company, there was uncertainty whether Intel would continue to invest in the product line. Read more [https://www.therobotreport.com/intel-spins-out-realsense-as-standalone-company/]. ---------------------------------------- Boston Dynamics' Atlas robot grabbing an automotive part. [https://www.therobotreport.com/wp-content/uploads/2025/04/atlas-sequencing-featured-300x195.jpg]https://www.therobotreport.com/wp-content/uploads/2025/04/atlas-sequencing-featured.jpg1. HYUNDAI TO BUY ‘TENS OF THOUSANDS’ OF BOSTON DYNAMICS ROBOTS Boston Dynamics and Hyundai Motor Group announced plans to deepen their partnership, which includes Hyundai purchasing “tens of thousands” of robots in the coming years. The automaker will also help by integrating its manufacturing capabilities with Boston Dynamics technologies. Read more [https://www.therobotreport.com/hyundai-purchase-tens-of-thousands-boston-dynamics-robots/]. The post Top 10 most popular robotics stories of 2025 [https://www.therobotreport.com/top-10-most-popular-robotics-stories-of-2025/] appeared first on The Robot Report [https://www.therobotreport.com].",
        "excerpt": "Between advances in humanoid robotics, exciting events, and large robot orders, there was no shortage of news to cover this year. \nThe post Top 10 most popular robotics stories of 2025 appeared first "
      }
    ],
    "arxiv": [
      {
        "id": "2601.03562",
        "title": "From Score to Sound: An End-to-End MIDI-to-Motion Pipeline for Robotic Cello Performance",
        "authors": [
          "Samantha Sudhoff",
          "Pranesh Velmurugan",
          "Jiashu Liu",
          "Vincent Zhao",
          "Yung-Hsiang Lu",
          "Kristen Yeon-Ji Yun"
        ],
        "abstract": "Title:\n          From Score to Sound: An End-to-End MIDI-to-Motion Pipeline for Robotic Cello Performance\n        \n          Robot musicians require precise control to obtain proper note accuracy, sound quality, and musical expression. Performance of string instruments, such as violin and cello, presents a significant challenge due to the precise control required over bow angle and pressure to produce the desired sound. While prior robotic cellists focus on accurate bowing trajectories, these works often rely on expensive motion capture techniques, and fail to sightread music in a human-like way.\nWe propose a novel end-to-end MIDI score to robotic motion pipeline which converts musical input directly into collision-aware bowing motions for a UR5e robot cellist. Through use of Universal Robot Freedrive feature, our robotic musician can achieve human-like sound without the need for motion capture. Additionally, this work records live joint data via Real-Time Data Exchange (RTDE) as the robot plays, providing labeled robotic playing data from a collection of five standard pieces to the research community. To demonstrate the effectiveness of our method in comparison to human performers, we introduce the Musical Turing Test, in which a collection of 132 human participants evaluate our robot's performance against a human baseline. Human reference recordings are also released, enabling direct comparison for future studies. This evaluation technique establishes the first benchmark for robotic cello performance. Finally, we outline a residual reinforcement learning methodology to improve upon baseline robotic controls, highlighting future opportunities for improved string-crossing efficiency and sound quality.",
        "link": "https://arxiv.org/abs/2601.03562",
        "submittedDate": "2026-01-09T00:35:11.985Z"
      },
      {
        "id": "2601.03686",
        "title": "Dual-Attention Heterogeneous GNN for Multi-robot Collaborative Area Search via Deep Reinforcement Learning",
        "authors": [
          "Lina Zhu",
          "Jiyu Cheng",
          "Yuehu Liu",
          "Wei Zhang"
        ],
        "abstract": "Title:\n          Dual-Attention Heterogeneous GNN for Multi-robot Collaborative Area Search via Deep Reinforcement Learning\n        \n          In multi-robot collaborative area search, a key challenge is to dynamically balance the two objectives of exploring unknown areas and covering specific targets to be rescued. Existing methods are often constrained by homogeneous graph representations, thus failing to model and balance these distinct tasks. To address this problem, we propose a Dual-Attention Heterogeneous Graph Neural Network (DA-HGNN) trained using deep reinforcement learning. Our method constructs a heterogeneous graph that incorporates three entity types: robot nodes, frontier nodes, and interesting nodes, as well as their historical states. The dual-attention mechanism comprises the relational-aware attention and type-aware attention operations. The relational-aware attention captures the complex spatio-temporal relationships among robots and candidate goals. Building on this relational-aware heterogeneous graph, the type-aware attention separately computes the relevance between robots and each goal type (frontiers vs. points of interest), thereby decoupling the exploration and coverage from the unified tasks. Extensive experiments conducted in interactive 3D scenarios within the iGibson simulator, leveraging the Gibson and MatterPort3D datasets, validate the superior scalability and generalization capability of the proposed approach.",
        "link": "https://arxiv.org/abs/2601.03686",
        "submittedDate": "2026-01-09T00:35:11.985Z"
      },
      {
        "id": "2601.03782",
        "title": "PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation",
        "authors": [
          "Wenlong Huang",
          "Yu-Wei Chao",
          "Arsalan Mousavian",
          "Ming-Yu Liu",
          "Dieter Fox",
          "Kaichun Mo",
          "Li Fei-Fei"
        ],
        "abstract": "Title:\n          PointWorld: Scaling 3D World Models for In-The-Wild Robotic Manipulation\n        \n          Humans anticipate, from a glance and a contemplated action of their bodies, how the 3D world will respond, a capability that is equally vital for robotic manipulation. We introduce PointWorld, a large pre-trained 3D world model that unifies state and action in a shared 3D space as 3D point flows: given one or few RGB-D images and a sequence of low-level robot action commands, PointWorld forecasts per-pixel displacements in 3D that respond to the given actions. By representing actions as 3D point flows instead of embodiment-specific action spaces (e.g., joint positions), this formulation directly conditions on physical geometries of robots while seamlessly integrating learning across embodiments. To train our 3D world model, we curate a large-scale dataset spanning real and simulated robotic manipulation in open-world environments, enabled by recent advances in 3D vision and simulated environments, totaling about 2M trajectories and 500 hours across a single-arm Franka and a bimanual humanoid. Through rigorous, large-scale empirical studies of backbones, action representations, learning objectives, partial observability, data mixtures, domain transfers, and scaling, we distill design principles for large-scale 3D world modeling. With a real-time (0.1s) inference speed, PointWorld can be efficiently integrated in the model-predictive control (MPC) framework for manipulation. We demonstrate that a single pre-trained checkpoint enables a real-world Franka robot to perform rigid-body pushing, deformable and articulated object manipulation, and tool use, without requiring any demonstrations or post-training and all from a single image captured in-the-wild. Project website at this https URL.",
        "link": "https://arxiv.org/abs/2601.03782",
        "submittedDate": "2026-01-09T00:35:11.985Z"
      },
      {
        "id": "2601.03807",
        "title": "Generational Replacement and Learning for High-Performing and Diverse Populations in Evolvable Robots",
        "authors": [
          "K. Ege de Bruin",
          "Kyrre Glette",
          "Kai Olav Ellefsen"
        ],
        "abstract": "Title:\n          Generational Replacement and Learning for High-Performing and Diverse Populations in Evolvable Robots\n        \n          Evolutionary Robotics offers the possibility to design robots to solve a specific task automatically by optimizing their morphology and control together. However, this co-optimization of body and control is challenging, because controllers need some time to adapt to the evolving morphology - which may make it difficult for new and promising designs to enter the evolving population. A solution to this is to add intra-life learning, defined as an additional controller optimization loop, to each individual in the evolving population. A related problem is the lack of diversity often seen in evolving populations as evolution narrows the search down to a few promising designs too quickly. This problem can be mitigated by implementing full generational replacement, where offspring robots replace the whole population. This solution for increasing diversity usually comes at the cost of lower performance compared to using elitism. In this work, we show that combining such generational replacement with intra-life learning can increase diversity while retaining performance. We also highlight the importance of performance metrics when studying learning in morphologically evolving robots, showing that evaluating according to function evaluations versus according to generations of evolution can give different conclusions.",
        "link": "https://arxiv.org/abs/2601.03807",
        "submittedDate": "2026-01-09T00:35:11.985Z"
      },
      {
        "id": "2601.03813",
        "title": "Integrating Sample Inheritance into Bayesian Optimization for Evolutionary Robotics",
        "authors": [
          "K. Ege de Bruin",
          "Kyrre Glette",
          "Kai Olav Ellefsen"
        ],
        "abstract": "Title:\n          Integrating Sample Inheritance into Bayesian Optimization for Evolutionary Robotics\n        \n          In evolutionary robotics, robot morphologies are designed automatically using evolutionary algorithms. This creates a body-brain optimization problem, where both morphology and control must be optimized together. A common approach is to include controller optimization for each morphology, but starting from scratch for every new body may require a high controller learning budget. We address this by using Bayesian optimization for controller optimization, exploiting its sample efficiency and strong exploration capabilities, and using sample inheritance as a form of Lamarckian inheritance. Under a deliberately low controller learning budget for each morphology, we investigate two types of sample inheritance: (1) transferring all the parent's samples to the offspring to be used as prior without evaluating them, and (2) reevaluating the parent's best samples on the offspring. Both are compared to a baseline without inheritance. Our results show that reevaluation performs best, with prior-based inheritance also outperforming no inheritance. Analysis reveals that while the learning budget is too low for a single morphology, generational inheritance compensates for this by accumulating learned adaptations across generations. Furthermore, inheritance mainly benefits offspring morphologies that are similar to their parents. Finally, we demonstrate the critical role of the environment, with more challenging environments resulting in more stable walking gaits. Our findings highlight that inheritance mechanisms can boost performance in evolutionary robotics without needing large learning budgets, offering an efficient path toward more capable robot design.",
        "link": "https://arxiv.org/abs/2601.03813",
        "submittedDate": "2026-01-09T00:35:11.985Z"
      },
      {
        "id": "2601.04191",
        "title": "Embedding Autonomous Agents in Resource-Constrained Robotic Platforms",
        "authors": [
          "Negar Halakou",
          "Juan F. Gutierrez",
          "Ye Sun",
          "Han Jiang",
          "Xueming Wu",
          "Yilun Song",
          "Andres Gomez"
        ],
        "abstract": "Title:\n          Embedding Autonomous Agents in Resource-Constrained Robotic Platforms\n        Comments:\n          This is an open-access, author-archived version of a manuscript published in European Conference on Multi-Agent Systems 2025\n        \n          Many embedded devices operate under resource constraints and in dynamic environments, requiring local decision-making capabilities. Enabling devices to make independent decisions in such environments can improve the responsiveness of the system and reduce the dependence on constant external control. In this work, we integrate an autonomous agent, programmed using AgentSpeak, with a small two-wheeled robot that explores a maze using its own decision-making and sensor data. Experimental results show that the agent successfully solved the maze in 59 seconds using 287 reasoning cycles, with decision phases taking less than one millisecond. These results indicate that the reasoning process is efficient enough for real-time execution on resource-constrained hardware. This integration demonstrates how high-level agent-based control can be applied to resource-constrained embedded systems for autonomous operation.",
        "link": "https://arxiv.org/abs/2601.04191",
        "submittedDate": "2026-01-09T00:35:11.986Z"
      },
      {
        "id": "2504.16680",
        "title": "Uncertainty-Aware Robotic World Model Makes Offline Model-Based Reinforcement Learning Work on Real Robots",
        "authors": [
          "Chenhao Li",
          "Andreas Krause",
          "Marco Hutter"
        ],
        "abstract": "Title:\n          Uncertainty-Aware Robotic World Model Makes Offline Model-Based Reinforcement Learning Work on Real Robots\n        \n          Reinforcement Learning (RL) has achieved impressive results in robotics, yet high-performing pipelines remain highly task-specific, with little reuse of prior data. Offline Model-based RL (MBRL) offers greater data efficiency by training policies entirely from existing datasets, but suffers from compounding errors and distribution shift in long-horizon rollouts. Although existing methods have shown success in controlled simulation benchmarks, robustly applying them to the noisy, biased, and partially observed datasets typical of real-world robotics remains challenging. We present a principled pipeline for making offline MBRL effective on physical robots. Our RWM-U extends autoregressive world models with epistemic uncertainty estimation, enabling temporally consistent multi-step rollouts with uncertainty effectively propagated over long horizons. We combine RWM-U with MOPO-PPO, which adapts uncertainty-penalized policy optimization to the stable, on-policy PPO framework for real-world control. We evaluate our approach on diverse manipulation and locomotion tasks in simulation and on real quadruped and humanoid, training policies entirely from offline datasets. The resulting policies consistently outperform model-free and uncertainty-unaware model-based baselines, and fusing real-world data in model learning further yields robust policies that surpass online model-free baselines trained solely in simulation.",
        "link": "https://arxiv.org/abs/2504.16680",
        "submittedDate": "2026-01-09T00:35:11.986Z"
      },
      {
        "id": "2509.19486",
        "title": "Supercomputing for High-speed Avoidance and Reactive Planning in Robots",
        "authors": [
          "Kieran S. Lachmansingh",
          "José R. González-Estrada",
          "Jacob Chisholm",
          "Ryan E. Grant",
          "Matthew K. X. J. Pan"
        ],
        "abstract": "Title:\n          Supercomputing for High-speed Avoidance and Reactive Planning in Robots\n        Comments:\n          Error in the graph size calculation, recalculated and resubmitted\n        \n          This paper presents SHARP (Supercomputing for High-speed Avoidance and Reactive Planning), a proof-of-concept study demonstrating how high-performance computing (HPC) can enable millisecond-scale responsiveness in robotic control. While modern robots face increasing demands for reactivity in human-robot shared workspaces, onboard processors are constrained by size, power, and cost. Offloading to HPC offers massive parallelism for trajectory planning, but its feasibility for real-time robotics remains uncertain due to network latency and jitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator must dodge high-speed foam projectiles. Using a hash-distributed multi-goal A* search implemented with MPI on both local and remote HPC clusters, the system achieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300 km away), with avoidance success rates of 84% and 88%, respectively. These results show that when round-trip latency remains within the tens-of-milliseconds regime, HPC-side computation is no longer the bottleneck, enabling avoidance well below human reaction times. The SHARP results motivate hybrid control architectures: low-level reflexes remain onboard for safety, while bursty, high-throughput planning tasks are offloaded to HPC for scalability. By reporting per-stage timing and success rates, this study provides a reproducible template for assessing real-time feasibility of HPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable pathway toward dependable, reactive robots in dynamic environments.",
        "link": "https://arxiv.org/abs/2509.19486",
        "submittedDate": "2026-01-09T00:35:11.986Z"
      },
      {
        "id": "2511.01219",
        "title": "Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference",
        "authors": [
          "Muhua Zhang",
          "Lei Ma",
          "Ying Wu",
          "Kai Shen",
          "Deqing Huang",
          "Henry Leung"
        ],
        "abstract": "Title:\n          Tackling the Kidnapped Robot Problem via Sparse Feasible Hypothesis Sampling and Reliable Batched Multi-Stage Inference\n        Comments:\n          10 pages, 8 figures. This work has been submitted to the IEEE for possible publication\n        \n          This paper addresses the Kidnapped Robot Problem (KRP), a core localization challenge of relocalizing a robot in a known map without prior pose estimate when localization loss or at SLAM initialization. For this purpose, a passive 2-D global relocalization framework is proposed. It estimates the global pose efficiently and reliably from a single LiDAR scan and an occupancy grid map while the robot remains stationary, thereby enhancing the long-term autonomy of mobile robots. The proposed framework casts global relocalization as a non-convex problem and solves it via the multi-hypothesis scheme with batched multi-stage inference and early termination, balancing completeness and efficiency. The Rapidly-exploring Random Tree (RRT), under traversability constraints, asymptotically covers the reachable space to generate sparse, uniformly distributed feasible positional hypotheses, fundamentally reducing the sampling space. The hypotheses are preliminarily ordered by the proposed Scan Mean Absolute Difference (SMAD), a coarse beam-error level metric that facilitates the early termination by prioritizing high-likelihood candidates. The SMAD computation is optimized for non-panoramic scans. The Translation-Affinity Scan-to-Map Alignment Metric (TAM) is proposed for reliable orientation selection at hypothesized positions and accurate final pose evaluation to mitigate degradation in conventional likelihood-field metrics under translational uncertainty induced by sparse hypotheses, as well as non-panoramic LiDAR scan and environmental changes. Real-world experiments on a resource-constrained mobile robot with non-panoramic LiDAR scans show that the proposed framework achieves competitive performance in both global relocalization success rate and computational efficiency.",
        "link": "https://arxiv.org/abs/2511.01219",
        "submittedDate": "2026-01-09T00:35:11.986Z"
      }
    ]
  },
  "total_count": 24
}